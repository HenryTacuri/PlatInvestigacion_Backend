[
         {
                  "titulo":"Brain tumour histopathology through the lens of deep learning: A systematic review",
                  "autor":"Chun Kiet Vong and Alan Wang and Mike Dragunow and Thomas I-H. Park and Vickie Shim",
                  "link":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S001048252401727X",
                  "ano":"2025",
                  "Texto":"Brain tumour histopathology through the lens of deep learning: A \nsystematic review\nChun Kiet Vong a,b\n, Alan Wang a,b,d\n, Mike Dragunow b,c, Thomas I-H. Park b,c, Vickie Shim a,*\na Auckland Bioengineering Institute, The University of Auckland, New Zealand\nb Centre for Brain Research, The University of Auckland, New Zealand\nc Department of Pharmacology, The Faculty of Medical and Health Sciences, The University of Auckland, New Zealand\nd Faculty of Medical and Health Sciences, The University of Auckland, New Zealand\nA R T I C L E  I N F O\nKeywords:\nGlioblastoma\nMachine learning\nDeep learning\nReview\nHematoxylin and eosin\nBioinformatics\n1  A B S T R A C T\nProblem: Machine learning (ML)\/Deep learning (DL) techniques have been evolving to solve more complex \ndiseases, but it has been used relatively little in Glioblastoma (GBM) histopathological studies, which could \nbenefit greatly due to the disease\u2019s complex pathogenesis.\nAim: Conduct a systematic review to investigate how ML\/DL techniques have influenced the progression of brain \ntumour histopathological research, particularly in GBM.\nMethods: 54 eligible studies were collected from the PubMed and ScienceDirect databases, and their information \nabout the types of brain tumour\/s used, types of -omics data used with histopathological data, origins of the data, \ntypes of ML\/DL and its training and evaluation methodologies, and the ML\/DL task it was set to perform in the \nstudy were extracted to inform us of trends in GBM-related ML\/DL-based research.\nResults: Only 8 GBM-related studies in the eligible utilised ML\/DL methodologies to gain deeper insights into \nGBM pathogenesis by contextualising histological data with -omics data. However, we report that these studies \nhave been published more recently. The most popular ML\/DL models used in GBM-related research are the SVM \nclassifier and ResNet-based CNN architecture. Still, a considerable number of studies failed to state training and \nevaluative methodologies clearly.\nConclusion: There is a growing trend towards using ML\/DL approaches to uncover relationships between bio\u00ad\nlogical and histopathological data to bring new insights into GBM, thus pushing GBM research forward. Much \nwork still needs to be done to properly report the ML\/DL methodologies to showcase the models\u2019 robustness and \ngeneralizability and ensure the models are reproducible.\n1. Introduction\nGlioblastoma (GBM) is the most common malignant brain tumour \n[1]. GBM is almost invariably fatal, with the patient\u2019s median overall \nsurvival being approximately one year [2]. The standard care for many \nnewly diagnosed GBM patients involves surgical tumour resection and \nradio-chemotherapy [3]. However, GBM tumours often recur in func\u00ad\ntional brain areas and are less sensitive to therapy than the original \ntumour, thus preventing another surgical resection [4]. A distinct \nfeature of GBM is its heterogeneity, which plays a significant role in the \ndifficulty of treating GBM [5,6]. Therefore, much research has investi\u00ad\ngated the nature of the heterogeneity of GBM and its contribution to \ntreatment difficulties and fatality.\nThe consensus amongst The Cancer Genome Atlas (TCGA) Research \nNetwork and others is that GBM can be categorised into four subtypes: \nproneural, neural, classical and mesenchymal [7,8]. Verhaak and col\u00ad\nleagues noticed that subtypes could be differentiated using distinct \nmolecular signatures. They reported that 100 % of the classical subtype \nexhibited a chromosome 7 amplification paired with chromosome 10 \nloss, with 97 % of the classical subtype having high levels of epidermal \ngrowth factor receptor (EGFR) gene amplifications [8] Verhaak and \ncolleagues further noted that mesenchymal subtype predominantly \nfeatured lower NF1 expression levels, while proneural subtypes had \nalterations platelet-derived growth factor alpha (PDGFRA) receptor and \nisocitrate dehydrogenase (IDH) 1 as their significant features [8] This \nshows that although GBM is heterogenous, patterns were observed. \n* Corresponding author.\nE-mail address: v.shim@auckland.ac.nz (V. Shim). \nContents lists available at ScienceDirect\nComputers in Biology and Medicine\njournal homepage: www.elsevier.com\/locate\/compbiomed\nhttps:\/\/doi.org\/10.1016\/j.compbiomed.2024.109642\nReceived 2 July 2024; Received in revised form 26 December 2024; Accepted 27 December 2024  \nComputers in Biology and Medicine 186 (2025) 109642 \nAvailable online 8 January 2025 \n0010-4825\/\u00a9 2025 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http:\/\/creativecommons.org\/licenses\/by\/4.0\/ ). \n\nBased on this, potential treatments can be more effective and person\u00ad\nalised to improve patient outcomes.\nMore recently, the World Health Organization (WHO) broadly clas\u00ad\nsified GBM into two types: IDH-wildtype (wt) GBM and IDH-mutant (mt) \nGBM [9]. Louis and colleagues further noted GBM variants under the \ntwo classifications mentioned above. They classified epithelioid GBM \n(Ep-GBM) under the IDH-wt group. It featured large epithelioid cells \nwith abundant eosinophilic cytoplasm, vesicular nuclei and large \nmelanoma-like nucleoli, all of which can be distinguished histopatho\u00ad\nlogically [9]. Louis and colleagues further categorised GBM with prim\u00ad\nitive neuronal component (GBM-PNC) to have highly cellular nodules in \nan otherwise diffuse astrocytoma in MRI screenings. They can be his\u00ad\ntopathologically identified by the Homer Wright rosettes along with \nlarge cell\/anaplastic features [9]. Small cell GBM can be characterised \nby uniform, small neoplastic cells and often exhibiting EGFR amplifi\u00ad\ncations, while granular cell GBM exhibits granular to macrophage-like, \nlysosomic-rich \ntumour \ncells \n[9]. \nOverall, \nthe \nhisto\u00ad\npathological\/morphological classification of GBM types and the shared \ngenes identified give rise to the potential of correlating image data and \nmolecular markers in GBM for better diagnosis and prognosis.\nRecent advancements in machine learning (ML) and deep learning \n(DL) have diversified its use, particularly in clinical use. ML\/DL can \nextract patterns from examples at its core, and it has proven to be more \nbeneficial with more enormous datasets, especially for more intricate \npatterns, which are usually too complex for human analyses. In the field \nof brain tumour research, many of the ML\/DL applications have \nrevolved around the segmentation and volumetric analyses of brain \ntumours from MRI and CT screenings [10\u201313]. This was often in the \nform of segmentation of the tumour volume in MRI images using various \nconvolutional neural networks (CNN) [14,15]. Classification tasks and \nimproving accuracy are also common objectives in ML\/DL-based MRI \nresearch. Studies would often develop novel pipelines and asses their \naccuracies in classifying various types of brain tumours [16,17]. These \ntasks would otherwise be incredibly difficult to achieve without the \nML\/DL\u2019s capabilities to recognise underlying patterns that are too \ncomplex for humans to understand.\nEmerging research has also utilised ML\/DL tools to recognise pat\u00ad\nterns in histopathological images, either as a segmentation tool or to \nclassify the tumours [18,19]. In segmentation, CNNs are standard tools \nused to segment tumour features [19\u201322]. ML\/DL has also been used to \nidentify and correlate patterns in brain tumour histopathologic images \nwith their corresponding molecular statuses. Notably, Cui and col\u00ad\nleagues extracted features from histopathology images of glioma pa\u00ad\ntients and correlated them with their IDH mutation statuses via \nmultiple-instance learning [18]. In another example, Fatima and col\u00ad\nleagues extracted features from segmented nuclei of meningioma his\u00ad\ntology images to classify different subtypes of meningioma using a \nsupport vector machine (SVM) ensemble ML model [23].\nIt is evident that in more recent brain tumour histopathological \nstudies, ML\/DL has been used more often. However, our recent litera\u00ad\nture survey suggests that relatively few works consolidate bio- \ninformation with image data, and fewer pieces of literature focused \ntheir investigations on GBM histopathology. This is important because \nGBM diagnoses are primarily based on expensive multi-omics assays on \ntissue biopsies [9]. These are more closely related to cheaper and \naffordable histopathological work than relatively more expensive and \nless accessible MRI\/CT scans. Therefore, it is logical to infer that the \ncontextualisation of bio-information acquired from biopsies and \nstate-of-the-art research using ML\/DL methods would result in a better \nunderstanding of GBM\u2019s microenvironment, as well as the development \nof accessible and standardised diagnostic and prognostic tools for GBM. \nThus, a knowledge gap exists that utilises ML\/DL to meaningfully \ncombine GBM bio-information and its corresponding histopathology \ndata. We propose that a review of ML\/DL utilisation in brain tumour \nresearch as a whole can inform us about what and how we can improve \nmulti-scale ML\/DL-based GBM research.\nTherefore, this systematic review consolidates the various ML\/DL \nmodels and their purpose in brain tumour histopathological research. \nThe primary contributions of this review are as follows. \n\u2022 Conduct a meta-analysis of various ML\/DL-based studies on brain \ntumours to investigate the current trend of ML\/DL use in brain \ntumour histopathological studies. From the current trends, identify \nthe common ML\/DL models with a focus on the task they perform in \nbrain tumour histopathology studies. Uncovering their role in un\u00ad\nderstanding brain tumour pathology is key here.\n\u2022 Suggest novel ML\/DL utilisation avenues in GBM histopathology \nstudies based on the current trends and gaps identified in the meta- \nanalysis.\nThe remainder of the article is split into four sections. The method\u00ad\nology section (Section 3) describes how the articles were searched, \nfiltered and analysed. Section 4 consists of the result section, where \ntrends in types of brain tumours used, types of datasets and dataset \navailability, and the ML\/DL models together with their purpose in each \nstudy were reported. Section 5 discusses the findings of the results, while \nSection 6 concludes the article.\n2. Methods\n2.1. Search strategy, eligibility screening and data extraction\nThis systematic review followed the Preferred Reporting Items for \nSystematic Reviews and Meta-Analyses (PRISMA) guidelines [24]. \nOriginal articles were sourced from the PubMed and Science Direct \nelectronic databases published up to March 2022 (from 2000-) to review \nthe recent advancements in the field. After the initial search, duplicated \narticles were identified and removed manually. Reviews, non-English \narticles and conference articles were excluded. The three main cate\u00ad\ngorical keywords used to identify relevant studies to this systematic \nreview were: 1.) Histology or histopathology, 2.) Machine learning or \ndeep learning (DL), and 3.) Brain tumour or glioblastoma. The studies \nthat include the three main categorical keywords were screened using \nBoolean operations in the databases\u2019 advanced search function. Table 1\ndetails the search parameters of PubMed and Science Direct.\nThe studies were further screened for eligibility in this systematic \nreview. During the initial search, studies involving MRI and CT with \nminimal to no histology work were included in the search results, which \nwere beyond the scope of this review. Thus, an additional NOT Boolean \noperation was implemented in the search to exclude non-relevant MRI\/ \nCT studies, along with the inclusion of the \u201chistology OR histopatholo\u00ad\ngy\u201d search terms (Table 1). Furthermore, the glioblastoma search term \nwas returning very sparse results. Hence, an additional brain tumour \nsearch term with the OR operator was added to find more relevant \nstudies (Table 1). The \u201csegmentation OR classification\u201d and \u201cdeep \nlearning OR machine learning\u201d search terms were included to constrain \nthe studies towards ML\/DL-based brain tumour research. Then, a further \nmanual screening was applied to remove the remaining non-relevant \nTable 1 \nSearch terms used in the advanced search feature from ScienceDirect and \nPubMed databases.\nDatabase\nSearch\nScienceDirect\n(\"brain tumour\" OR \"glioblastoma\") AND (\"segmentation\" OR \n\"classification) AND (\"histology\" OR \"histopathology\") AND (\"deep \nlearning\" OR \"machine learning\") NOT \"magnetic resonance \nimaging\"\nPubMed\n((glioblastoma) OR (glioma) OR (brain tumour)) AND \n((segmentation) OR (classification)) AND ((histology) OR \n(histopathology)) AND ((machine learning) OR (neural network)) \nNOT ((magnetic resonance) OR (computer tomography) OR \n(positron emission))\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n2 \n\nstudies that did not include all 3 of the categorical keywords.\n2.2. Risk of bias assessment\nThe screened articles were assessed for their methodological quality \nby conducting a risk of bias assessment. Based on the QualSyst Assess\u00ad\nment Tool for quantitative studies [25], ten questions were asked for \neach of the screened articles within the scope of this review. The ques\u00ad\ntions were as follows: 1.) Was a clear objective stated? 2.) Was the study \ndesign appropriate to answer the study question\/goal? 3.) Was the \nsubject selection or information source described appropriate? 4.) Were \nsubject characteristics or input variables clearly described and appro\u00ad\npriate? 5.) Was the methodology and outcome clearly described? 6.) \nWas there an appropriate sample\/training size for the study? 7.) Were \nthere analyses and validation appropriate for the study? 8.) Was there an \nestimate of variance, if applicable? 9.) Were the result details appro\u00ad\npriate and sufficient? 10.) Did the result support the conclusion made in \nthe study? The questions were answered either yes, partial or no, \ndenoted under the scores of 2,1 and 0, respectively. The scores across the \nten questions were summated to acquire the summary score, and the \nfinal score was calculated by dividing the summary score by the highest \npossible score for the study. Articles with a score of 0.78 or higher were \ndeemed eligible for data extraction. The author, C.V., initially assessed \nthe risk, which was then cross-checked with the corresponding author, \nV.S.\n2.3. Data extraction and analyses\nAfter the quality assessment, specific data was extracted from each \nstudy. ML or DL methods used in these studies were identified with a \nparticular focus on how they were used in relation to histopathological \nwork. Five data criteria were extracted from each article and investi\u00ad\ngated: 1.) The type of brain tumour used in the study, 2.) What histo\u00ad\npathological type, and if applicable, what combination of other data \ntypes and the histopathological data, were used in the study? 3.) Where \nthe data was collected 4.) The type of ML\/DL tools, and its training and \nevaluative methodologies conducted in the study, 5.) How were the ML\/ \nDL tools used to achieve the study\u2019s research goals? These first and third \ncriteria were developed to understand the landscape of brain tumour \nstudies and the available data, while the three other criteria ascertain \nthe current trends in brain tumour histopathological studies and how \nML\/DL is utilised in brain tumour studies. Within the fourth criterion, \nML\/DL training and testing methodologies, along with its evaluation \nmethodologies (when applicable), were collected to investigate the \nreproducibility of the studies\u2019 models and to observe the trends in recent \nML\/DL model design. The resultant extracted criteria would highlight \nthe innovative trends that can push ML\/DL-based brain tumour research \nin a new direction.\n3. Results\n3.1. Search results\nA total of 388 articles were initially retrieved from the search. Of the \n388 articles, 324 articles were deemed irrelevant for this systematic \nreview and 8 duplicate articles were removed. Interestingly, many \nirrelevant studies used MRI\/CT-based datasets. 2 articles were removed \nas they failed to meet our risk of bias assessment criteria (Supplementary \nTable 1). Notably, most articles included in this systematic review were \npublished in recent years (Fig. 1). This is indicative of the nascent nature \nof this scope of research.\nAs the scope of this review is brain tumour studies that used ML\/DL \ntechniques in analyzing histopathological work for integration with \navailable bioinformation, 54 eligible articles were selected (Fig. 2). We \nidentified the following features \u2013 1.) brain tumour types, 2.) image stain \ntypes, 3.) databases used to obtain histopathology work or additional \ndata, 4.) how ML\/DL methods were used. Tables 2 and 3 detail the \nculmination of the analyses completed on the 54 articles, with Table 2\nproviding a general summary of how the ML\/DL tools were utilised in \nthe group, while Table 3 provides a detailed overview of the data sources \nand ML\/DL training and evaluation methodologies.\n3.2. Brain tumour types and data sources\nThe purpose of the collection and analysis of types of brain tumours \nand the data sources used in the 54 studies is to give us an insight into \nthe landscape of which brain tumour is more studied and the data \navailability of the studies, respectively. To this end, each study noted the \nmain types of tumours used and which data centres or hospitals they \ncollected their specimens or data. From the review, 37 studies were \nreported to be GBM-related, while the remaining 17 were not GBM- \nrelated. Of the GBM-related studies, 12 focused on GBM only, while \n18 focused on gliomas including GBM, and the remaining 7 studies \neither studied human-derived GBM cell lines or studied various brain \ntumours including GBM (Fig. 3). In GBM-unrelated studies, the most \ncommon brain tumour studied was gliomas, excluding GBM (6 studies; \nFig. 1. Histogram of final selection of articles sorted based on the year it is publish from ScienceDirect and PubMed.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n3 \n\nFig. 3), followed by meningioma brain tumours (Fig. 3). This indicates \nthat much ML\/DL-histopathological research focuses on GBM and LGG. \nInterestingly, the surveyed glioma-GBM studies tended to compare the \nLGG against GBM tumour types against each other. For instance, Barker \nand colleagues developed a pipeline to accurately distinguish between \nGBM and LGG histopathology slides [26]. What is also interesting to \nnote is that although GBM has been a keen area of study in this sys\u00ad\ntematic review in the past two decades, the general consensus is that the \nmedian overall survival rate of patients has only improved from \napproximately 6 months\u201310 months in the last two decades [27,28]. \nThis indicates that despite the keen interest in studying GBM, our un\u00ad\nderstanding of the disease remains nascent and inadequate to improve \npatient outcomes.\nLooking at the datasets used in all studies in our analysis, we report \nthat The Cancer Genome Atlas, a publicly available dataset, is the most \nused public dataset in our review (21 articles; Fig. 4). We report that the \nother two popular publicly available datasets used were the Computa\u00ad\ntional Precision Medicine (CPM) dataset and the Medical Image \nComputing and Computer Assisted Intervention (MICCAI) dataset, with \n3 articles and 2 articles reported to use these datasets, respectively \n(Fig. 4). However, 25 articles used non-public datasets (data collected \nfrom hospitals, research centres or in-house cell lines) in their studies \n(Fig. 4). In GBM-related studies, a similar trend is observed in the ratio \nbetween the most popular publicly available dataset and the non-public \ndatasets used for studies. It is concerning to note that 4 of the 54 articles \ndid not clearly specify where their data was collected from (Fig. 4), \nwhich would prove to be difficult for others to replicate its results. \nOverall, there is a clear interest in using publicly available datasets in \nbrain tumour-related studies, but a similar amount of interest in curating \nthe data from research centres, hospitals or cell lines.\n3.3. Stain types and analyses\nThis section explores primarily why the ML\/DL-based tools were \nused in the studies. To understand why, we first report what type of \nhistopathological data they are working with and whether they were \nintegrated with other data types (e.g. -omics data, patient data, clinical \ndiagnostic data) in their studies\u2019 ML\/DL models. We also investigated \nhow these data were analysed, categorising them as studies that only \nsegment or processed images: Studies that primarily predict patient \nsurvival and tumour grades using only histopathological data are cat\u00ad\negorised under typical analysis, whilst studies that integrate histopath\u00ad\nological data with other data types (i.e. additional -omics data) for their \nanalysis were categorised as unique analysis. The results from this sec\u00ad\ntion give us insight into what types of data are being combined with \nhistopathology in ML\/DL-based brain tumour research and what novel \nways the data are being combined to enhance brain tumour research.\nThe eligible studies used hematoxylin and eosin (H&E) stained im\u00ad\nages as their data type, with 41 of the 54 studies combining various other \ndata types with H&E data for more complex analyses and investigations \n(Fig. 5). Interestingly, 3 studies utilised DAPI stained images, stimulated \nRaman histology (SRH) and immunohistochemical (IHC) stained im\u00ad\nages, and it was included in the relevant studies as we believe that the \nmethodologies in these studies are similar enough to other H&E-based \nstudies and can be transferable to H&E-based applications. Orzan and \ncolleagues utilised IHC staining with their respective transcriptomic \ndata to train their random forest (RF) classifier to discriminate between \ndifferent GBM subtypes (proneural, mesenchymal and classical). Woll\u00ad\nman and colleagues used nuclei segmentations from DAPI-stained im\u00ad\nages to train their model to accurately segment nuclei [19]. Hollon and \ncolleagues trained their model to classify and segment SRH images of \nbrain tumours; it was able to accurately classify various types of brain \ntumours, including GBM, diffuse LGG subtypes (oligodendroglioma and \nFig. 2. Flowchart of how the articles from the search was screened for relevancy and assessed for eligibility.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n4 \n\nTable 2 \nTable summarising how ML\/DL tools were used to achieve the task\/s of each eligible paper in the systematic review.\nAuthors\nPublication \nDate (Year)\nBrain Tumour specified\nData type\nUtilisation of ML\/DL\n[43].\n2007\nMeningioma (Meningotheliomatous, \nFibroblastic, Transitional, Psammomatous)\nHematoxylin and eosin (H&E) \nimages and clinical data (tumour \nsubtypes)\nAuthors used SOM to cluster and classify meningioma \nsubtypes. They utilised a class separation measure to \nshow that a subset of features can accurately \ndistinguish the four meningioma subtypes.\n[30]\n2022\nGlioblastoma (GBM) and Lower-Grade \nGlioma (LGG)\nH&E images, genomic and clinical \ndata (survival data and tumour \nsubtype)\npresented a multi-modal fusion framework based on \nmulti-task correlation learning (MultiCoFusion) for \nsurvival analysis and cancer grade classification using \nwhole slide images (WSIs) (trained under the ResNet- \n152 model) and mRNA expression (trained under \nSCGN) data. As a result, the multi-modal framework \nwas able to learn better representations than \ntraditional feature extraction models for accurate \nsurvival prediction.\n[66]\n2015\nMeningioma (Fibroblastic, Meningothelial, \nPsammomatous, Transitional)\nH&E images and clinical data \n(meningioma subtypes)\nThe H&E image patch features were used to train their \ncustom fractal model and extract optimal features for \nmeningioma classification. Once the optimal features \nwere extracted, the SVM, Bayesian and kNN algorithms \nwere used to classify them with their known subtypes. \nThe fractal-model-design-based SVM, Bayesian, and \nkNN classifiers achieved an overall classification \naccuracy of 94.12 %, 92.50 % and 79.70 %, \nrespectively, which outperformed classical energy- \nbased selection approaches (86.31 %, 83.19 % and \n51.63 % for the co-occurrence matrix, and 76.01 %, \n73.50 % and 50.69 % for the energy texture signatures; \nrespectively).\n[67]\n2021\nGBM\nH&E and Ki-67-stained histological \nimages\nPropose an unsupervised tissue cluster level graph cut \n(TisCut) method for segmenting histological images \ninto meaningful compartments. TisCut could segment \nnecrotic sections accurately in the dataset, with \ncomparable performance to trained models such as TL- \nInceptionV3, U-Net, U-Net-MA, and U-Net-AS.\n[26]\n2016\nGBM and LGG\nH&E images and clinical data \n(tumour subtype)\nExtraction of spatially localised features of shape, \ncolour and texture from tiled regions covering the \nslide. The tiles were clustered based on their features \nvia Kmeans++ algorithm. The various feature types \nfrom the clusters were normalized and passed through \nthe Elastic Net linear regression model to classify the \nclusters, and weighted voting aggregate the final \ndecision value (GBM vs. LGG) based on the \nclassification of the clusters. Their method was \nevaluated on 203 brain cancer cases with an accuracy \nof 93.1 % and achieved a 100 % accuracy in the 2014 \nMICCAI Pathology Classification Challenge.\n[68]\n2008\nAstrocytomas (low-grade and high-grade)\nH&E images and clinical data \n(tumour subtype)\nExtracted image features were fed into FCM to grade \ntumours (low vs. high). The FCM was designed based \non the histopathologists\u2019 expertise in the Department \nof Pathology, University Hospital of Patras, Greece, \nwhich was then used to accurately grade the brain \ntumours at the time (90.26 % and 93.22 % accuracy on \nlow and high grade, respectively).\n[69]\n2022\nGliomas (with focus on IDH mutation status)\nH&E images\nUsed the dataset to train the CAGAN to normalise \nstains. The aim is to create a network that can \nconsistently normalise stains so that further training of \nimages for other purposes may be more effective. They \nachieved better performance of benchmark algorithms \nby 5\u201310 % compared to baselines not using \nnormalisation.\n[70]\n2021\nGBM and LGG\nH&E images\npresent a lightweight slice-wise CS-Net building on \nnovel hierarchical dimension-decomposed (HDD) \nconvolutions and a novel instance-aware loss for 2D \nand 3D microscopy image segmentation. In the case of \nthe brain tumour images, they were utilised to train the \nnetwork for nuclei segmentation. As a result, the \nnetwork\u2019s nuclei segmentation was able to outperform \nsome of the more common models (including \nvariations of U-Net, HoVer-Net, Mask-RCNN and \nDCAN)\n[52]\n2017\nGBM and LGG\nH&E images\nOutput accurate probability maps of histological \nobjects but also depict clear contours simultaneously \nfor separating clustered object instances, further \nboosting the segmentation performance. GBM and LGG \nimages and ground truths were used to train and \n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n5 \n\nTable 2 (continued)\nAuthors \nPublication \nDate (Year) \nBrain Tumour specified \nData type \nUtilisation of ML\/DL\nvalidate the network\u2019s performance. The network was \nable to outperform most other methods in the 2015 \nMICCAI gland segmentation challenge and the 2015 \nMICCAI nuclei segmentation challenge\n[40]\n2022\nGBM\nH&E images and associated gene \ndata from spatial transcriptomics\nUsing their trained RESEPT pipeline, they could \nsegment and identify a prominent area in a WSI \ncontaining elongated nuclei characteristic of \ninfiltrating GBM cells. These areas matched with the \ninfiltrating tumour marker genes, and RESEPT was \nable to recognise the tumour, non-tumour and \ninfiltrating tumour architecture.\n[55]\n2022\nGBM and LGG\nH&E images and clinical data \n(tumour subtype)\nThey developed a new characterisation approach, \nwhere evolutionary algorithms were used to generate a \nsmall set of features that accurately represent each \ntissue. The aim was to optimize the output of pre- \ntrained deep networks to mitigate computational \nbottlenecks in the form of irrelevant\/redundant \nfeatures. GBM and LGG H&E images were used to train \nthe DNN in this pipeline, and the optimised features \nachieved 93 % classification accuracy.\nKomosin\u2019ski \net al., 2000 \n[71]\n2000\nVarious brain tumours including GBM\nmicroscopic images and clinical \ndata (tumour subtype)\nProposed a pipeline that uses the GA and kNN \nclassifiers to select and weight features in the brain \ntumour images to improve the predictive accuracy \n(astrocytic vs. glial tumours). The best-achieved \nclassification accuracy exceeded 80 %, which was \nconsidered the minimum accuracy needed for medical \ndiagnosis\n[72]\n2021\nHuman-derived Glioblastoma stem cells \n(GSC)\nH&E images and biomarker data\nThe H&E tumour images were used to identify tumour \nregions, which were then trained in an unspecified ML \nmodel using Iba-1, F4\/80 and CD45 markers. \nIncorrectly classified images in the article were \nremoved from the final analysis.\n[36]\n2017\nLGG\nH&E images and overall survival \n(OS), molecular biomarkers and \nclinical factors data\nSVM was used to classify the long and short OS based \non the H&E image features via the bag-of-words (BOW) \nmethodology. The BOW methodology used K-means \nclustering to cluster statistical features extracted from \nthe H&E image patches and created a \"dictionary\" for \nthe SVM to use and classify. The study showed that it \nwas able to discriminate OS with a predictive area \nunder the receiver operating characteristic curve \n(AUC) of 0.76 with the dictionary alone, 0.82 when \nsupplemented with molecular markers, and 0.89 when \nfurther supplemented with other clinical factors\n[46]\n2022\nGBM and LGG\nH&E images\nProposed meta-learning and multi-task learning on the \nU-Net model. This allowed the model to learn more \ngeneralised features and increase its generalisation and \ngeneralizability to unseen data. The H&E images were \nused to train and test the model. Meta-MTL U-Net was \nable to maintain improved performance in nuclei \nsegmentation over decreasing training samples \ncompared to some of the common models (HoVer-Net \nand TripleU models)\n[73]\n2019\nGBM and LGG\nH&E images\nThey presented a CNN model, Micro-Net, to segment \ncells, nuclei and glands in fluorescence and histology \nimages. LGG and GBM H&E images were used to train \nand evaluate the model. It was shown that it was able \nto outperform other recent models (U-Net, SAMS-Net, \nFCN8, etc.) in cell and nuclei segmentation\n[74]\n2022\nGBM and LGG\nH&E images and tumour samples \nfor genomics analysis\nThey developed a MIL model that predicts tumour \npurity in H&E images. The images in the TCGA brain \ntumour were used to train and evaluate the model\u2019s \nperformance. The model was able to predict the \ntumour purity in 8 TCGA cohorts successfully, and the \npredictions correlated very well with the subsequent \ngenomic tumour purity values\n[54]\n2022\nGBM and LGG\nH&E images and matching CNV, \nmutation and RNA-Seq data\nDeveloped the PORPOISE workflow, a weakly \nsupervised MMF DL algorithm to integrate WSI and \nmolecular profile data to perform tasks such as survival \nanalysis. GBM and LGG samples were used as training, \nbut additional LGG samples were used to evaluate their \nperformance. Overall, it was able to accurately predict \nsurvival outcomes and discover features that correlate \nwith poor and favorable outcomes.\n[42]\n2022\nNeuroblastoma\nH&E images, classification and \nprognostic evaluation data\nTo extract their features, they segmented \nneuroblastoma nuclei via the HoVer-Net model (pre- \n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n6 \n\nTable 2 (continued)\nAuthors \nPublication \nDate (Year) \nBrain Tumour specified \nData type \nUtilisation of ML\/DL\ntrained on the PanNuke Dataset). The nucleus and \npatient-level features were then fed into the K-means \nalgorithm to learn visual words (i.e., clustering \ncentres). The features were then further optimised \nbefore it would predict survival and thus calculate the \ndegree of risk to the pathological prognosis. It achieved \nan AUC of 0.946 in the training\/test dataset and an \nAUC of 0.938 in the independent validation dataset, \nthus showing good generalizability.\n[75]\n2018\nHuman-derived GSC\nH&E, magnetic resonance (MR) and \nfluorescence images\nThe software segments images into classes of interest, \nlabelling them into objects of interest. These \nsegmented images were further processed in ImageJ to \nextract labelled area measurements. The labelled areas \nwere used to investigate the invasive pattern of \ntumours\n[31]\n2021\nGBM\nH&E images and proteomics \nanalysis data\nThey used the models for 3 different prediction tasks, \nincluding G-CIMP phenotype, immune response and \ntelomere length. The images were trained in various \nDL CNN models to find the best performance for their \nuse case. Once trained, the model was applied to \ndiscover histological features associated with G-CIMP \nphenotype, telomere length and immune response. \nNMF was used to perform unsupervised clustering of \ntumour samples and to identify proteogenomic \nfeatures.\n[34]\n2022\nMedulloblastoma\nH&E images and associated clinical \nand molecular data\nNuclei in the images were segmented via watershed \nsegmentation, and their features were extracted. The \ntop-performing features were fed into SVM classifiers \nto distinguish molecular subtypes and disease-specific \noutcomes. The overall pipeline was able to distinguish \nthe different molecular subtypes with an AUC of 0.7, \nand survival within Group 3 tumours was predicted \nwith an AUC of 0.92. They proposed that this model \ncan be used to study medulloblastoma genetic \nexpression phenotypes as it could distinguish \nmeaningful features of disease pathology\n[48].\n2022\nGBM\nH&E images, classification, single- \ncell data and spatial transcriptomics \ndata\nTrained VGG16 with the H&E images and \nclassification (infiltrating, necrosis, necrotic edge, \ncellular, vascular) ground truths. The model was then \nused to predict spotwise histological phenotype. \nTranscriptional data were integrated into the \nsegmented image to corroborate with histologic \nestimation. This was used to infer the relationships \nbetween genotypic data with phenotypic observations.\n[76]\n2018\nLGG\nImmunohistochemical (IHC) images\nThey used an ML algorithm that scores CD31 positivity \nby segmenting DAB-positive pixels. Once trained, it \nwas used to segment CD31-positive objects (e.g., \nmicrovessels) and was summed in each ROI.\n[77]\n2022\nGBM and LGG\nH&E images and clinical data \n(tumour subtype)\nThe aim is to use ML algorithms such as SNN to \ndistinguish GBM and LGG tumours in WSIs. Features \nextracted from the segmented nuclei in the H&E \nimages were clustered (100 clusters) via k-means \nclustering, and the cluster centres were then fed into \nSVM, kNN and SNN to compare their performance in \npredicting GBM vs. LGG. The proposed SNN model \noutperformed kNN and SVM models (97.21 % accuracy \ncompared to 93.04 % and 95.13 %, respectively).\n[78]\n2017\nGBM\nH&E images\nIncorporated StackedPSD-KSPM and AlexNet-KSPM \nDL methods for feature extraction and FE-KSPM ML \nmodel for three-category classification (tumour, \nnecrosis and transition to necrosis). The H&E image \npatches were used to train and test the DL models. To \nthis end, the authors found that sparse feature encoders \nand feature extraction strategies based on DL \ntechniques consistently improved the performance of \ntissue histology classification.\n[79]\n2022\nGBM\nIHC images\nA bifocal CNN is used that takes in 2 patches of \ndifferent sizes, and they are each fed into one of 2 \nconvolutional sub-nets; one serves as a feature \nconcatenation module, and the other is a classification \nlayer. This allowed the trained model to assign the \npositivity of CD276 expression by \"halo cells\", and a \nheat map was produced. It achieved an accuracy of \n97.7 %, outperforming other models, such as Resnet- \n50.\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n7 \n\nTable 2 (continued)\nAuthors \nPublication \nDate (Year) \nBrain Tumour specified \nData type \nUtilisation of ML\/DL\n[44]\n2021\nMedulloblastoma\nH&E images and clinical data (i.e. \nsubtypes)\nCreated a pipeline where medulloblastoma images \nwere trained on ResNet-50, MobileNet, and DenseNet- \n201 to extract features. The discrete wavelet transform \n(DWT) method enhanced feature extractions by giving \nspatial-time-frequency representations. This was fed \ninto DCT and PCA methods to fuse and drop redundant \nfeatures. Finally, features were used to classify \nmedulloblastoma subtypes (normal, classic, \ndesmoplastic, large cell, and nodular) via LDA and ESD \nclassifiers (99.4 % accuracy).\n[47]\n2021\nMedulloblastoma\nH&E images and clinical data (i.e. \nsubtypes)\nSimilar to the MB-AI-His pipeline by the same author. \nVarious CNN models were used to train with pediatric \nmedulloblastoma H&E images for feature extraction, \nand optimal features were selected to be classified via \nBi-LSTM. The classification technique is capable of \nbinary (normal vs. medulloblastoma) and multi- \nclassification of medulloblastoma subtypes (normal, \nclassic, desmoplastic, large cell, and nodular) (100 % \nand 99.35 % accuracy for binary- and multi- \nclassification respectively).\n[35]\n2022\nGBM\nIHC images, overall survival and \nassociated miRNA data\nUsing their IHC images, they trained the VGG16 \nnetwork to extract various features optimised for PCA \nanalysis. Together with the associated miRNA and \noverall survival, the features were classified with their \nunspecified classifiers to determine low and high-risk \nGBM groups (AUC = 95 %).\n[18]\n2020\nGliomas\nH&E images and associated IDH \nstatus\nThe tissue images were trained in a MIL CNN model, \nwhere the model takes small patches of the images as \ninstances of the bag and calculates their scores. These \nscores were aggregated to generate the classification \nresult of the IDH1 mutation (positive vs. negative). \nThey were able to achieve an optimum accuracy of \naround 78 %.\n[29]\n2015\nGBM and LGG\nH&E images and clinical data \n(tumour subtype)\nThey proposed an ensemble CNN pipeline, where the \nCNNs have a LeNeT-like architecture. The first CNN \nwill distinguish between GBM and LGG, and any LGG \nclassification results will be fed into the 2nd CNN to \ndetermine between tumour grade II or III. They \nachieved 96 % accuracy in distinguishing GBM and \nLGG while achieving 71 % accuracy in distinguishing \nbetween LGG tumour grades II and III.\n[80]\n2014\nMeningioma (Meningotheliomatous, \nFibroblastic, Transitional, Psammomatous)\nH&E images and clinical data \n(tumour subtype)\nAuthors utilised k-means clustering to segment nuclei \nand used shape-based and texture-based (multilayer \nperceptron) to classify meningioma subtypes. They \nachieved 92.50 % classification accuracy with their \nhybrid classifier technique.\n[45]\n2018\ngliomas (GBM-specified)\nH&E and IHC images\nUtilised CNN-VGG19 to organise histomorphologic \ninformation in brain tumour H&E images to visualise \nmeningioma subtype classifications. t-SNE was used to \ndiscretise the relationship between classes, and their \ndistribution was used to create statistically driven \nclassifications. After tuning, they reduced errors in \nmulti-class classifications to about 4 %.\n[23]\n2017\nMeningioma (Meningotheliomatous, \nFibroblastic, Transitional, Psammomatous)\nH&E images and clinical data \n(tumour subtype)\nThe authors investigated various feature extraction \ntechniques, feature selection techniques and classifiers \nto achieve accurate classification of the various \nmeningioma subtypes. Together with GA evolution, \nthey could classify meningioma subtypes with 94.88 % \naccuracy in their dataset.\n[20]\n2020\nVarious brain tumours including GBM\nStimulated Raman histology (SRH) \nimages and clinical data (tumour \nsubtype)\nUsed ResNet-v2 to extract features and predict the 13 \ndiagnostic classes. T-SNE was used to discretise the \nrelationship between each class based on the extracted \nfeatures. They also implemented semantic \nsegmentation by mapping each image patch\u2019s tumour \nclass probability maps into the WSI. Overall, they were \nable to achieve a 94.6 % classification accuracy \n(Classes included malignant glioma (glioblastoma and \ndiffuse midline glioma, diffuse lower-grade gliomas \n(oligodendrogliomas and diffuse astrocytomas), \npilocytic astrocytoma, ependymoma, lymphoma, \nmetastatic tumours, medulloblastoma, meningioma, \npituitary adenoma, gliosis\/reactive astrocytosis\/ \ntreatment effect, white matter, grey matter and \nnondiagnostic tissue).\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n8 \n\nTable 2 (continued)\nAuthors \nPublication \nDate (Year) \nBrain Tumour specified \nData type \nUtilisation of ML\/DL\n[81].\n2016\nVarious gliomas including GBM\nH&E images and clinical data \n(tumour subtype)\nPresented a patch-based (MIL CNN) together with t- \nSNE based classification. With this pipeline, they were \nable to segment and classify various glioma types \n(GBM, oligodendroglioma (ODG), oligoastrocytoma \n(OC), diffuse astrocytoma (DA), anaplastic \nastrocytoma (AAC), anaplastic oligodendroglioma \n(AODG), LGG with the best accuracy of 77.1 %\n[33]\n2021\nLGG (grade II and III) and GBM (grade IV)\nH&E images, clinical data (tumour \nsubtypes) and associated molecular \ndata\nTrained various glioma images on a range of CNNs to \nclassify ODG vs. non-ODG (identified initially with \nassociated molecular data) and grading diffuse \ngliomas; ResNet50V2, Inception V4, Xception, \nDenseNet201 were used to classify ODG vs. non-ODG \nvia majority voting of patch classifications, while \nMnasNet, EfficientNet-B4, EfficientNet-B5 and \nDenseNet201 CNNs were used to classify diffuse \nglioma grading (grade II-IV).\n[82]\n2021\nGBM, anaplastic astrocytoma (AAC), \nastrocytoma (AC), oligodendroglioma \n(ODG), anaplastic oligodendroglioma \n(AODG)\nH&E images\nProposed the SD-Net-WCE model to classify pathology \nimages into 6 types (ODG, AODG, AC, AAC, GBM and \nnon-tumour). The H&E images were used to train and \nevaluate its performance, and the model was compared \nagainst DenseNet and Inception-FCN models. The SD- \nNet-WCE model outperformed the 2 popular models \nwith an accuracy of 87 %.\n[56]\n2019\nAC, ODG and GBM\nH&E images, clinical (tumour \nsubtype, survival data) and \ngenotyping data\nMainly used H&E images from both databases to train \nthe FCN-GoogLeNet model to train and segment the \nmicrovasculature of the H&E images. The \nmicrovessels\u2019 nuclei were further segmented by \nthresholding. Then its features, such as microvascular \nproliferation, microvascular area, microvessel density, \nwas calculated. Overall, they used this to correlate \nwith glioma subtype (i.e., increased microvascular \ndensity correlated well with GBM subtypes and TERT- \nmut cases). Additionally, their survival analysis \nshowed that microvascular features can be used to \ncluster cases into 2 groups with different survival \nperiods.\n[37]\n2015\nGliomas, but GBM is not specified\nH&E images\nSegmented endothelial and non-endothelial cells in \nglioma brain tumours were fed into the trained random \nforest classifier to evaluate its classification \nperformance. This paper aimed to showcase their \npipeline\u2019s ability to classify endothelial nuclei \naccurately and propose that it can be used to allude to \nthe angiogenesis in brain tumours, which can signal \ndisease progression and a negative prognostic factor.\n[83]\n2017\nLGG\nH&E images and genotyping data\nSimilar to their older paper [37], they set out to use a \nrandom forest classifier to phenotype microvascular \nstructures in LGG H&E images to predict survival. \nAdditionally, they implemented molecular data to \nexplore pathways associated with these phenotypes.\n[38]\n2013\nGBM\nH&E images\nThey used the sparse auto-encoder to learn \nunsupervised intrinsic features in H&E image patches. \nOnce extracted features were used to classify necrosis, \ntransition to necrosis, and viable regions via a multi- \nclass regularised SVM. They were able to map the \nclassified patches onto the WSI, which achieved a \nclassification accuracy of 84 %.\n[84]\n2020\nGBM\nIHC and RNA-seq data\nGenerated a panel of selected biomarkers that can \ndiscriminate between different GBMs (proneural, \nmesenchymal, classical) using RNA-seq data. They \ngenerated a molecular panel via IHC staining with the \npanel and used it with their transcriptomic data to \ntrain the random forest classifier.\n[32]\n2021\nLGG and GBM\nH&E images and molecular data\nUsed U-Net to segment nuclei in H&E images. The \ncellularity from the nuclei segmentations was fed into \nthe ResNet CNN with the molecular information and \nthe feature extracted from the same H&E image to \nclassify glioma subtypes (LGG II, LGG III, HGG). They \nachieved 93.81 % accuracy when classifying between \nLGG and GBM and a 73.95 % accuracy in classifying \nLGG II and LGG III.\n[85]\n2015\nNot specified\nH&E images\nThe authors developed a pipeline where a learned \ndictionary of cell shapes was used to detect cells in the \nH&E image via sparse reconstruction. Then, the \nstacked denoising autoencoder, trained with the \noriginal data and their structured labels, is used to \n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n9 \n\nTable 2 (continued)\nAuthors \nPublication \nDate (Year) \nBrain Tumour specified \nData type \nUtilisation of ML\/DL\nsegment the cells. They could accurately segment \nheavily clustered and abnormally shaped nuclei that \nwould have been very difficult to segment otherwise.\n[86]\n2016\nGBM\nH&E images\nUsing learned dictionaries in their DL method allows \nfor good classification with sparse training data. They \ncould distinguish between a microvascular \nproliferative (MVP) vessel and a non-MVP vessel in \nGBM H&E images. Additionally, the authors showed \nthat DFDL exhibits a more graceful decay in \nclassification accuracy as training images become \nsparser.\n[39]\n2017\nLGG\nH&E images\nTrained SVM and random forest classifiers to \ndistinguish under or over-segmented nuclei in LGG II \nH&E images. The goal is to develop a pipeline that can \nassess nuclei segmentation quality. The authors were \nable to achieve an 84.7 % classification accuracy\n[19]\n2019\nGBM\nDAPI-stained nucleic images\nProposed a modified version of U-Net, which \nimplements gated recruitment units (GRU) to allow for \niterative refinement of feature maps. The DAPI-stained \nGBM nuclei segmentations were used to train this \nmodel. Their model was able to achieve better \nsegmentation performance while using much fewer \nparameters than the competing models (i.e. U-Net and \nDeconv network)\n[87]\n2015\nNot specified\nH&E images\nA CNN was used to segment nuclei in the brain tumour \nH&E images. It was able to outperform other models \nsuch as SVM, random forest and deep belief network\n[22]\n2017\nGBM and LGG\nH&E images and clinical data \n(tumour subtype)\nThe author proposed a CNN -SVM pipeline that can \ndistinguish GBM from LGG H&E images. AlexNet was \nused to extract features from the H&E images, and they \nwere then pooled, selected for, and classified using the \nSVM method. Furthermore, they extended their \nframework to segment the images by modifying the \nSVM to classify positive and negative image patches \nbased on extracted features. A segmentation\/heat map \nwas produced as a result. They achieved accurate \nclassification and segmentation results in the MICCAI \n2014 challenge.\n[49]\n2018\nLGG and GBM\nH&E images and clinical data \n(tumour subtype)\nThe authors created a Deep CNN model to train and \nclassify GBM and LGG H&E images in the TCGA \ndataset. Their model outperformed (96.5 % \nclassification accuracy) their benchmark (LeNet, \nZFNet, VGGNet).\n[41]\n2020\nMixed glioma, ODG, AC and GBM\nH&E images, genotypic and clinical \ndata (survival data)\nProposed the DeepSurvNet model, which predicts the \nsurvival class (class I = 0\u20136 months, II = 6\u201312 months, \nIII = 12\u201324 months, IV = >24 months survival after \ndiagnosis) based on H&E image data. TCGA dataset \nwas used to train the various CNNs (VGG19, \nGoogleNet, ResNet50, InceptionV3 and MobileNetV2) \nand they were evaluated based on their classification \nperformance with locally derived datasets. GoogleNet \noutperformed the other CNNs in the classification of \nsurvival classes. Additionally, they correlated various \nmutated genes with the associated survival classes and \nsuggested that these genetic fingerprints are associated \nwith patient survival.\n[53]\n2021\nGBM\nH&E images, RNA-seq data\nThe authors trained a DenseNet model to semantically \nsegment leading edge (LE), infiltrating tumour (IT), \ncellular tumour (CT), cellular tumour microvascular \nproliferation (CTmvp), cellular tumour \npseudopalisading region around necrosis (CTpan), \ncellular tumour perinecrotic zones (CTpnz), cellular \ntumour necrosis (CTne) and background of the H&E \nimages from the Ivy Gap dataset. These segmented \nfeatures were then correlated with genetic signatures \nassociated with the segmented image. This was \nevaluated on the TCGA datasets. The authors found \nthat the GBM tumour sample had different gene \nsignatures that were driven by different cell types in \nthe tumour microenvironment.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n10 \n\nTable 3 \nDetailed overview of data origin in training and testing datasets, proposed ML\/DL model, training-testing-validation parameters, evaluation metrics,and what other models the proposed models are being compared to.\nAuthors\nPublication \nDate (Year)\nTraining Dataset\nProposed model\nData Processing\ntraining\/test\/validation \nparameters\nAccuracy evaluation \nmetric\nTesting Dataset?\nWhat models are being \ncompared?\n[43].\n2007\nMeningioma tissue from \nneurosurgical resections at \na hospital in Bielefeld, \nGermany\nSelf-Organizing Maps (SOM) \ncombined with Discrete \nWavelet Transform (DWT) for \nfeature selection and \nclustering.\n80H&E images, subdivided \ninto 1280 image tiles \n(256x256 pixels each).\nFour-patients-left-out \ncross-validation, where \none patient per class was \nleft out during training. \nClassification was based on \nmajority vote of the winner \nnode in the SOM.\nCorrect classification as a \npercentage, average of \n79 %\nSame dataset (four- \npatients-left-out \ncross-validation)\nN\/A\n[30]\n2022\nHistopathological images \nand genomic data from \nTCGA-LGG and TCGA- \nGBM projects\nMultiCoFusion: a multi-modal \nfusion framework using \nResNet-152 and sparse graph \nconvolutional network \n(SGCN)\n953 samples for 469 \npatients from TCGA, \nincluding 1024x1024 ROI \nhistopathological images \nand mRNA expression data\n80 % training, 20 % \ntesting, repeated 15 times \nwith alternate training \n(cross-validation) for \nsurvival and grade \nclassification tasks\nC-index of 0.857 for \nsurvival analysis (a \ngeneralisation of AUC for \ncensored data), micro- \nAUC of 0.923 for grade \nclassification. Model \nperformance was \nimproved with the \ninclusion of genomic \nmRNA data\nSame dataset (20 % \nheld-out for testing)\nCompared with \nPathomic Fusion and \ntraditional methods \n(LASSO-Cox, MLP, \nLogistic Regression, \nPWMK)\n[66]\n2015\nFour subtypes of grade I \nmeningioma tissue \nbiopsies from \nneurosurgical resections at \nthe Bethel Department of \nNeurosurgery, Bielefeld, \nGermany\nFractal dimension-based \nmodel with wavelet packet \n(WP) decomposition and \nmachine learning models \n(SVM, Bayesian, k-NN)\n320 images subdivided \ninto 512x512 pixel sub- \nimages after truncation\nLeave-one-patient-out \ncross-validation across 20 \npatients\nClassification accuracy as \na percentage: SVM: \n94.12 %, Bayesian: 76.47 \n%, k-NN: 82.35 %\nSame dataset (leave- \none-patient-out \ncross-validation)\nCompared with Bayesian \nand k-Nearest Neighbors \nclassifiers\n[67]\n2021\nThree datasets: brain \nhistological images (GBM), \nskin melanoma images, \nand lung necrosis images. \nUnspecified where it was \ncollected\nTisCut (Tissue Cluster Level \nGraph Cut) for unsupervised \nsegmentation\nH&E stained brain images \nfor necrosis detection \n(500x5000 pixels)\nNo training, unsupervised \nmethod with graph cut \npartitioning\nBrain tumour-specific \naccuracy: Jaccard-Index- \ncoefficient (JIC): 70.24 \n%, Dice similarity \ncoefficient (DSC): 80.64 \n% for TisCut; BTA-SVM: \nJIC: 55.87 %, DSC: 68.79 \n%; TL-InceptionV3: JIC: \n65.05 %, DSC: 77.05 %; \nU-Net: JIC: 67.22 %, DSC: \n78.58 %\nSame dataset \n(unsupervised \nsegmentation \napplied to the brain \ndataset)\nCompared with BTA- \nSVM, TL-InceptionV3, \nU-Net models\n[26]\n2016\n604 images from TCGA \n(364 GBM, 240 LGG)\nCoarse-to-fine profiling with \nElastic Net classifier for GBM \nvs LGG classification\nH&E stained images, \n1024x1024 tiles used for \nfine profiling after coarse \nclustering\n5-fold cross-validation on \nTCGA dataset (364 GBM, \n240 LGG)\nClassification accuracy \non TCGA dataset: 93.1 %, \nAUC: 0.96 for Elastic Net, \nand 100 % classification \naccuracy on MICCAI \nPathology Challenge \nmodels; Bueno et al.: \nAccuracy: 98.1 %, Chang \nand Parvin: 85.83 %, Xu \net al.: Accuracy: 97.8 %\nSame dataset \n(TCGA) for 5-fold \ncross-validation; \nadditional 45 tissue \nslices used during \ncomparison with \nMICCAI challenge \nmodels\nCompared to manual \nsubsetting methods (e. \ng., Bueno et al., Xu et al.)\n[68]\n2008\n100 Hematoxylin-Eosin \nstained biopsies, classified \ninto low-grade (41 cases) \nand high-grade (59 cases), \nfrom the Department of \nPathology of the \nUniversity Hospital of \nPatras, Greece\nFuzzy Cognitive Maps (FCM) \nwith Activation Hebbian \nLearning (AHL) algorithm\nQualitative assessments of \neight histopathological \nfeatures provided by \npathologists (e.g., \ncellularity, mitoses) of \nH&E images\nThe model was evaluated \non 100 cases, and the \nActivation Hebbian \nLearning (AHL) algorithm \nwas used to optimize the \nFCM. Tumor classification \nwas performed after four \ninteraction cycles.\nClassification accuracy \nLow-grade: 90.26 %, \nHigh-grade: 93.22 %; \nFCM Grading Tool overall \naccuracy: 92 %; \nCompared with ID3 \nDecision Tree: 80 %, J48 \nDecision Tree: 85.71 %, \nSame dataset (100 \nsamples)\nID3 Decision Tree, J48 \nDecision Tree, Fuzzy \nDecision Tree\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n11 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\nFuzzy Decision Tree: 93 \n%\n[69]\n2022\nFour public histopathology \nimage datasets: TCGA-IDH \n(1494 whole-slide images \nfrom 921 glioma patients), \nCAMELYON16 (400 \nslides), CAMELYON17 \n(1000 slides), and \nBreakHis (7909 images)\nColour Adaptive Generative \nNetwork (CAGAN) for stain \nnormalisation using both \nsupervised and unsupervised \nlearning, ResNet34 \nclassification model to classify \nafter stain normalisation\nStain augmentation of \nH&E images with \nsupervised and \nunsupervised stain \nnormalisation using a \nGAN. 1191 samples from \nTCGA-IDH, into \n1024x1024 pixel tiles\nDual-decoder structure, \nsupervised (target domain) \nand unsupervised (source \ndomain), with histogram \nloss. 1191 slides for \ntraining and 154 for \nvalidation and 149 for \ntesting for classification \nmodel\nSSIM (Structural \nSimilarity Index \nMeasure): 0.984 for \nCAGAN, 0.870 for \nMacenko, 0.948 for \nVahadane, PSNR (Peak \nSignal-to-Noise Ratio): \n32.86 for CAGAN, 23.41 \nfor Macenko, 26.14 for \nVahadane. Classification \naccuracy, F1 and AUC: \n0.981, 0.973 and 0.981 \nfor CAGAN method, \n0.938, 0.899 and 0.885 \nfor Macenko method, \n0.908, 0.910 and 0.921 \nfor Vahadane.\nSame datasets used \nfor testing, with \ncross-domain testing \non CAMELYON17 \nafter training on \nCAMELYON16\nCompared to other stain \nnormalisation methods \nlike Macenko, Reinhard, \nVahadane, StainGAN, \nSTST, Tellez et al.\n[70]\n2021\nTwo datasets for \nmitochondria \nsegmentation (EPFL \ndataset and Kasthuri++\ndataset) and the CPM-17 \ndataset for nuclei \nsegmentation\nAuthors proposed CS-Net \n(Derived from U-Net), a \nlightweight deep network for \ncellular segmentation using \nhierarchical dimension- \ndecomposed (HDD) \nconvolutions, with two \nvariants: CS-Net (2D) and CS- \nNet (2.5D++)\n40 pathological slides into \n128x128 image tiles, \nrandomly cropped\n26 train and 14 test split\nDSC and Aggregated \nJaccard Index (AJI): 88.3 \nand 71.1 for CS-Net (2D), \n86.9 and 70.5 for HoVer- \nNet, 85.6 and 59.4 for \nSegNet + WS\nSame dataset (14 \ntest)\nCompared to a large \nrange of models, some \nstate-of-the arts include \nMask-RCNN, U-Net, \nFCN, SegNet, HoVer-Net\n[52]\n2017\nTwo datasets: 2015 \nMICCAI Gland \nSegmentation (GLaS) \nChallenge dataset and \n2015 MICCAI Nuclei \nSegmentation Challenge \ndataset, containing images \nfrom glioblastoma and \nlower-grade glioma tissues\nUsed DCAN (Deep Contour- \nAware Network), a multi-task \nlearning framework \ncombining object and contour \ndetection for instance \nsegmentation of histology \nimages.\nManually segmented \nnuclei from the H&E \nimages in 15 image tiles \nand 18 images\n500 manually segmented \nnuclei in training, 18 \nimages used for evaluation\nDice coefficient (D1) and \nobject-level Dice \ncoefficient (D2): 0.876 \nand 0.748 for proposed \nmodel, 0.826 and 0.694 \nfor Team 2\u2019s model, \n0.792 and 0.642 for Team \n3\u2019s model for the 2015 \nMICCAI nuclei \nsegmentation challenge\nSame dataset (18 \nimages held-out)\nCompared to other \nteams\u2019 model for the \n2015 MICCAI nuclei \nsegmentation challenge\n[40]\n2022\nMultiple datasets \nincluding 10x Genomics \nVisium spatial \ntranscriptomics datasets \non the human and mouse \ncortex, and in-house \nAlzheimer\u2019s disease (AD) \nsamples.\nRESEPT (REconstructing and \nSegmenting Expression \nmapped RGB images based on \nsPatially resolved \nTranscriptomics), which uses \ndeep learning (based on the \nResNet101 architecture) for \ncharacterising tissue \narchitecture by converting \nspatial transcriptomics into \nRGB images and performing \nsegmentation with \nconvolutional neural \nnetworks (CNN).\nThe datasets were \nprocessed by converting \nspatial transcriptomics \ndata into RGB images using \n10x Genomics Visium \ndatasets.\n16-fold Leave-One-Out \nCross-Validation (LOOCV). \nThe final model was \nselected based on the \nMoran\u2019s I autocorrelation \nindex for the testing data\nAdjusted Rand Index \n(ARI) and \nFowlkes\u2013Mallows index \n(FM). RESEPT achieved \nARI: 0.706 and FM: 0.780 \nfor segmentation \naccuracy.\nSame datasets used \nfor training and \ntesting, with Leave- \nOne-Out Cross- \nValidation (LOOCV) \nstrategy\nCompared to Seurat, \nBayesSpace, SpaGCN, \nstLearn, STUtility, \nHMRF, and Giotto\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n12 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\n[55]\n2022\nTCGA dataset, containing \n32,072 whole-slide images \n(WSIs) across 32 primary \ntumor types, including \nGBM (35 WSIs) and LGG \n(39 WSIs)\nA deep neural network (DNN) \nmodel with evolutionary \nfeature selection method that \ncompresses deep feature \nvectors extracted from \ngigapixel histopathology \nimages, reducing the size of \nthese vectors by 11,000 times \nwhile maintaining \nclassification accuracy\nWSIs were preprocessed by \nextracting patches \n(1000x1000 pixels) from \neach WSI\nFeature selection is \nperformed by using an \nevolutionary algorithm \nwith multi-objective \noptimization.\nF1-score for both LGG \nand GBM: 97 % and 97 % \nfor proposed model, 93 % \nand 94 % for PCA, 86 % \nand 87 % for \nAutoencoder model, 91 % \nand 90 % for ANN-based \nfeature selectors\nSame dataset used \nfor both training and \ntesting (TCGA WSIs)\nCompared to PCA, \nAutoencoder, and ANN- \nbased feature selectors\nKomosin\u2019ski \net al., 2000\n2000\nVarious CNS tumours were \ncollected from the \nDepartment of Pathology, \nUniversity School of \nMedicine in Poznan\u2019.\nEvolutionary algorithm with \ngenetic algorithm \noptimization technique for \nfeature selection and \nweighting to improve the \nclassification accuracy (using \nkNN classifier) for CNS \ntumours based on microscopic \nimages.\nMicroscopic images of \nneuroepithelial tumours \nthat are segmented into \napproximately 1300 \nregions (512x512 pixels)\nGenetic algorithm with \nuniform crossover and bit- \nflipping mutation to \noptimize the feature \nweights and improve \nclassification accuracy, \nwith two-level cross- \nvalidation\nClassification accuracy: \n83.43 for astrocytic \ntumours and 77.83 for \nglial tumours\nSame dataset (two- \nlevel cross- \nvalidation)\nN\/A\n[72]\n2021\nIn-house glioblastoma \nstem cells (GSCs) derived \nfrom human and mouse \nmodels. Serial \ntransplantation into \nimmunocompetent BL6 \nmouse models was used to \nmimic the tumor \nmicroenvironment (TME).\n\"Built-in\" machine learning \n(ML) algorithm - model not \nspecified. Part of the Inform \nsoftware package\nH&E stained and GFP \nstained images, and 5 \nrandomly selected regions \nfrom the tumour area. Cell \nmarkers Iba-1, F4\/80 and \nCD45 used to train the \nalgorithm into \nphenotyping these cells\nN\/A\nN\/A\nN\/A\nN\/A\n[36]\n2017\nH&E stained slides from \nthe LGG cohort of The \nCancer Genome Atlas \n(TCGA). The dataset \nincludes Grade II and III \ntumours from 53 patients, \nsplit into two groups based \non short and long overall \nsurvival (OS)\nUsing a bag-of-words (BoW) \nmachine learning approach to \ncreate a visual dictionary of \nimage-derived features \nassociated with overall \nsurvival (OS). A SVM model \nwould then be used to predict \nOS categories\nH&E images were colour \nseparated to H and E \ncomponents using Pipeline \nPilot, H component was \nused to segment the nuclei \nand the image was tiled \ninto 256x256 patches with \n50-pixel overlap. The \nsegmentations had their \nfeature extracted. Various \nclinical data were also \nextracted per patient\nSVM model was trained \nusing a bag-of-words \nrepresentation of image \npatches, clustered into 100 \nvisual words using K- \nmeans clustering. The \noptimal model was \ndetermined through 10- \nfold cross-validation.\nAUC and F1 score: 0.76 \nAUC with machine \nlearned dictionary alone, \nbut 0.82 when \nsupplemented with \nmolecular biomarkers \nand 0.89 when also \nsupplemented with \nclinical factors.\nSame dataset was \nused for training and \ntesting, with 10-fold \ncross-validation.\nN\/A\n[46]\n2022\nTwo public nuceli \nsegmentation dataset were \nused. Monuseg: 30 \nhistopathology images \nfrom 7 tissue types (breast, \nkidney, liver, prostate, \nbladder, colon, and \nstomach). CPM-17: 64 \nhistopathology images, \nincluding images from \nglioblastoma multiforme \n(GBM), lower-grade \nglioma (LGG)\nMeta Multi-Task Learning \n(Meta-MTL) model (Based off \nU-Net architecture) for nuclei \nsegmentation that uses a \nmodel-agnostic meta-learning \n(MAML) algorithm to improve \nmodel generalization with \nfewer training samples\nH&E images cropped into \n256x256 pixels and data \naugmentation applied: \nelastic transformation, \nscaling, shift, rotation and \nflipping\nNo explicit statement on \nhow the dataset was used \nto train and split models \nand whether cross- \nvalidation was used\nDICE coefficient and AJI \nfor over all segmentation \nquality. Panoptic Quality \n(PQ) to evaluate nuclei \nsegmentation of \noverlapping regions: \n0.8756 DICE, 0.7081 AJI \nand 0.6791 PQ in \nablation studies, and \nproposed model \noutperformed the state- \nof-the-art models in \nsparser datasets while \nmaintaining near \nSame dataset used \nfor evaluation\nVarious state-of-the-art \nmodels compared \nincluding: HoVer-Net, \nTripleU, DCAN, Mask R- \nCNN, and others\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n13 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\nidentical performance at \nlarger datasets\n[73]\n2019\nThree datasets were used \nin this article. An in-house \ndataset containing 10 \nmultiplexed fluorescent \nmicroscopy images. CPM \n2017 challenge: 64 images \nwith various tumours \nincluding GBM and LGG. \nThe third dataset used was \nthe Warwick-QU dataset \nfrom the GLaS challenge\nMicro-Net, a model with \nmulti-resolution input and \nbypass layers\nH&E images from CPM \ndataset were cropped into \n300x300 pixels image tiles. \nVarious data augmentation \napplied, such as gaussian \nfilter, rotation and \nflipping.\n32 images were used for \ntraining and another 32 \nimages were used for \ntesting\nDICE, F1, Object Dice \n(OD), Pixel Accuracy and \nObject Hausdorff (OH) \nrespectively: 82.43, \n71.79, 74.12, 63.53, \n27.53 for the proposed \nmodel. State-of-the-arts \nlike U-Net performed \nwith 78.39, 66.43, 67.35, \n80.28, 40.49\nSame dataset used \n(32 images reserved \nin CPM for testing)\nCompared against U- \nNet, FCN8, and DCAN\n[74]\n2022\nTwo datasets were used. \n10 cohorts from the TCGA \ndataset which included \nvarious brain tumours \nincluding GBM and LGG. \nHistologic sections from \nthe East Asian cohort from \nSingapore were also used, \nwhich included 179 lung \nadenocarcinoma patients\nA Multiple instance learning \n(MIL) model (Derived from a \nResNet18 architecture) \ndesigned to predict tumour \npurity from H&E-stained \nhistopathology slides. The \nmodel can learn spatial \ntumour purity distributions \nwithout requiring pixel-level \nannotations, based on \ncorresponding genomic \nsequencing data.\nH&E images cropped into \nnon-overlapping 512x512 \npixel patches and the \ncorresponding tumour \npurity level in the patch \nwas extracted from \nadjacent slide, to be used \nas ground truth labels\n60 % training, 20 % \nvalidation and 20 % testing \ndistribution\nMean absolute error \nbetween purity values \nand model predictions, \nand between purity \nvalues and pathologists\u2019 \n% tumour nuclei \nestimates: GBM and LGG \npredictions achieved \n<0.15 mean absolute \nerror compared to \npathologists\u2019 estimates \nwhile maintaining <0.2 \nwhen compared to \ngenomic purity values\nSame dataset (20 % \nheld-out for testing)\nN\/A\n[54]\n2022\n6592 gigapixel WSIs from \n5720 patient samples \nacross 14 cancer types \nfrom the TCGA (including \nLGG and GBM)\nPathology-Omics Research \nPlatform for Integrative \nSurvival Estimation \n(PORPOISE), which uses a \nmultimodal fusion (MMF) \nalgorithm integrating WSIs \nand molecular profile features \n(mutation status, RNA-seq, \nand copy-number variations)\n512x512 pixel patches \nwere extracted from the \nWSIs, and genomic data \nwas processed from RNA \nsequencing and mutation \nprofiles.\n5-fold cross-validation on \nthe paired WSI-molecular \ndatasets from the 14 cancer \ntypes (GBM inclusive)\nSurvival AUC and cross- \nvalidated concordance \nindex (c-index). MMF \n(WSI and molecular \nfeatures combined) \nmodel: 0.662 AUC and \n0.644 c-index. AMIL \n(WSIs only): 0.615 AUC \nand 0.578 c-index. SNN \n(molecular features \nonly): 0.588 AUC and \n0.606 c-index\nSame dataset (5-fold \ncross validation; \nGBM excluded)\nN\/A\n[42]\n2022\nWSIs of neuroblastoma \nwere retrospectively \nobtained from the \nAffiliated Children\u2019s \nHospital of Xi\u2019an Jiaotong \nUniversity. In total, 563 \nWSIs from 107 patients \nwith neuroblastoma \ncollected\nLogistic regression approach \nto classify neuroblastoma \npatients into favorable \nhistology (FH) and \nunfavorable histology (UH) \ngroups\nHoVer-Net model was used \nto segment nuclei and had \ntheir features extract. 13 \ncell-level morphological \nfeatures and 36 intensity \nfeatures were extracted \nfrom the nuclei in various \ncolor spaces (RGB, Lab, \nHSV, H&E). Patient-level \ndata was also collected and \nused as features\nBootstrap resampling \ntraining, with 80 % \ntraining 20 % test, and \nanother 20 % withheld for \nindependent validation\nAUC, Classification \naccuracy, and Mathhews \nCorrelation Coefficient \n(MCC) respectively. \nProposed model: 0.938, \n0.865 and 0.630. SVM: \n0.913, 0.814 and 0.599. \nSiamese-kNN: 0.910, \n0.909, 0.,828.\nSame dataset (20 % \nheld-out for testing)\nSVM and siamese kNN \nneural netowrk\n[75]\n2018\nIn-House GBM animal \nmodels: Patient-derived \nxenograft (PDX) mouse \nmodel and cdkn2a\u2212\/ \nThe ilastilk software, used to \nsegment image data using \nsupervised random forest \nalgorithms\nN\/A\nN\/A\nN\/A\nN\/A\nN\/A\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n14 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\n\u2212PDGFRB lentivirus- \ninduced mouse GBM \nmodel\n[31]\n2021\nHistopathology slides with \nassociated quantified \nfeatures (cellularity, \nnecrosis, tumor nuclei, \nage, tumour weight) from \nthe Cancer Imaging \nArchive (TCIA) used to \ntrained the model\nTrained various convolutional \nneural networks (InceptionV1 \nto V4, InceptionResNetV1 and \nV2, and self-designed simple \nCNNs) to find the best \nperforming to use in their \nanalysis\nHistopathology images \ncropped into 299x299 \npixel patches, with 49 \npixels of overlap from edge \nto edge. Patches with >30 \n% background were not \nused. Genotypic \ninformation were \nassociated with the \nhistopathology images.\n70 % training 15 % \nvalidation and 15 % testing \nat patient level\nN\/A\nN\/A\nInceptionV1 to V4, \nInceptionResNetV1 and \nV2, and self-designed \nsimple CNNs were tested \nagainst each other\n[34]\n2022\n69H&E medulloblastoma \nformalin-fixed paraffin \nembedded whole-slide \nimages obtained from \npatients under the age of \n18 from the Children\u2019s \nHospital Los Angeles and \ndigitized at University \nHospital\nTested various classifier \nalgorithms trained on nuclear \nhistomorphometric features \nto distinguish various \nmolecular subgroups\nNuclei in 2000x2000 \nimage tiles segmented with \na watershed algorithm, and \nnuclear morphological \nfeatures (texture, shape, \narchitectural \nrearrangement) were \nextracted. Molecular \nsubtypes and survival was \nrecorded for ground truth \nlabelling\n100 iterations of per- \npatient 3-fold cross- \nvalidation training.\nAUC for molecular \nsubtype prediction (0.7 - \nLDA when SHH and WNT \nvs. Group 3 and 4 \ntumours) and AUC for \nsurvival prediction (0.92 \n- neural network for \nGroup 3 survival)\nSame dataset (3-fold \ncross-validation)\n4 models were tested \nand compared (Random \nForest, Neural network, \nSVM and LDA)\n[48].\n2022\nPatients at the Department \nof Neurosurgery of the \nMedical Center, University \nof Freiburg (Freiburg, \nGermany)\nA customised pipeline that \nutilises various models for \nvarious tasks in the article \n(machine-learning-based \nnuclei segmentation with \nilastik, artificial neural \nnetwork to predict tumour \ncontent and a pre-trained \nVGG16 CNN model that will \nbe trained to predict various \nhistological morphology)\nNo clear data processing \nfor the cell detection \nmodel. Performed single- \ncell sequencing, and \ncorresponding spatially \nresolved transcriptomics \nfrom the same donor to \ntrain the ANN. 500x500 \npixel image patches of \nH&E secitons at random \npositions across samples \nfrom all patients were used \nto train the CNN model.\nNo clear training \nparameter provided for the \ncell detection model. 2000 \ntraining spots with 19 \ndifferent cell numbers were \noutlined for the prediciton \nof tumour content. No clear \ntraining parameters were \noutlined for the \nhistomorphological \nprediction model.\nF-score: Ilastik +\nCellprofiler cell detection \n0.841, overall accuracy of \n95 % in predicting \ntumour content and \nsignificant correlation \nbetween training and \nvalidation datasets. \nRefined necrosis \nmorphological \nsegmentation with an F- \nscore of 0.893 when \noverlapped to the \nnecrosis segmentation \nbased on transcriptional \ndata.\nSame dataset\nN\/A\n[76]\n2018\nIn-house TS603 \nsubcutaneous xenograft \ntumour tissues, which are \nmouse glioma models\nMachine-learning algorithm \nin the inForm software was \nused in this article\nN\/A\nN\/A\nN\/A\nN\/A\nN\/A\n[77]\n2022\nTCGA dataset, containing \nH&E WSIs of 599 GBM and \n515 LGG cases, totalling \ninto 239,600 patches for \nGBM and 206,000 patches \nfor LGG tissue\nProposed the shallow neural \nnetwork (SNN) model\nH&E WSIs cropped into \n128x128 pixel tiles, and \ntheir disease classifcation \nwas collected for each tile\n5-fold cross-validation \nmethod was applied\nClassification accuracy, \nPrecision, Recall, \nSpecificity, F1 score, \nrespectively. SNN: 97.21, \n0.9782, 0.9667, 0.9787, \n0.9714. SVM: 95.13, \n0.9194, 0.8903, 0.9241, \n0.9036\nSame dataset (5-fold \ncross validation)\nSVM and kNN models\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n15 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\n[78]\n2017\nThe TCGA dataset was \nused in this article. GBM \nand kidney renal clear cell \ncarcinomas were the two \nspecified diseases used in \nthe TCGA dataset\nProposed models include both \nhuman-engineered features \nand unsupervised feature \nlearning. The study used \ncombinations such as Cellular \nMorphometric Features \n(CMF) and Predictive Sparse \nDecomposition (PSF) with \nKernel-based Spatial Pyramid \nMatching (KSPM) and Sparse \nFeature Encoding (SFE), and \nusing a linear SVM classifier\nH&E stained histology \nimages were processed into \n1000x1000 pixels, and \nfeature extraction includes \nCMF from segmented \nnuclei and Dense\/Salient \nSIFT for other types of \nfeatures\nCross-validation with 10 \niterations, with 160 images \nper category used for \ntraining\nCorrect classification \nrate, reported as the \nmean and standard error. \nCMF-LCDL-LSPM SPE \nmodel reported near 94 % \nclassification rate while a \npre-trained Alex-Net \nbased network can \nachieve similar \nperformance as well\nSame dataset used \n(From cross- \nvaldiation splits)\nDense SIFT (DSIFT), \nDense Color-Texture \n(DCT), and deep \nlearning models like \nAlexNet\n[79]\n2022\nWSIs from Australian \nGenomics and Clinical \nOutcomes of Glioma \n(AGOG) tissue bank, as \nwell as a second cohort \nfrom the Sydney Brain \nTumour Bank\nBifocal Convolutional Neural \nNetwork (BCNN), used for \nautomatic detection, \nprofiling, and counting of \ncells in whole-slide images\nH&E and \nimmunochemically \nlabelled for CD276, \ncropped into 32x32 and \n64x64 pixel patches. Data \naugmentation (rotation, \ncontrast, sharpness) was \napplied to improve model \ngeneralization\n50,714 image patches, split \ninto 36,734 for training \nand 14,005 for testing\nAccuracy, Precision, \nRecall, and F1-Score, \nrespectively. BCNN \nachieved 97.7 for all \nmetrics. Resnet-50 \nachieved 94.0 % for all \nmetrics\nSame dataset \n(14,005 for testing)\nResNet-50 model\n[44]\n2021\nPediatric medulloblastoma \nwas collected from \nGuwahati Medical College \nand Hospital (GMCH) and \nGuwahati Neurological \nResearch Centre (GNRC). \nThe dataset consists of \nimages from 15 children, \nwith a total of 204 images.\nMB-AI-His, a computer-aided \ndiagnosis system that \ncombines deep learning (DL) \nand textural analysis \ntechniques. It utilises three \npre-trained CNNs (ResNet-50, \nDenseNet-201, and \nMobileNet) for spatial feature \nextraction and integrates \nthese with Discrete Wavelet \nTransform (DWT) and \nDiscrete Cosine Transform \n(DCT)\nHistopathological images \nwere resized to 224x224x3 \npixels image patches. \nSpatial features were \nextracted using CNNs, and \ntextural features were \nadded using DWT\n5-fold cross-validation was \nused\nThe accuracy for binary \nclassification reached \nnearly 100 % for ResNet- \n50, DenseNet-201 and \nMobileNet. For multi- \nclass classification, the \nhighest accuracy was \n95.74 % for ResNet-50, \n94.54 % for MobileNet, \nand 97.16 % for \nDenseNet-201\nSame dataset\nResNet-50, DenseNet- \n201 and MobileNet \nmodels were compared \nagainst each other\n[47]\n2021\nPediatric medulloblastoma \nwas collected from \nGuwahati Medical College \nand Hospital (GMCH) and \nGuwahati Neurological \nResearch Centre (GNRC). \nThe dataset consists of \nimages from 15 children, \nwith a total of 204 images.\nCoMB-Deep, a deep learning \npipeline that combines \nfeatures from 10 different \nCNNs (DenseNet-201 +\nInception \nV3 + ResNet-50 + MobileNet \n+ DarkNet-53 +\nNasNetMobile) with the \nDiscrete Wavelet Transform \n(DWT) and uses Bi-Directional \nLong Short-Term Memory (Bi- \nLSTM) for classification\nHistopathological images \nresized appropriately as \ninput to the CNNs for \nfeature extraction, and \nfeatures were added using \nDWT. Data augmentation \n(e.g., flipping, translation, \nscaling, and shearing) \nwasn applied prior to \ntraining\n10-fold cross-validation \nwere used\nClassification accuracy. \nResNet-50 and \nInceptionResNet are \ncapable of 100 % \naccuracy in binary \nclassification, but the \ncombination of CNNs for \nfeature extraction in the \nCoMB-Deep model \nachieved 99.35 % multi- \nclass classification \naccuracy, which superior \nthan end-end single CNN- \nbased classification\nSame dataset (10- \nfold cross \nvalidation)\nShuffleNet, ResNet-50, \nSqueeze, MobileNet, \nInception V3, DenseNet- \n201, Inception-ResNet \nV2, Xception, \nNasNetMobile, DarkNet- \n53\n[35]\n2022\n37 Tissue samples from \nGBM patients from the \nGeneral University \nHospital of Patras, Greece\nHybrid deep learning system \nfor risk stratification of GBM \npatients. The system used \nmachine learning algorithms \nfor feature extraction (VGG16 \nCNN) and data analysis, \nRNA extraction from FFPE \ntissues, followed by qRT- \nPCR to quantify miRNA \nexpression levels. IHC \nimages cropped into \n224x224x3 image patches \n10 iterations of \nbootstrapping, and training \nand validation were \nperformed on the miRNA \nexpression data\nClassification accuracy \nand F1-score. Random \nforest algorithm achieved \nthe highest performance, \nwith an accuracy of \n94.32 %, an F1 score of \nSame dataset\nModels include LDA, \nSVM, kNN and na\u00efve \nBayes\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n16 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\nincluding random forest, LDA, \nSVM, and kNN\nto be fed into the VGG16 \nCNN model for feature \nextraction\n92.82 %, and an AUC of \n97 % in the validation \ndata. Sensitivity for high- \nrisk patients was 95.36 \n%, and specificity for \nlow-risk patients was \n90.67 %.\n[18]\n2020\n1121 histopathology \nimages of glioma patients, \nwith clinical information \ncollected from TCGA \ndataset. 682 images with \nthe IDH1 mutation and \n439 images without the \nmutation\nMultiple-Instance Learning- \nbased Convolutional Neural \nNetwork (MIL-based CNN), a \nCNN to extract features from \nhistopathology images and \nintegrates MIL to classify the \npresence of the IDH1 \nmutation\nHistopathological images \nwere segmented into \nsmaller patches of different \nsizes (32x32, 64x64, \n128x128, 256x256, \n512x512, and full-size) \nbefore being input into the \nCNN\n10-fold cross-validation \ntechnique used to train and \nevaluate the model\nClassification accuracy, \nprecision, recall, AUC. \nThe model achieved an \naccuracy of 78.95 %, a \nprecision of 82.23 %, and \na recall of 83.43 %. The \narea under the curve \n(AUC) was 0.84\nSame dataset (10- \nfold cross \nvalidation)\nN\/A\n[29]\n2015\nDigital pathology images \nof LGG and GBM from the \nTCGA dataset\nEnsemble of CNN to grade \ngliomas\nWSI images were cropped \ninto tiles (1024x1024 \npixels) and only tiles with \ntissue occupying at least \n90 % of the tile were \nselected for further \nanalysis. Nuclei were \nsegmented but not cropped \nindividually to leave it in \nplace in the tile. Patches of \n256x256 were then used \nfor the model\n80 % training and 20 % \ntesting split.\nClassification accuracy, \nprecision, sensitivity, \nspecificity. The model \nachieved a 96 % accuracy \nin classifying GBM and \nLGG, with 0.94, 0.98 and \n0.94 precision, sensitivity \nand specificity in \npredicting the GBM class.\nSame dataset (20 % \nheld-out for testing)\nN\/A\n[80]\n2014\n320 images of meningioma \ntumor samples obtained \nfrom the Bethel \nDepartment of \nNeurosurgery, Bielefeld, \nGermany\nProposed a Hybrid \nclassification technique that \nintegrates texture and shape \ncharacteristics for the \nclassification of four \nmeningioma subtypes. It uses \nGrey-Level Co-occurrence \nMatrix (GLCM) textural \nfeatures and a Multilayer \nPerceptron (MLP) classifier \nfor transitional and \npsammomatous subtypes, \nwhile nuclei shape-based \nfeatures are used for \nmeningothelial and \nfibroblastic subtypes\nH&E images were \nconverted into grayscale \nimages, and nuclei were \nsegmented via K-means \nclustering for feature \nextraction. The shape of \nthe nuclei is used in Phase \n1, and GLCM textural \nfeatures are extracted for \nPhase 2 of the classification \nprocess.\n64 images per subtype used \nfor training and 16 images \nfor testing\nClassification accuracy. \nThe Hybrid classifier \nachieved an average \naccuracy of 92.50 % \nacross five test trials, \nwhich outperformed \nother feature extraction \ntechniques such as \nRADWPT (88 % overall \naccuracy) and WPFD \n(90.31 % overall \naccuracy)\nSame Dataset (16 \ntesting)\nN\/A\n[45]\n2018\nWSIs for their own \nneuropathology service \nwas used, although where \nthe data comes from is not \nspecified. The data \nincludes gliomas, \nmetastatic carcinomas, \nmeningiomas, lymphomas, \nand schwannomas\nThe proposed model is a \nmulti-class Convolutional \nNeural Network (CNN) \nclassifier based on VGG19, \nusing t-distributed stochastic \nneighbour embedding (t-SNE) \nfor dimensionality reduction \nand visualisation of high- \ndimensional data.\nH&E images cropped into \n1024 x 1024-pixel tiles. \nEach tile is classified into \none of the 13 \nneuropathological \ncategories, and \ndimensionality reduction is \nperformed using t-SNE for \nfurther analysis\n85 % of the dataset used for \ntraining and 15 % for \ntesting\nClassification accuracy \nand AUC: The model \nachieved an accuracy of \n94.8 % for the \nclassification of 13 tissue \nand lesion types. The \nAUC of the proposed \nmodel is 0.99. Using t- \nSNE dimensional \nreductionality technique \nbefore classifcaiton \nSame dataset (15 % \nvalidation set)\nN\/A\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n17 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\nyielded better results \nthan using the PCA \ntechnique, but no other \nmodel was used as \ncomparison.\n[23]\n2017\nFour subtypes of grade-I \nbenign meningioma from \nthe Bethel Department of \nNeurosurgery, Bielefeld, \nGermany. Total of 320 \nimages collected\nThe proposed model is an \nevolutionary classification \nframework based on nuclear \nspatial and spectral features. \nThe framework applies a SVM \nclassifier combined with the \ngenetic algorithm to select \noptimal features and classifier \nparameters for meningioma \nsubtype classification\nNuclei isolated using \nadaptive color \nthresholding and \nwatershed transformation. \nVarious morphological, \nintensity, and texture \nfeatures were computed in \nthe RGB color space. \nFeatures were extracted \nfrom the isolated nuclei to \nbe used for training the \nmodel\n5-fold cross validation \ntraining and testing, and \nfeatures were selected \nusing Genetic Algorithm\nThe proposed framework \nachieved an overall \nclassification accuracy of \n94.88 %, while other \nclassifiers like the linear- \nSVM and MLP only \nachieved an overall \nclassification accuracy of \n88.75 % and 85.63 %, \nrespectively\nSame dataset (5-fold \ncross validation)\nLinear SVM, RBF SVM, \nquadratic SVM, MLP, \nRandom Forest, kNN, \nand Naive Bayes\n[20]\n2020\n415 patients from two \ndatasets: University of \nMichigan (UM) images \nfrom a prototype clinical \nStimulated Raman \nHistology (SRH) \nmicroscope; and UM \nimages from one NIO \nImaging System\nProposed an Inception- \nResNet-v2 architecture \nclassification model\nSliding window algorithm \nto crop images into \n300x300 pixels patches \nwith 100 pixel step size. \nImage patches undergo \ncontrast enhancement and \ntransformation to create \nthree-channel input for the \nCNN model. Data \naugmentation is also \napplied to mitigate class \nimbalance, including \ntransformations like \nrotation, shift, and \nreflection\n415 patient training group \nwith 16 patient validation \nset. Performance evaluated \nwith University of Miami \nand Columbia University \ndatasets\nClassification accuracy \nand AUC: The proposed \nmodel achieved an \noverall diagnostic \naccuracy of 94.6 % in the \nexperimental arm of the \ntrial, compared to 93.9 % \naccuracy using \nconventional H&E \nhistology interpreted by \npathologists. The model \nalso achieved an area \nunder the curve (AUC) of \n97 %\nTesting dataset from \ntwo other image \ndatasets: Columbia \nUniversity images \nfrom a second NIO \nImaging System; and \nUniversity of Miami \nimages from a third \nNIO Imaging System\nN\/A\n[81].\n2016\nH&E WSIs of gliomas \n(including GBM) and Non- \nSmall-Cell Lung \nCarcinoma (NSCLC) cases \nfrom the TCGA dataset\nProposed a patch-based \nConvolutional Neural \nNetwork (CNN) combined \nwith an Expectation- \nMaximization (EM)-based \nmodel.\nImage patches from large \nH&E images were \ngenerated, and patches \nwith <30 % or too bloody \nwere discarded. Data \naugmentation (rotation, \nmirror, adjusting H and E \ncomponent of image) was \napplied to each patch\n80 % training 20 % testing \nsplit\nClassification accuracy: \nEM-CNN with logistic \nregression (LR) achieved \nthe best accuracy of 77.1 \n% for gliomas, whereas \nother combinations of \nCNN + classifier, like the \nCNN-SVM, only achieved \n0.697.\nSame dataset (20 % \nwithheld for testing)\nCompared various CNN \n+ classifier combos: \nCNN-Vote, CNN-SMI, \nCNN-Fea-SVM, and \nPretrained CNN-Fea- \nSVM\n[33]\n2021\n468H&E glioma slides \nfrom Catholic University \nof Korea Yeouido St. \nMary\u2019s Hospital from 2017 \nto 2019 during routine \nclinical 1p\/19q \nfluorescence in situ \nhybridization (FISH) test\nThe proposed model is a deep \ntransfer learning model based \non ResNet50V2 for the \nclassification task.\nROI sections of the H&E \nimages were tiled into \n1024x1024 pixels and then \ncropped into 224x224 \npixels when inputted into \nthe model. Random image \naugmentation (e.g., \nflipping, rotation, scaling, \nand Gaussian noise) was \napplied. Molecular \ninformation from FISH was \nprocessed together with \nassociated images for \n70 % training, 10 % \nvalidation, 20 % testing\nThe proposed model \nachieved a balanced \naccuracy of 0.8727 using \nthe majority voting \ntechnique for glioma \nsubtype classification \n(ODG vs. non-ODG). For \ngrading, the best- \nbalanced accuracy was \n0.5801 using the \nMnasNet model for grade \nII, III, and IV gliomas\nSame dataset (20 % \ntesting)\nResNet50V2, \nInceptionV4, Xception, \nDenseNet201, and \nMnasNet\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n18 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\nclassifying \noligodendroglioma and \nnon-oligodendroglioma\n[82]\n2021\n97,252 histopathological \nimages and molecular data \nfrom 323 glioma patients, \ncollected from the Central \nNervous System Disease \nBiobank at Huashan \nHospital, Fudan \nUniversity, Shanghai. The \ndataset includes various \nbrain tumour tissues and \nbackground (non-tumour \ntissues)\n\u201cAI Neuropathologist for \nGlioma\u201d platform is proposed, \nwhich is a Squeeze-and- \nExcitation block DenseNet \n(SD-Net-WCE), which uses \ndeep convolutional neural \nnetworks (CNNs) for the \nclassification of six glioma \nsubtypes. Using additional \nmolecular data, the subtyped \ncases can be further classified \nwith logical algorithms \nsimilar to decision trees\nH&E images were \npreprocessed by resizing \nthem to 512 x 384 pixels \nand undergoing data \naugmentation (e.g., \nflipping, rotation, and \nrandom changes in \nbrightness and contrast)\n6-fold cross-validation \nmethod\nSD-Net-WCE shows the \nbest classification \naccuracy at 87 % \ncompared to DenseNet \nand Inception-FCN. It \nachieved a patch-level \naccuracy of 86.5 % and a \npatient-level accuracy of \n87.5 % for classifying \nglioma subtypes. \nSensitivity and specificity \nacross different subtypes \nare also reported, with \nglioblastoma achieving \n74.6 % and 97.60 % \nsensitivity and \nspecificity, respectively.\nSame dataset (6-fold \ncross-validation)\nDenseNet and Inception- \nFCN\n[56]\n2019\n350 adult patients who \nwere pathologically \ndiagnosed with glioma \nbetween 2012 and 2017 at \nthe Huashan Hospital of \nFudan University. This \nincludes H&E sections \nwith associated genotypic \ndata\nThree types of FCNs (FCN- \nGoogLeNet preferred) were \ntested as a model for \nsegmentation.\nH&E images were cropped \ninto various patches, with \nthe smallest patch being \n204x220 pixels, the largest \npatch being 4594 \u00d7 3718, \nwith the average being \n763 \u00d7 815 pixels. Data \naugmentation techniques \nlike random rotation and \ncolor disruption were \napplied. These images \nwere associated with their \ncorresponding genotypic \ninformation\n58 WSIs for training and \n292 WSIs for testing\nPixel accuracy. FCN- \nGoogLeNet achieved the \nhighest segmentation \nperformance, with a pixel \naccuracy (PA) of 95.7 %, \na mean pixel accuracy \n(MPA) of 54.9 %, and a \nDice coefficient of 95.4 % \nfor microvessel regions\n292 testing sets from \nthe same dataset and \n195 patches from \nthe independent \nTCGA dataset were \nused as an \nindependent \nvalidation set\nFCN-GoogLeNet, FCN- \nVGG, and U-Net\n[37]\n2015\nWSIs from The Cancer \nGenome Atlas (TCGA), \nspecifically focusing on \nglioma brain tumours.\nAn interactive machine \nlearning framework that \nenables users to rapidly build \nclassifiers for histological \nentities using a browser-based \nsystem. The model employs \nactive learning to iteratively \nimprove classifier \nperformance by selecting the \nmost ambiguous samples for \nlabeling by the user. In this \narticle, the random forest \nclassifier is used in the \npipeline\nWSI images were \nsegmented to isolate cell \nnuclei, and a total of 48 \ndescriptive features were \nextracted from each cell to \ndescribe their size, shape, \nand texture. These features \nwere used to build the \nclassifier\n18 sampling iterations, 169 \nsamples used for training \nand 321 independent \ntesting set (cross-validated)\nThe classifier achieved an \nAUC of 0.902. The \nabundance of detected \nendothelial cells was \npositively correlated with \nPECAM-1 expression \n(associated genomic data \nfrom TCGA)\nSame dataset (321 \nwithheld for testing)\nN\/A\n[83]\n2017\n781 (464 tumours) images \nfrom the TCGA LGG \ndataset were used.\nProposes HistomicsML, an \ninteractive machine-learning \nsystem for histopathology \nimage analysis, which uses \nactive learning to efficiently \nclassify histologic objects. The \nWSIs were tiled into \n4096x4096 tiles. Nuclei \nwere segmented from these \ntiles and 48 histomic \nfeatures describing shape, \nintensity and texture\nN\/A\nThe VECN classifier \n(based from the random \nforest classifier) achieved \nan AUC of 0.964\nSame dataset (67 \nslide used to \nvalidate)\nN\/A\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n19 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\nmodel involves iterative \nfeedback between users and \nthe system to train \nclassification rules\n[38]\n2013\nImages of glioblastoma \nmultiforme (GBM) and \nclear cell kidney \ncarcinoma (KIRC) were \nobtained from TCGA. The \nGBM dataset contained \n1400 images, and the KIRC \ndataset contained 2500 \nimages\nThe proposed model uses a \nvariation of the Restricted \nBoltzmann Machine (RBM) \nwith added sparsity \nconstraints for unsupervised \nfeature learning. A SVM \nclassifier is then trained using \nthe learned features to classify \ndifferent tissue types\nWSIs split into 1000x1000 \npixel tiles, and 25x25 pixel \npatches were extracted. \nThese patches would be \nused to learn features and \nthen classified for various \ntissue classes\n4000 patches per tissue \nclass, with cross-validation \nrepeated 100 times\nAn overall classification \naccuracy of 84.3 % was \nachieved when \ndistinguishing between \nnecrosis, transition to \nnecrosis, and tumour in \nGBM\nSame dataset (cross- \nvalidation testing \nsets)\nN\/A\n[84]\n2020\n197 glioblastoma (GBM) \npatients obtained from the \nInstitutional Database of \nSpedali Civili of Brescia. \nAdditionally, RNA \nsequencing analysis was \nperformed on 51 cases\nThey propose an integrated \nmolecular and \nimmunohistochemical (IHC) \napproach combined with a \nmachine-learning algorithm \n(random forest classifier) to \npredict glioblastoma \ntranscriptional subtypes \n(GliTS)\nRNA sequencing was first \ndone to validate gene \nexpression profile, and IHC \nstaining was used to \nquantify these biomarkers. \nHierarchical cluster \nanalysis was performed \nusing Kendall correlation \ncoefficient as similarity \nmetric and Ward criterion, \nand these features will be \nlearned by the model\n39 of 51 cases for testing, \n(assumed 12 cases used to \ntrain classifier)\nConcordance between \nIHC classification and \ntranscriptional data was \nused as a metric of \naccuracy. The predictive \nmodel achieved 79.5 % \noverall concordance, \nwith a concordance of 90 \n% for the mesenchymal \n(MES) subgroup. The \nclassical (CL) subgroup \nshowed 81.3 % \nconcordance, while the \nproneural (PN) subgroup \nhad a lower concordance \nof 69.2 %\nSame dataset (39 \ntesting set)\nN\/A\n[32]\n2021\n549 patient cases from \nTCGA dataset. This \nincludes LGG and HGG \n(GBM). WSIs and \nmolecular data were used\nThe proposed model \nintegrates pathology images \nwith molecular data using a \ndeep neural network (DNN), \nincluding a regular DNN and \nthe ResNet architecture. The \nmodel can incorporate \ncellularity features and \nmolecular information for \nbetter classification\nROIs extracted from H&E \nimages had their nuclei \nsegmented using an Unet \nmodel trained on the \nMonuSeg dataset. \nCellularity features were \ncalculated based on these \nnuclei and passed to the \nmodel together with the \ncolour-normalized stained \ntissue samples and \nassociated molecular \ninformation\n80 % training and 20 % \ntesting using a 5-fold cross- \nvalidation\nWhen classifying \nbetween HGG and LGG, \nthe proposed model \nachieved 90.16 % \nclassification accuracy \nwith the pathology image \nalone, but when \ncombined with genetic \nand cellularity features, \nthe accuracy increased to \n93.81 %. This \nimprovement was also \nseen in grading LGG II \nand III (from 70.69 % to \n73.95 %). This performed \nbetter than most other \nmodels, given that the \nperformance of this \nmodel was trained and \ntested with a much larger \ndataset compared to the \nother models trained and \ntested by other authors\nSame dataset (5-fold \ncross-validation)\nCNN, SVM, ResNet, \nElasticNet classifier, \nDecision tree\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n20 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\n[85]\n2015\nTwo data sets including \nabout 2000 lung tumor \ncells and 1500 brain \ntumour cells. Where the \ndata is collected is \nunspecified.\nUsed sparse reconstruction \nand stacked denoising \nautoencoders (sDAE) to build \na cell segmentation model\nEach cell in in the \nhistopathological images \nwas centralised in a 45x45 \npixel patch for cell \ndetection, and a 28x28 \npixel patch for cell \nsegmentation. Data \naugmentation is applied \n(rotation and random \ntranslation)\nNo clear mention on how \nthe training and testing \ndata was split\nPrimary measure of \naccuracy is F1 score. The \nproposed model achieved \n0.96 F1 score for cell \ndetection and 0.85 F1 \nscore in segmentation \nperformance in the brain \ntumour data, which \noutperformed other state- \nof-the-art methods like \nLaplacian-of-Gaussian \n(LoG), Iterative Radial \nVoting (IRV), ITCN, and \nSingle-Pass Voting (SPV)\nSame dataset (not \nclearly specified)\nLaplacian-of-Gaussian \n(LoG), Iterative Radial \nVoting (IRV), ITCN, and \nSingle-Pass Voting (SPV)\n[86]\n2016\n190 GBM images from the \nTCGA dataset\nDiscriminative Feature- \noriented Dictionary Learning \n(DFDL) method that learns \nclass-specific dictionaries for \nautomatic feature discovery\nPatches were extracted \nfrom the images at 20x20 \npixels, which would be \nvectorized for dictionary \nlearning.\n20 images of 190 images \n(3000x3000 pixel size) \nrandomly picked for \ntraining\nOverall accuracy for the \nTCGA dataset (containing \nGBM images) was 92.85 \n%, which is competitive \ncompared to other \ndictionary learning \nmethods like LC-KSVD \nand FDDL\nSame dataset \n(remaining images \nused to test)\nWND-CHARM, SRC, \nSHIRC, LC-KSVD, and \nFDDL\n[39]\n2017\nImages from The Cancer \nGenome Atlas (TCGA), \nspecifically focusing on \nWHO Grade II lower-grade \nglioma (LGG) and lung \nadenocarcinoma (LUAD) \ncases\nA machine-learning-based \nsemi-automated workflow to \nassess the quality of nucleus \nsegmentation in \nhistopathology images using \ntexture features. The \nmethodology uses a \nclassification model trained \non labelled image patches \nfrom segmentation results\n512x512 pixel patches \nextracted from labelled \nregions, and a set of texture \nand intensity features is \ncomputed from the red, \ngreen, and blue channels. A \nstepwise variable selection \nis applied to reduce \nredundant features\n10-fold cross-validation \ntechnique used to train and \nevaluate the model\nF1 score. Random forest \nclassifier achieved F1 \nscores of 84.71 %, 95.49 \n% and 73.76 % in good, \nunder and over cases, \nrespectively. It was able \nto outperform the SVM \nclassifier in the testing set\nSame dataset (10- \nfold cross \nvalidation)\nRandom forest and SVM \nclassifiers were tested \nagainst each other to see \nwhich one performs \nbetter\n[19]\n2019\n50 maximum intensity \nprojection tissue images of \nglioblastoma cells\nProposed GRUU-Net, which \ncombines the base U-net \narchitecture with Gated \nRecurrent Units (GRU), which \nis capable of multi-scale \nfeature aggregation through \nthe CNN and iterative \nrefinement using GRUs\nNuclei microscopy images \nwere segmented by two \nexperts by drawing \ncontours around them. \nData augmentation was \nperformed\n25 training, 5 validation \nand 20 test images\nobject-wise Jaccard \nsimilarity index (SEG), \nDice and Hausdorff \ndistance. GRUU-Net \nconsistently \noutperformed (0.886 \nDice and 0.648 SEG) in \nnuclei segmentation in \nthe testing datasets \ncompared to other state- \nof-the-art models like U- \nNet and UP-PT\nOwn 20 test images \nand 22 microscopy \nimages from the Cell \nTracking Challenge, \nwhich includes DAPI \nstained cell nuclei in \nGBM tissue\nU-Net and UP-PT, ASPP- \nNet\n[87]\n2015\nUnclear specification of \nwhere the dataset was \ncollected from. Dataset \ninclude brain tumours, \nneuroendocrine tumours \n(NET) and breast cancer \nimages\nProposed a deep \nconvolutional neural network \n(CNN) framework to segment \nnuclei combined with a \nselection-based sparse shape \nmodel and a repulsive \ndeformable model to separate \nindividual nuclei\nSmall patches were \ncropped from the images \n(55x55x3 pixels), and the \nYUV colour space was used \nfor the image \nrepresentation. \nAugmentation techniques \nlike rotation were applied \nto the patches\nNo clear mention on how \nthe training and testing \ndata was split\nDice coefficient (DSC), \nHausdorff Distance (HD), \nand Mean Absolute \nDistance (MAD). The \nproposed model was able \nto segment better than \nmore traditional \nsegmentation techniques \nlike marker-based \nwatershed and graph-cut \nNot clearly specified\nMean shift (MS), \nisoperimetric graph \npartition (ISO), \nsuperpixel (SUP), \nmarker-based watershed \n(MWS), graph-cut and \ncolouring (GCC), and \nrepulsive level set (RLS)\n(continued on next page)\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n21 \n\nTable 3 (continued)\nAuthors \nPublication \nDate (Year) \nTraining Dataset \nProposed model \nData Processing \ntraining\/test\/validation \nparameters \nAccuracy evaluation \nmetric \nTesting Dataset? \nWhat models are being \ncompared?\ncolouring, achieving a \nsuperior DSC of 0.85, HD \nof 5.06 and competitive \nMAD of 3.26\n[22]\n2017\nTwo histopathology image \ndatasets: the MICCAI 2014 \nBrain tumour Digital \nPathology Challenge and a \ncolon cancer dataset. The \nMICCAI dataset consisted \nof 23 GBM and 22 LGG \nimages\nProposed a model in which a \nCNN (based on the AlexNet \narchitecture) is used to extract \nnecessary features to be \npooled, selected and classified \nvia an SVM classifier for \nsegmentation and \nclassification tasks\nOverlapping image patches \nof 336x336 pixels were \ngenerated from H&E \nimages. Patches with \nmajority white background \nwas excluded from \ntraining, and then further \nresized to 224x224 pixels.\n5-fold cross-validation was \nperformed for \nclassification, and leave- \none-out cross-validation \nfor segmentation\nThe proposed model \nachieved 97.5 % \nclassification accuracy in \nthe MICCAI 2014 Brain \ntumour Digital Pathology \nChallenge for \ndistinguishing GBM from \nLGG. For segmentation, \nthe model achieved 84 % \naccuracy in detecting \nnecrosis vs. non-necrosis \nregions\nSame dataset\nCompared against other \nstate-of-the-art methods \nin the training data from \nMICCAI 2014 challenge.\n[49]\n2018\nGBM and LGG \nhistopathological images \nfrom TCGA, with 100 \nimages per glioma subtype \nbeing collected for this \nstudy\nProposed model is a deep CNN \nthat has been to train to \nclassify the glioma subtypes\nH&E images were cropped \ninto 1000x1000 patches, \nand 100 image patches \nwere generated per image.\n4-fold cross-validation \ntechnique to train and test \nmodel\nClassification accuracy. \nThe proposed model \nachieved 96.5 % mean \nclassification accuracy, \nwhich is comparable to \nVGGNet (97 %) and \nbetter than ZFNet (95.2 \n%) and LeNet (79.4 %).\nSame dataset (4-fold \ncross-validation)\nLeNet, ZFNet, and \nVGGNet\n[41]\n2020\nWSIs and clinical data of \n490 brain cancer patients \nfrom the TCGA dataset was \nused.\nDeepSurvNet, a deep CNN \n(Based on GoogleNet) that is \ndesigned to classify brain \ncancer patient survival rates \nbased on histopathological \nimages\nROIs from H&E images had \ntheir patches extracted at \n256x2566, 512x512 and \n1024x1024 pixels.\n80 % training, 18 % \nvalidation, and 2 % testing \nsets, trained and test with 3 \ndifferent testing folds\nThe model achieved \nprecision and AUC of \n0.99 and 1 with 256x256 \npatches and used the \nGoogleNet architecture, \nwhich outperformed \nother state-of-the-art \nmodels like InceptionV3 \nand ResNet50. The best \nmodel also achieved an \naverage precision and \nAUC of 0.8 and 0.96, \nrespectively, with the \nindependent dataset.\nSame dataset (2 % \ntesting set) and \nindependet dataset \nof tissue samples \ncollected from 9 \npatients at SA \nPathology\nVGG19, GoogleNet, \nResNet50, InceptionV3, \nand MobileNetV2\n[53]\n2021\nThe Ivy GAP dataset, \nwhich includes 32 GBM \npatients diagnosed by \nprimary surgery type with \na total of 805 WSIs\nProposed a DenseNet \nsegmentation model (based \non the tiramisu design) to \nsegment various regions in the \ntumour.\nH&E images were resized \nto 1024x1024 pixels, and \npatches of size 512x512 \npixels were extracted for \ntraining. Data \naugmentation techniques \nsuch as random crops and \nvertical flips were applied\nTraining, validation and \ntest sets were split into \n14:1:1 ratio.\nAccuracy score (Rand \nindex). The model \nachieved an overall \naccuracy across ~70 % \nacross all eight classes.\nSame dataset. The \ntrained model was \napplied to the TCGA \ndataset for further \nanalysis with the \nvarious segmented \nregions\nN\/A\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n22 \n\nastrocytomas) and more [20]. Much of the same trend is observed in \nGBM-related studies, suggesting that the studies tended to investigate \nGBM concerning clinical and biological data.\nKnown clinical data, such as overall survival and tumour subtypes, \nwere typically paired with H&E data. Studies utilising the survival +\nH&E data typically model their ML\/DL pipelines to predict patient \nsurvival based on H&E images. In contrast, tumour subtypes\/grades +\nH&E data were used for tumour subtype\/grade classifications. However, \nwhile these studies provide a good tool for accessible and affordable \nprognosis\/diagnosis, they lack additional insights into GBM disease \npathology as they lack additional context from biological data. These \nanalyses were referred to as \u201cTypical\u201d analyses for the rest of the article. \nThis category represents the majority of studies, as 26 articles in this \nreview have been reported to perform typical analysis (Fig. 6). Ertosun \nand Rubin illustrate the methodology of \u201ctypical\u201d analysis as they have \nbuilt an accurate model that could distinguish GBM from LGG, and even \ngrade LGG based on H&E-stained images only [29].\nHowever, several articles (12 out of 54; Fig. 6) performed more \nFig. 3. Distribution of what brain tumour types the studies focused on. Studies were grouped either under GBM-related or GBM-unrelated studies, and further \nstratified to the specified types of brain tumours that has been specified. Data derived from Tables 2 and 3\nFig. 4. Studies were distributed by the dataset they used. The top 3 public dataset were compared against the number of articles that collected their data from \nnon-public dataset, and against articles that did not clearly specify where they collected their data. Abbreviations: Computational Precision Medicine (CPM), \nGenomic Data Commons (GDC), Glioblastoma (GBM), Medical Image Computing and Computer Assisted Intervention (MICCAI), The Cancer Genome Atlas (TCGA). \nData is derived from Table 3.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n23 \n\nunique analyses using additional data sets, such as genomic or proteo\u00ad\nmic data, to provide biological context to the analysis\/prediction, thus \ngaining insight into the disease, particularly for GBM. For example, Tan \nand colleagues used H&E images and corresponding survival and mRNA \nexpression datasets to classify GBM and LGG grades and predict their \noverall survival [30]. Similarly, Wang and colleagues utilised the Gli\u00ad\noma CpG island methylator phenotype (G-CIMP), telomere length and \nimmune response profiles to associate with various histopathological \nfeatures in H&E images. This has provided some much-needed context in \nthe overall classification task, as it allowed Wang and colleagues to find \nassociations in the various immune profile with histologic features in \ntheir predictive model [31]. In addition, several articles in the \u201cunique \nanalysis\u201d have also shown marked improvements in model prediction \ntasks when biological context is given. For instance, Pei and colleagues \ndesigned a model that would predict HGG and LGG and found that by \ncombining the genetic features from genomic profiling and H&E-based \ncellularity features, they were able to enhance the prediction accuracy of \nthe model, compared to a model without genetic features [32]. Simi\u00ad\nlarly, Tan and colleagues reported improved performance of their model \nin predicting long- and short-term overall survival in LGG patients by \nintegrating associated mRNA biomarkers and H&E features collected \nfrom the TCGA dataset, although the selected biomarkers were not \nspecified [30]. This suggests a relationship exists between the \nH&E-derived feature set and mRNA biomarker-derived feature set that \nwould improve predictive capabilities, which the authors can further \nexplore.\nWe also report that the number of articles is spread relatively evenly \nbetween the three categories of analyses within the GBM-related studies, \nindicating (12 \u201cuniquely analysed\u201d studies, 14 \u201ctypically analysed\u201d and \n11 \u201csegmentation\/processing\u201d studies; Fig. 6). Furthermore, most \nstudies categorised as \u201cUnique\u201d are GBM-related (Fig. 6), indicating that \ncontext and biological insight-driven studies in ML\/DL-aided GBM \nresearch are more critical than other brain tumour research. Interest\u00ad\ningly, the oldest article published in the \u201cunique analysis\u201d category is in \n2020, indicating that this type of analysis is relatively novel.\nWe further stratified the articles within the \u201cUnique analysis\u201d cate\u00ad\ngory to investigate how many of them used biological data effectively to \nexplore relationships between it and histological data. We found that \nonly 8 of 12 GBM-related studies within the \u201cUnique analysis\u201d category \nexplored the relationship between biological features and histological \nfeatures to gain insights into GBM pathology, while the other 4 GBM- \nrelated studies only used the biological features as contextual data to \nimprove their prediction model (Fig. 7).\nOverall, studies conducting \u201cunique analysis\u201d are relatively novel. \nIntegrating H&E data, additional -omics data, and clinical data has \nproven critical in better understanding GBM pathology and enhancing \nthe predictive capabilities of ML\/DL models.\n3.4. ML\/DL utilisation\nIn this section, the overarching aim is to ascertain what model ar\u00ad\nchitectures and how these models are commonly used. To achieve this, \nwe collected the type of preferred model architecture used and the task it \nperformed. Moreover, we collected the training parameters, evaluation \nmetrics and the common model architectures used to compare with their \npreferred models (Table 3). This would clearly indicate how stand\u00ad\nardised the training parameters and evaluation methodology are in \nGBM-related studies.\nThe eligible studies utilised ML\/DL techniques for predictive classi\u00ad\nfication and segmentation tasks. Typically, ML classifier and clustering \nalgorithms were used on histopathological data and various clinical data \nto cluster and predict classes, whether it be: (1.) tumour subtypes or \ntumour grades [29,33,34], (2.) predefined survival classes [34\u201336], (3.) \nnuclei types or feature types in the tissue [37,38], and in one case, it was \nFig. 5. The data types used in their studies distributed the number of \narticles. Three categories that the studies were distributed into are: Studies \nonly using H&E (H&E only), studies combining H&E with various data types \n(H&E + other data types) and studies using different histological data types \naltogether (Other). Data is derived from Tables 2 and 3\nFig. 6. The number of studies distributed by the types of ML\/DL-aided \nanalyses. Typical analyses included H&E + tumour subtype\/grade or H&E \n+ survival data. Unique analyses include H&E data combined with various \nother data types (genomic, proteomic, etc.) for their analyses. The remaining \narticles only used ML\/DL techniques to investigate segmentation or processing \nperformance. Data is derived from Tables 2 and 3\nFig. 7. Distribution of articles within the \u201cUnique analysis\u201d category that \neither used biological purely as a \u201cContextual\u201d information without \nfurther analyses, or deeply explored the relationship between to biolog\u00ad\nical data and histological data to offer additional \u201cBiological insights\u201d. \nData derived from Table 3.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n24 \n\nused to determine the quality of nuclei segmentations [39](Table 3). \nMoreover, DL CNN models were commonly used to extract features from \nhistopathological images for both segmentation and classification tasks. \nFor instance, Chang and colleagues developed the RESEPT network \ncontaining the ResNet101 backbone network to segment and identify \nGBM\u2019s characteristic elongated nuclei. They could match these infil\u00ad\ntrative tumour-marking genes, thus allowing them to distinguish \ntumour, non-tumour and infiltrating tumour zones [40]. In another \nexample, Shirazi and colleagues used various CNNs to be trained using \nH&E data and corresponding survival data so that it can accurately \npredict their survival classes [41]. However, the common trend \nobserved from the literature survey was that the CNN architectures were \ncombined with classifiers to create robust models. Liu and colleagues \nimplemented HoVer-Net CNN to first segment neuroblastoma nuclei and \nhave their features extracted. They then implemented K-means clus\u00ad\ntering, a classifier algorithm, to cluster the features and learn visual \nwords (e.g., cluster centres). These were used to predict patient survival \nand thus predict the risk to the pathological prognosis [42].\nIn the analysed studies, some classifier algorithms were used more \noften than others. Classifier\/clustering algorithms include SVM, k \nnearest neighbour (kNN)\/k-Means clustering and random forest (RF) \nclassifiers. In Fig. 8-A, the most common classifiers used in the preferred \nmodels were derivatives of SVM (6 articles overall; Fig. 8-A), kNN\/K- \nMeans clustering (3 articles overall; Fig. 7-A) and RF classifiers (3 arti\u00ad\ncles overall; Fig. 8-A). In the GBM-related studies, the SVM remained the \nmost popular classifier (3 articles; Fig. 8-A).\nOf note are the studies utilising unique techniques to solve their \nclassification problems. In one case, Lessmann and colleagues used the \nself-organizing map (SOM) ML algorithm to cluster certain features in \nmeningioma H&E images and classify their subtypes (Meningothelial, \nFibroblastic, Transitional, Psammomatous)[43]. Several other studies \nutilised less popular algorithms, such as the linear and quadratic \ndiscriminant analysis (LDA and QDA, respectively), na\u00efve Bayes, and \nt-distributed Stochastic Neighbour Embedding (t-SNE) to solve their \nclassification problems [20,23,35,44,45].\nAnother popular DL technique is the CNN architecture. In the rele\u00ad\nvant studies, many authors used the deep residual networks, better \nknown as ResNet, and many of its derivatives, as the primary architec\u00ad\nture for their CNN models. 6 GBM-related studies utilised many versions \nof the ResNet model or built their own derivative model, while the 2nd \nmost popular model used in GBM-related studies was the U-Net model \nand its derivatives; 3 studies featured derivatives of U-Net as their main \nmodel (Fig. 8-B). This was followed closely by the VGG model; 4 overall \nstudies, with 2 GBM-related studies using VGG-derived models as their \npreferred model (Fig. 8-B). Moreover, the U-Net model and its de\u00ad\nrivatives are used primarily for segmentation tasks [19,32,46]. Models \nsuch as the ResNet and VGG derived models tended to be used to extract \nfeatures from the images, and with the model\u2019s decoding feature or with \nthe help of classifiers, it was used for classification tasks [31,45,47,48] \nInterestingly, several studies opted for simpler architectures as their \nmodel of choice. For example, Yonekura and colleagues developed a \ncustom but simple CNN network consisting of 7 convolutional layers. \nThey trained their model on GBM and LGG H&E images to discriminate \nbetween the two types of gliomas [49].\nWe report that SVM and kNN-based classifiers are also often used as \nbenchmark classifiers (Fig. 9-A). At the same time, the ResNet, U-Net \nand Inception-based CNN architectures remain the most common state- \nof-the-art benchmarking model in GBM-related studies (Fig. 9-B). This \nsolidifies our findings that many of the models used in the GBM-related \nresearch have revolved around the refinement of existing and state-of- \nthe-art models in the past 2 decades.\nWhen investigating how the models were trained and evaluated for \ntheir performance, we found that most of the train-evaluation strategies \nthe studies used were variations of the cross-fold validation strategy (23 \narticles with 13 articles being GBM-related; Fig. 10-A). The cross- \nvalidation strategy is considered a gold standard in evaluating the \nrobustness of a model, as it builds validation metrics on several models \ntrained on multiple sub-parts of a training set, known as \u201cfolds\u201d [50]. \nAlthough the gold standard train-evaluation strategy represents a large \nportion of GBM-related studies, the number of studies reporting the \nusage of single train\/test or train\/validation\/test splits is concerning (17 \nout of 37 articles; Fig. 10-A). Even more concerning, 5 out of 37 \nGBM-related articles did not specify how their model\u2019s performance was \nevaluated. This suggests that model robustness may not be at the fore\u00ad\nfront of the ML\/DL model design ethos in GBM-related studies.\nWe also observed that many GBM-related studies used the same \ndataset to evaluate their model, as 30 of 37 GBM-related studies have \nreserved a portion of their training dataset as their testing dataset to \nevaluate their models\u2019 performance (Fig. 10-B). However, 4 GBM- \nrelated studies collected an independent testing dataset to evaluate \ntheir performance. For instance, Shirazi and colleagues collected histo\u00ad\npathological data from 9 patients from a locally derived dataset to \nevaluate the performance of their model, which was trained on the \nTCGA public dataset. They achieved competitive performance in pre\u00ad\ndicting overall survival classes in the locally derived dataset compared \nto the TCGA dataset [41]. This showcased good generalizability in their \nmodel and further strengthened their model design.\nIn Fig. 11, we report the evaluation metrics employed by studies to \nbenchmark their segmentation and classification tasks. We report that \nthe classification accuracy and area under the receiver operating char\u00ad\nacteristic curve (AUC) evaluation metric (16 and 7 GBM-related studies, \nrespectively; Fig. 11) is most used in evaluating classification perfor\u00ad\nmance, and we observe that the two metrics are often paired together. \nSimilarly, the two most common evaluation metric in segmentation \ntasks, the DICE score and Accuracy score (also known as the Rand Index) \n(8 and 7 GBM-related studies found to use these metrics, respectively; \nFig. 11), are often paired together as well. This suggests that the stan\u00ad\ndard pairing of evaluation metrics for segmentation tasks and \nFig. 8. Distribution of popular classifiers (A) and convolutional neural networks (CNN)(B) used either as the main architecture or a base-architecture that \nhas been built upon, in the relevant studies. Abbreviations: Support vector machine (SVM), k nearest neighbour (kNN), random forest (RF). Data is derived \nfrom Table 3.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n25 \n\nclassification tasks should be DICE + Rand Index and Classification \nAccuracy + AUC, respectively.\nA notable observation made in the analysed studies was that many of \nthe studies\u2019 objectives were to evaluate their model performances \nagainst other models. Still, more recent studies tended to apply their \nmodel for novel discoveries or complex biological analyses rather than \nevaluative purposes. For instance, Pei and colleagues utilised the U-Net- \nResNet hybrid model to discern various types of gliomas and had their \nmodel performance evaluated. They used the segmented nuclei features, \nhistopathologic features and IDH statuses for their classification tasks, \nand they evaluated its performance at 93.81 % classification accuracy \nbetween GBM and LGG, and a 73.95 % classification accuracy between \nLGG grade II and III [51]. On the other spectrum, Ravi and colleagues\u2019 \napproach to utilising ML\/DL techniques differs, as they integrated their \nVGG classification model with various other data forms to create a \ncomplex analysis system that investigates how GBM can dynamically \nadapt to various environments. In Ravi\u2019s study, they used a VGG \npre-trained model to classify tumour histologic microenvironments in \nGBM whole slide image (WSI) patches (infiltrating, necrosis, necrotic \nedge, cellular, and vascular microenvironments). The histologic \nphenotype classified by the model was integrated with transcriptomic, \nproteomic and metabolic data to contextualise the histologic phenotype \nand investigate recurring transcriptomic patterns spatially. They \nconsequently characterised GBM at various molecular levels in a \nspatially resolved manner. They discovered that metabolic alterations \nsuch as hypoxia could lead to significant gene copy-number alterations. \nThe data indicated that regional hypoxia metabolism ultimately repre\u00ad\nsents drivers of microevolution that enable the evolution of \ntherapy-resistant phenotypes [48]. The main critique is that although \nRavi and colleagues used ML\/DL tools in a novel way, their scope of \nstudying hypoxic areas may be narrow, considering that GBM exhibits \nhigh inter- and intra-tumoural heterogeneity [5,6]. Overall, this could \nsuggest that although the ML\/DL integration into GBM research is \nnascent, it is slowly evolving and maturing to use these tools for more \ncomplex analyses and, thus, for potentially novel discoveries.\n4. Discussion\nOur literature review found that most relevant articles were pub\u00ad\nlished recently, suggesting that ML\/DL utilisation in histopathological \nbrain tumour research is nascent. Our literature review further \nFig. 9. Distribution of popular state-of-the-art classifiers (A) and convolutional neural networks (CNN)(B) used as a benchmark comparison in relevant \nstudies. Abbreviations: Support vector machine (SVM), k nearest neighbour (kNN), random forest (RF). Data is derived from Table 3.\nFig. 10. Distribution of A.) various training-evaluation strategies of the \nrelevant studies and B.) the type of dataset used to evaluate the ML\/DL \nmodel performance. Data is derived from Table 3.\nFig. 11. Distribution of top 4 evaluation metrics used for segmentation \nand classification tasks. Studies that did not evaluate model performance \nwere omitted from this analysis. Data is derived from Table 3.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n26 \n\ncorroborates this, as many relevant articles mainly evaluated the ML\/DL \nmodels\u2019 performance for various tasks [22,36,52]. Moreover, several \nstudies have explored more complex analyses using ML\/DL techniques \nto make potentially novel discoveries by integrating histological data \nfrom proteomic and genomic studies [41,48,53]. The additional \nbio-information has proven advantageous in improving model perfor\u00ad\nmance when provided in context with histological data [32,54]. Because \nof this, we envisage that studies using ML\/DL-aided complex analyses \nwill become the predominant form of study in the near future as the field \nmatures. However, we also believe there is still considerable work to be \ndone in the field of ML\/DL-aided brain tumour histopathological \nresearch, as the ML\/DL-aided histopathological research resulting in \nbiological insights into GBM are few relative to the evaluative perfor\u00ad\nmance studies in our literature survey.\nFurthermore, many of the relevant studies in our literature review \ninvestigated the general characterisation of brain tumour grades and\/or \nsubtypes [20,40,55], with some studies investigating further via char\u00ad\nacterisation of tumour histologic microenvironments in the WSIs [48,53,\n56]. We believe that ML\/DL-aided investigations into GBM histologic \ntumour microenvironments will become more prominent moving for\u00ad\nward, as GBM is known to be characterised by its high inter-tumoural \nand intra-tumoural heterogeneity [57,58], and this is expressed at \nvarious inter-related levels (histologic, cellular and molecular level). \nNotably, Phillips and colleagues showed that high-grade gliomas (HGG), \nwhich includes GBM, can be characterised into three subtypes using a \npanel of molecular gene expression profiles: proneural, proliferative and \nmesenchymal. They also found a correlation between the molecular \nsubtypes and histologic grading of HGG. Moreover, they reported that \nproneural subtypes tended to have more prolonged survival than the \nproliferative and mesenchymal subtypes [7]. In a more recent study, \nGarofano and colleagues suggested that GBM can be subtyped based on \nbiological traits of single cells and bulk tumours, and they were \ndistributed along the neurodevelopmental and metabolic axis: pro\u00ad\nliferative\/progenitor, neuronal, mitochondrial and glycolytic\/plur\u00ad\nimetabolic subtypes. Moreover, Garofano and colleagues found that \nmitochondrial GBM subtypes were particularly vulnerable to oxidative \nphosphorylation \ninhibitors. \nFinally, \nthey \npostulate \nthat \nthis \npathway-based classification of GBM can inform survival and enable \nmore precise targeting of cancer metabolism [59]. Another study by \nMartinez-Lage and colleagues reported that immune heterogeneity ex\u00ad\nists in different subtypes of GBM and that mesenchymal GBMs showed \nthe highest levels of immune infiltration. In contrast, proneural GBMs \nwere observed to have the least infiltration. Likewise, they reported that \nvarious types of T-lymphocytes are heterogeneously involved in various \nsubtypes, with higher percentages of CD163+ T-cells associated with \nworse prognosis [60]. This showcases that various aspects of GBM play a \nkey role in its pathogenesis and that even within an aspect of GBM, its \nnature can be heterogeneous, thus culminating in many complex re\u00ad\nlationships in GBM that must be accounted for to understand its pa\u00ad\nthology fully.\nRecent studies have given some hope for a better understanding of \nvarious aspects of GBM pathogenesis. Namely, Ravi and colleagues \neffectively integrate ML\/DL segmentation and classification of histo\u00ad\nlogic features with transcriptomic, proteomic and metabolic features to \ncreate a multi-layered and accurate characterisation of necrotic features \nof GBM [48]. Indeed, the mentioned study has constrained its scope to a \nsmaller aspect of GBM, which would not be the best representation of \nGBM. However, we understand its limitations as the investigative efforts \ninto other complex aspects of GBM would require a significant increase \nin expensive multi-omics and resources needed to complete such a \nstudy. This challenge seems to be understood by other studies under\u00ad\ntaking similar integrative research in our literature review, hence the \nrelatively low number of this type of research. Given this limitation, it \nwould still be possible to integrate various \u201csmaller\u201d relationships that \nstudies such as Garofano\u2019s and Martinez-Lage have found and integrate \nhistologic features for the ML\/DL models to learn and predict. With a \nmore collaborative approach, these trained models for various con\u00ad\nstrained relationships can be integrated into a larger \u201cmacro\u201d model. \nThis approach would have to be an iterative approach, as newer dis\u00ad\ncoveries will prompt the development of models to relate these insights \nto histologic features, which would then be embedded into the \u201cmacro\u201d \nmodel. As a result, the \u201cmacro\u201d model would provide the most \nfeature-rich prediction and context to histopathological work, while \nmaintaining a good representation of GBM\u2019s heterogeneous nature. \nAlthough, in practicality, this approach would require a substantial \namount of collaborative effort and investments, we believe it to be a \nnecessary step, in the long term, towards fully understanding GBM pa\u00ad\nthology and providing a comprehensive yet accessible tool for clinicians \nand researchers alike.\nOur literature review also investigated the types of classifiers and \nCNN models used in various brain tumour research. We report that SVM, \nRF and kNN\/k-Means are amongst the most common classifiers today for \nclassification tasks specific to brain tumour research. ResNet and U-Net \nwere the more common CNNs used in our literature review. Our brief \nliterature inquiry suggests that recent studies used the same common ML \nclassifiers we found in our systematic literature review in the scope of \nclassifier tool utilisation. For example, Kang and colleagues used a \ncollection of classifiers, including SVM, RF and kNN, to evaluate their \nperformance in classifying MRI brain tumour images. They found that \nSVM is the optimum classifier for their use case. Interestingly, Kang and \ncolleagues also considered the AdaBoost classifier, an ensemble of \nclassifiers combined to produce more accurate classification outcomes, \nalthough it did not perform as well as SVM [61]. In another example, \nSaha and colleagues developed EMCNet, an automated COVID-19 \ndiagnosis tool with an ensemble of ML classifiers to detect COVID-19 \npresence from patient X-ray images. They combined SVM, decision \ntree and AdaBoost into an ensemble classifier for a more accurate clas\u00ad\nsification performance [62]. Our literature inquiry determined that \nensemble classifiers seemed popular and accurate, and these types of \nclassifiers were not considered as thoroughly in the brain tumour his\u00ad\ntopathology research field. Similarly, as illustrated by Kang and col\u00ad\nleagues, ensemble CNNs can also combine feature extraction layers from \nvarious CNNs (including ResNet) to generate a more accurate classifi\u00ad\ncation performance overall [61]. Upon a literature inquiry into the CNNs \navailable for biomedical applications, although we found studies that \nused derivations of ResNet and U-Net, we also observed a variety of \nmodels that combined ResNet or U-Net into other models. For example, \nUpschulte and colleagues proposed a contour proposal network (CPN), \nwhich can detect overlapping objects in an image, and it could imple\u00ad\nment a U-Net or ResNet backbone into their network. As a result, their \nmodel outperformed basic U-Net and ResNet models in detecting various \nobjects, including neuronal cell bodies, U2OS cell nuclei and synthetic \nshapes [63]. Given the \u201cmacro\u201d model approach that we have suggested, \nwe suggest the implementation of deeper and more robust models such \nas the ResNet backboned models or ensemble models, so that more \nbiological data can be integrated to improve performance and robust\u00ad\nness of the final model.\nThe biggest concern noted in our literature review is that the number \nof studies that failed to report how the dataset was split for training and \nevaluation was larger than expected. Secondarily, a larger-than- \nexpected number of studies forego the evaluation of their model \nrobustness through cross-validation approaches, which is the gold \nstandard in assessing the variance in accuracy predictions [50]. \nFurthermore, most of the articles in this literature review curate an \nevaluative set from the same dataset from which the training set was \nbuilt, albeit the evaluative set does not contain any data that the model \nwould have \u201cseen\u201d in the training set. This could still pose the issue of \ngeneralizability, especially when 14 of the 37 GBM-related studies \ncollect their data from a single source of non-public dataset. Moreover, \nThakkar and colleagues noted in their epidemiologic review of GBM that \nAsian Pacific Islanders have significantly better survival rates than \nwhites and blacks at all time differences and that survival rates between \nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n27 \n\nmen and women differ as well [64]. These population-based differences \nare very important and are not as well represented when a \nmono-sourced dataset is used. Therefore, it is reasonable for clinicians to \nhold the accuracy variance and generalizability of the model to a very \nhigh standard, as the predictions from said model could determine the \ncourse of the patients\u2019 lives. We should uphold these standards so that \nthe performance of our model can instil confidence in our clinicians to \nmake the correct decisions when treating our patients. Lastly, we noted \nthat several articles had poor reporting on where their data is sourced \nand how their models were trained and evaluated, which would result in \npoor model reproducibility. Thus, we suggest: 1.) using public datasets \ncontaining data from multiple institutions in combination with locally \nderived datasets to improve model generalizability, and 2.) A standard \nmethod of reporting includes where the data is sourced from and how \nmany were sourced, how the data was split into cross-validation folds, \nand the results of the accuracy metrics (Classification accuracy and AUC \nfor classification tasks, and DICE and Rand Index scores for segmenta\u00ad\ntion tasks). This would improve reproducibility and ensure a stand\u00ad\nardised reporting of model robustness and generalizability, thus \nfacilitating the collaborative efforts of designing the overarching \n\u201cmacro\u201d model and guaranteeing the adoption of the model in the \nclinical setting.\nWe acknowledge that the scope has constrained the scope to only \nhistopathological studies, but much of the literature in our initial survey \nutilised ML\/DL tools in MRI\/CT patient brain scans for brain tumour \nresearch. While important, there were relatively few ML\/DL-aided his\u00ad\ntopathological studies compared to purely MRI\/CT-based studies, as we \nfound in our literature survey. We see this as a gap in the overall brain \ntumour research as MRI\/CT data are known to be expensive to image \nand collect. In contrast, pathological slides are the standard and more \ncost-effective procedure to classify patient tumour type, grade and po\u00ad\ntential prognosis [65]. Hence, histopathological studies and analyses are \nmore accessible than MRI\/CT data in less privileged areas. Therefore, we \nbelieve that a wider usage of ML\/DL techniques in histopathological \nstudies is important in the future so that more ML\/DL tools for analysing \nhistopathological samples for prognostic and diagnostic purposes are \nmade accessible for clinical use.\n5. Conclusion\nIn conclusion, we have conducted a systematic review of the litera\u00ad\nture concerning ML\/DL-based studies on brain tumours, particularly in \nGBM, and analysed the current trends in the how and why ML\/DL \nmodels were used in histopathological studies. We sought to understand \nhow these ML\/DL-based studies uncovered biological insights towards \nGBM pathology. This would allow us to uncover the cutting-edge ways \nthat studies have used ML\/DL models to learn more about GBM pa\u00ad\nthology, and to find potential improvements in how the ML\/DL-based \nGBM should be conducted.\nWe report that the majority of articles in this review focused on \ndesigning and improving model performance in classifying tumour \ngrades\/subtypes (i.e. classifying between GBM and LGG grades) or \nsegmenting tumour microenvironments (necrotic region, tumour core \netc.), while a small number (8 out of 37 GBM-related studies) imple\u00ad\nmented the models in a way that effectively integrates biological data \nwith histopathological data to gain new insights into GBM pathogenesis. \nWe also found that the two main types of models were utilised in this \nreview were classifier models (SVM and kNN being the most popular \nclassifiers) and CNNs (ResNet and U-Net based architectures are the \nmost popular CNNs).\nHowever, we also uncovered limitations in the articles in this review \nwhich concerned us. Namely, we found that a larger-than-expected \nnumber of articles failed to report where the data was sourced from \nwhich gives rise to reproducibility concerns. Furthermore, over half of \nthe GBM-related studies did not use the gold standard cross-validation \nstrategy and curated evaluation sets from the same dataset as the \ntraining set, which posed a concern towards model robustness and \ngeneralizability, respectively.\nOverall, we postulate that ML\/DL-based GBM research would evolve \nto be more collaborative in nature, to tackle the complexity of the dis\u00ad\nease. This would culminate to the design of a \u201cmacro\u201d model that would \nintegrate various smaller biology-contextualised and histology-based \npredictive models for various aspects of GBM. These suggestions \nbelow are the key actionable steps that would facilitate that evolution. \n\u2022 Integration of genomic or proteomic data in the models with histo\u00ad\nlogical data, by extracting the -omics features (i.e. marker presence, \nshape, texture) and combining it with extracted histological features \n(i.e. nuclei shape, texture, cellularity), and training the model to \npredict the multi-omics features\n\u2022 Designing ensemble models (i.e. SVM-kNN ensemble classifiers, or \nResNet-Inception ensemble CNNs) and deeper CNN models (i.e. \nResNet-backboned U-Net models) to achieve the task. This would \nfacilitate the design of models that can integrate more biological data \nand provide more robust and accurate models overall\n\u2022 Using multiple datasets, preferably one from a publicly available \ndataset like TCGA and a locally derived dataset (i.e. from a hospital \nand local research centre) to train and evaluate the ML\/DL models. \nThis would improve model generalizability and instil confidence in \nclinicians to adopt the models in a clinical setting\n\u2022 Standardised training\/testing\/evaluation splits and standardised \nreporting of it: \no Clear description of where the datasets are acquired and how \nmuch data was collected\no How the data was split for cross-validation\no Clear description and reporting of the evaluation metric (Classi\u00ad\nfication accuracy and AUC for classification tasks, and DICE and \nRand Index scores for segmentation tasks)\nCRediT authorship contribution statement\nChun Kiet Vong: Writing \u2013 review & editing, Writing \u2013 original \ndraft, Validation, Methodology, Investigation, Formal analysis, Data \ncuration, Conceptualization. Alan Wang: Writing \u2013 review & editing, \nSupervision. Mike Dragunow: Writing \u2013 review & editing. Thomas I-H. \nPark: Writing \u2013 review & editing, Supervision. Vickie Shim: Writing \u2013 \nreview & editing, Validation, Supervision.\nEthics statement\nNo ethics required for this systematic review.\nDeclaration of competing interest\nThe authors declare that there are no competing financial and\/or \npersonal interests that would influence the work reported in this article.\nAcknowledgements\nWe acknowledge The University of Auckland\u2019s Post-Graduate \nScholarship for making this work possible.\nAppendix A. Supplementary data\nSupplementary data to this article can be found online at https:\/\/doi. \norg\/10.1016\/j.compbiomed.2024.109642.\nReferences\n[1] H. Ohgaki, P. Kleihues, The definition of primary and secondary glioblastoma, Clin. \nCancer Res. 19 (2013) 764\u2013772, https:\/\/doi.org\/10.1158\/1078-0432.CCR-12- \n3002\/85807\/AM\/THE-DEFINITION-OF-PRIMARY-AND-SECONDARY.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n28 \n\n[2] C. Luo, K. Song, S. Wu, N.U.F. Hameed, N. Kudulaiti, H. Xu, Z.Y. Qin, J.S. Wu, The \nprognosis of glioblastoma: a large, multifactorial study, Br. J. Neurosurg. 35 (2021) \n555\u2013561, https:\/\/doi.org\/10.1080\/02688697.2021.1907306\/SUPPL_FILE\/IBJN_ \nA_1907306_SM6507.DOCX.\n[3] B. Campos, L.R. Olsen, T. Urup, H.S. Poulsen, A comprehensive profile of recurrent \nglioblastoma, Oncogene 35 (2016) 5819\u20135825, https:\/\/doi.org\/10.1038\/ \nonc.2016.85.\n[4] M. Weller, T. Cloughesy, J.R. Perry, W. Wick, Standards of care for treatment of \nrecurrent glioblastoma\u2014are we there yet? Neuro Oncol. 15 (2013) 4, https:\/\/doi. \norg\/10.1093\/NEUONC\/NOS273.\n[5] D.A. Nathanson, B. Gini, J. Mottahedeh, K. Visnyei, T. Koga, G. Gomez, A. Eskin, \nK. Hwang, J. Wang, K. Masui, A. Paucar, H. Yang, M. Ohashi, S. Zhu, J. Wykosky, \nR. Reed, S.F. Nelson, T.F. Cloughesy, C.D. James, P.N. Rao, H.I. Kornblum, J. \nR. Heath, W.K. Cavenee, F.B. Furnari, P.S. Mischel, Targeted therapy resistance \nmediated by dynamic regulation of extrachromosomal mutant EGFR DNA, Science \n343 (2014) 72, https:\/\/doi.org\/10.1126\/SCIENCE.1241328.\n[6] J.M. Stommel, A.C. Kimmelman, H. Ying, R. Nabioullin, A.H. Ponugoti, \nR. Wiedemeyer, A.H. Stegh, J.E. Bradner, K.L. Ligon, C. Brennan, L. Chin, R. \nA. DePinho, Coactivation of receptor tyrosine kinases affects the response of tumor \ncells to targeted therapies, Science 318 (2007) 287\u2013290, https:\/\/doi.org\/10.1126\/ \nSCIENCE.1142946\/SUPPL_FILE\/STOMMEL.SOM.PDF, 1979.\n[7] H.S. Phillips, S. Kharbanda, R. Chen, W.F. Forrest, R.H. Soriano, T.D. Wu, A. Misra, \nJ.M. Nigro, H. Colman, L. Soroceanu, P.M. Williams, Z. Modrusan, B.G. Feuerstein, \nK. Aldape, Molecular subclasses of high-grade glioma predict prognosis, delineate a \npattern of disease progression, and resemble stages in neurogenesis, Cancer Cell 9 \n(2006) 157\u2013173, https:\/\/doi.org\/10.1016\/J.CCR.2006.02.019\/ATTACHMENT\/ \n0B967CEF-1B18-4C7E-90C7-7AF2804EB01D\/MMC7.XLS.\n[8] R.G.W. Verhaak, K.A. Hoadley, E. Purdom, V. Wang, Y. Qi, M.D. Wilkerson, C. \nR. Miller, L. Ding, T. Golub, J.P. Mesirov, G. Alexe, M. Lawrence, M. O\u2019Kelly, \nP. Tamayo, B.A. Weir, S. Gabriel, W. Winckler, S. Gupta, L. Jakkula, H.S. Feiler, J. \nG. Hodgson, C.D. James, J.N. Sarkaria, C. Brennan, A. Kahn, P.T. Spellman, R. \nK. Wilson, T.P. Speed, J.W. Gray, M. Meyerson, G. Getz, C.M. Perou, D.N. Hayes, \nAn integrated genomic analysis identifies clinically relevant subtypes of \nglioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR and NF1, \nCancer Cell 17 (2010) 98, https:\/\/doi.org\/10.1016\/J.CCR.2009.12.020.\n[9] D.N. Louis, A. Perry, G. Reifenberger, A. von Deimling, D. Figarella-Branger, W. \nK. Cavenee, H. Ohgaki, O.D. Wiestler, P. Kleihues, D.W. Ellison, The 2016 World \nHealth organization classification of tumors of the central nervous system: a \nsummary, Acta Neuropathol. 131 (2016) 803\u2013820, https:\/\/doi.org\/10.1007\/ \ns00401-016-1545-1.\n[10] P. Blanc-Durand, A. Van Der Gucht, N. Schaefer, E. Itti, J.O. Prior, Automatic lesion \ndetection and segmentation of 18F-FET PET in gliomas: a full 3D U-Net \nconvolutional neural network study, PLoS One 13 (2018) e0195798, https:\/\/doi. \norg\/10.1371\/journal.pone.0195798.\n[11] A.P. Nanthagopal, R.S. Rajamony, A region-based segmentation of tumour from \nbrain CT images using nonlinear support vector machine classifier, J. Med. Eng. \nTechnol. 36 (2012) 271\u2013277, https:\/\/doi.org\/10.3109\/03091902.2012.682638.\n[12] J. Unkelbach, T. Bortfeld, C.E. Cardenas, V. Gregoire, W. Hager, B. Heijmen, \nR. Jeraj, S.S. Korreman, R. Ludwig, B. Pouymayou, N. Shusharina, J. S\u00a8oderberg, \nI. Toma-Dasu, E.G.C. Troost, E. Vasquez Osorio, The role of computational methods \nfor automating and improving clinical target volume definition, Radiother. Oncol. \n153 (2020) 15\u201325, https:\/\/doi.org\/10.1016\/j.radonc.2020.10.002.\n[13] X. Zhao, Y. Wu, G. Song, Z. Li, Y. Zhang, Y. Fan, A deep learning model integrating \nFCNNs and CRFs for brain tumor segmentation, Med. Image Anal. 43 (2018) \n98\u2013111, https:\/\/doi.org\/10.1016\/j.media.2017.10.002.\n[14] M. Ali, S.O. Gilani, A. Waris, K. Zafar, M. Jamil, Brain tumour image segmentation \nusing deep networks, IEEE Access 8 (2020) 153589\u2013153598, https:\/\/doi.org\/ \n10.1109\/ACCESS.2020.3018160.\n[15] C.G.B. Yogananda, B.R. Shah, M. Vejdani-Jahromi, S.S. Nalawade, G. \nK. Murugesan, F.F. Yu, M.C. Pinho, B.C. Wagner, K.E. Emblem, A. Bj\u00f8rnerud, B. Fei, \nA.J. Madhuranthakam, J.A. Maldjian, A fully automated deep learning network for \nbrain tumor segmentation, Tomography 6 (2020) 186, https:\/\/doi.org\/10.18383\/ \nJ.TOM.2019.00026.\n[16] V. Rajinikanth, S. Kadry, R. Damasevicius, R.A. Sujitha, G. Balaji, M. \nA. Mohammed, Glioma\/glioblastoma detection in brain MRI using pre-trained \ndeep-learning scheme, in: Proceedings of the 2022 3rd International Conference on \nIntelligent Computing, Instrumentation and Control Technologies: Computational \nIntelligence for Smart Systems, ICICICT 2022, 2022, pp. 987\u2013990, https:\/\/doi.org\/ \n10.1109\/ICICICT54557.2022.9917904.\n[17] N.A. Zebari, C.N. Mohammed, D.A. Zebari, M.A. Mohammed, D.Q. Zeebaree, H. \nA. Marhoon, K.H. Abdulkareem, S. Kadry, W. Viriyasitavat, J. Nedoma, \nR. Martinek, A deep learning fusion model for accurate classification of brain \ntumours in Magnetic Resonance images, CAAI Trans Intell Technol 9 (2024) \n790\u2013804, https:\/\/doi.org\/10.1049\/CIT2.12276.\n[18] D. Cui, Y. Liu, G. Liu, L. Liu, A multiple-instance learning-based convolutional \nneural network model to detect the IDH1 mutation in the histopathology images of \nglioma tissues, J. Comput. Biol. 27 (2020) 1264\u20131272, https:\/\/doi.org\/10.1089\/ \ncmb.2019.0410.\n[19] T. Wollmann, M. Gunkel, I. Chung, H. Erfle, K. Rippe, K. Rohr, GRUU-Net: \nintegrated convolutional and gated recurrent neural network for cell segmentation, \nMed. Image Anal. 56 (2019) 68\u201379, https:\/\/doi.org\/10.1016\/j. \nmedia.2019.04.011.\n[20] T.C. Hollon, B. Pandian, A.R. Adapa, E. Urias, A.V. Save, S.S.S. Khalsa, D. \nG. Eichberg, R.S. D\u2019Amico, Z.U. Farooq, S. Lewis, P.D. Petridis, T. Marie, A. \nH. Shah, H.J.L. Garton, C.O. Maher, J.A. Heth, E.L. McKean, S.E. Sullivan, S. \nL. Hervey-Jumper, P.G. Patil, B.G. Thompson, O. Sagher, G.M. McKhann, R. \nJ. Komotar, M.E. Ivan, M. Snuderl, M.L. Otten, T.D. Johnson, M.B. Sisti, J.N. Bruce, \nK.M. Muraszko, J. Trautman, C.W. Freudiger, P. Canoll, H. Lee, S. Camelo-Piragua, \nD.A. Orringer, Near real-time intraoperative brain tumor diagnosis using \nstimulated Raman histology and deep neural networks, Nat. Med. 26 (2020) \n52\u201358, https:\/\/doi.org\/10.1038\/s41591-019-0715-9.\n[21] T. Kurc, S. Bakas, X. Ren, A. Bagari, A. Momeni, Y. Huang, L. Zhang, A. Kumar, \nM. Thibault, Q. Qi, Q. Wang, A. Kori, O. Gevaert, Y. Zhang, D. Shen, M. Khened, \nX. Ding, G. Krishnamurthi, J. Kalpathy-Cramer, J. Davis, T. Zhao, R. Gupta, J. Saltz, \nK. Farahani, Segmentation and classification in digital pathology for glioma \nresearch: challenges and deep learning approaches, Front. Neurosci. 14 (2020) 27, \nhttps:\/\/doi.org\/10.3389\/fnins.2020.00027.\n[22] Y. Xu, Z. Jia, L.-B. Wang, Y. Ai, F. Zhang, M. Lai, E.I.-C. Chang, Large scale tissue \nhistopathology image classification, segmentation, and visualization via deep \nconvolutional activation features, BMC Bioinf. 18 (2017) 281, https:\/\/doi.org\/ \n10.1186\/s12859-017-1685-x.\n[23] K. Fatima, H. Majeed, H. Irshad, Nuclear spatial and spectral features based \nevolutionary method for meningioma subtypes classification in histopathology, \nMicrosc. Res. Tech. 80 (2017) 851\u2013861, https:\/\/doi.org\/10.1002\/jemt.22874.\n[24] D. Moher, A. Liberati, J. Tetzlaff, D.G. Altman, D. Altman, G. Antes, D. Atkins, \nV. Barbour, N. Barrowman, J.A. Berlin, J. Clark, M. Clarke, D. Cook, R. D\u2019Amico, J. \nJ. Deeks, P.J. Devereaux, K. Dickersin, M. Egger, E. Ernst, P.C. G\u00f8tzsche, \nJ. Grimshaw, G. Guyatt, J. Higgins, J.P.A. Ioannidis, J. Kleijnen, T. Lang, \nN. Magrini, D. McNamee, L. Moja, C. Mulrow, M. Napoli, A. Oxman, B. Pham, \nD. Rennie, M. Sampson, K.F. Schulz, P.G. Shekelle, D. Tovey, P. Tugwell, Preferred \nreporting Items for systematic reviews and meta-analyses: the PRISMA statement, \nPLoS Med. 6 (2009), https:\/\/doi.org\/10.1371\/JOURNAL.PMED.1000097.\n[25] L.M. Kmet, L.S. Cook, R.C. Lee, Standard quality assessment criteria for evaluating \nprimary research papers from a variety of fields. https:\/\/doi.org\/10.7939\/ \nR37M04F16, 2004.\n[26] J. Barker, A. Hoogi, A. Depeursinge, D.L. Rubin, Automated classification of brain \ntumor type in whole-slide digital pathology images using local representative tiles, \nMed. Image Anal. 30 (2016) 60\u201371, https:\/\/doi.org\/10.1016\/j. \nmedia.2015.12.002.\n[27] S. Mohammed, M. Dinesan, T. Ajayakumar, Survival and quality of life analysis in \nglioblastoma multiforme with adjuvant chemoradiotherapy: a retrospective study, \nRep. Practical Oncol. Radiother. 27 (2022) 1026, https:\/\/doi.org\/10.5603\/RPOR. \nA2022.0113.\n[28] M.J. Tait, V. Petrik, A. Loosemore, B.A. Bell, M.C. Papadopoulos, Survival of \npatients with glioblastoma multiforme has not improved between 1993 and 2004: \nanalysis of 625 cases, Br. J. Neurosurg. 21 (2007) 496\u2013500, https:\/\/doi.org\/ \n10.1080\/02688690701449251.\n[29] M.G. Ertosun, D.L. Rubin, Automated grading of gliomas using deep learning in \ndigital pathology images: a modular approach with ensemble of convolutional \nneural networks, AMIA Annu Symp Proc 2015 (2015) 1899\u20131908.\n[30] K. Tan, W. Huang, X. Liu, J. Hu, S. Dong, A multi-modal fusion framework based on \nmulti-task correlation learning for cancer prognosis prediction, Artif. Intell. Med. \n126 (2022), https:\/\/doi.org\/10.1016\/j.artmed.2022.102260.\n[31] L.B. Wang, A. Karpova, M.A. Gritsenko, J.E. Kyle, S. Cao, Y. Li, D. Rykunov, \nA. Colaprico, J.H. Rothstein, R. Hong, V. Stathias, M. Cornwell, F. Petralia, Y. Wu, \nB. Reva, K. Krug, P. Pugliese, E. Kawaler, L.K. Olsen, W.W. Liang, X. Song, Y. Dou, \nM.C. Wendl, W. Caravan, W. Liu, D. Cui Zhou, J. Ji, C.F. Tsai, V.A. Petyuk, J. Moon, \nW. Ma, R.K. Chu, K.K. Weitz, R.J. Moore, M.E. Monroe, R. Zhao, X. Yang, S. Yoo, \nA. Krek, A. Demopoulos, H. Zhu, M.A. Wyczalkowski, J.F. McMichael, B. \nL. Henderson, C.M. Lindgren, H. Boekweg, S. Lu, J. Baral, L. Yao, K.G. Stratton, L. \nM. Bramer, E. Zink, S.P. Couvillion, K.J. Bloodsworth, S. Satpathy, W. Sieh, S. \nM. Boca, S. Sch\u00fcrer, F. Chen, M. Wiznerowicz, K.A. Ketchum, E.S. Boja, C. \nR. Kinsinger, A.I. Robles, T. Hiltke, M. Thiagarajan, A.I. Nesvizhskii, B. Zhang, D. \nR. Mani, M. Ceccarelli, X.S. Chen, S.L. Cottingham, Q.K. Li, A.H. Kim, D. Feny\u00a8o, K. \nV. Ruggles, H. Rodriguez, M. Mesri, S.H. Payne, A.C. Resnick, P. Wang, R.D. Smith, \nA. Iavarone, M.G. Chheda, J.S. Barnholtz-Sloan, K.D. Rodland, T. Liu, L. Ding, \nA. Agarwal, M. Amin, E. An, M.L. Anderson, D.W. Andrews, T. Bauer, C. Birger, M. \nJ. Birrer, L. Blumenberg, W.E. Bocik, U. Borate, M. Borucki, M.C. Burke, S. Cai, A. \nP. Calinawan, S.A. Carr, S. Cerda, D.W. Chan, A. Charamut, L.S. Chen, D. Chesla, A. \nM. Chinnaiyan, S. Chowdhury, M.P. Cie\u00b4slik, D.J. Clark, H. Culpepper, T. Czernicki, \nF. D\u2019Angelo, J. Day, S. De Young, E. Demir, S.M. Dhanasekaran, R. Dhir, M. \nJ. Domagalski, B. Druker, E. Duffy, M. Dyer, N.J. Edwards, R. Edwards, K. Elburn, \nM.J. Ellis, J. Eschbacher, A. Francis, S. Gabriel, N. Gabrovski, L. Garofano, G. Getz, \nM.A. Gillette, A.K. Godwin, D. Golbin, Z. Hanhan, L.I. Hannick, P. Hariharan, \nB. Hindenach, K.A. Hoadley, G. Hostetter, C. Huang, E. Jaehnig, S.D. Jewell, N. Ji, \nC.D. Jones, A. Karz, W. Kaspera, L. Kim, R.B. Kothadia, C. Kumar-Sinha, J. Lei, F. \nD. Leprevost, K. Li, Y. Liao, J. Lilly, H. Liu, J. Lub\u00ednski, R. Madan, W. Maggio, \nE. Malc, A. Malovannaya, S. Mareedu, S.P. Markey, A. Marrero-Oliveras, \nN. Martinez, N. Maunganidze, J.E. McDermott, P.B. McGarvey, J. McGee, \nP. Mieczkowski, S. Migliozzi, F. Modugno, R. Montgomery, C.J. Newton, G. \nS. Omenn, U. Ozbek, O.V. Paklina, A.G. Paulovich, A.M. Perou, A.R. Pico, P. \nD. Piehowski, D.G. Placantonakis, L. Polonskaya, O. Potapova, B. Pruetz, L. Qi, \nS. Ramkissoon, A. Resnick, S. Richey, G. Riggins, K. Robinson, N. Roche, D. \nC. Rohrer, B.R. Rood, L. Rossell, S.R. Savage, E.E. Schadt, Y. Shi, Z. Shi, Y. Shutack, \nS. Singh, T. Skelly, L.J. Sokoll, J. Stawicki, S.E. Stein, J. Suh, W. Szopa, D. Tabor, \nD. Tan, D. Tansil, R.R. Thangudu, C. Tognon, E. Traer, S. Tsang, J. Tyner, K.S. Um, \nD.R. Valley, S. Vasaikar, N. Vatanian, U. Velvulou, M. Vernon, W. Wan, J. Wang, \nA. Webster, B. Wen, J.R. Whiteaker, G.D. Wilson, Y. Zakhartsev, R. Zelt, H. Zhang, \nL. Zhang, Z. Zhang, G. Zhao, J. Zhu, Proteogenomic and metabolomic \ncharacterization of human glioblastoma, Cancer Cell 39 (2021) 509\u2013528.e20, \nhttps:\/\/doi.org\/10.1016\/j.ccell.2021.01.006.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n29 \n\n[32] Linmin Pei, K.A. Jones, Z.A. Shboul, J.Y. Chen, K.M. Iftekharuddin, Deep neural \nnetwork analysis of pathology images with integrated molecular data for enhanced \nglioma classification and grading, Front. Oncol. 11 (2021) 668694, https:\/\/doi. \norg\/10.3389\/fonc.2021.668694.\n[33] S. Im, J. Hyeon, E. Rha, J. Lee, H.-J. Choi, Y. Jung, T.-J. Kim, Classification of \ndiffuse glioma subtype from clinical-grade pathological images using deep transfer \nlearning, Sensors 21 (2021) 3500, https:\/\/doi.org\/10.3390\/s21103500.\n[34] J. Whitney, L. Dollinger, B. Tamrazi, D. Hawes, M. Couce, J. Marcheque, \nA. Judkins, A. Margol, A. Madabhushi, Quantitative nuclear histomorphometry \npredicts molecular subtype and clinical outcome in medulloblastomas: preliminary \nfindings, J. Pathol. Inf. 13 (2022) 100090, https:\/\/doi.org\/10.1016\/j. \njpi.2022.100090.\n[35] V. Bafiti, S. Ouzounis, C. Chalikiopoulou, E. Grigorakou, I.M. Grypari, \nG. Gregoriou, A. Theofanopoulos, V. Panagiotopoulos, E. Prodromidi, D. Cavouras, \nV. Zolota, D. Kardamakis, T. Katsila, A 3-miRNA signature enables risk \nstratification in glioblastoma multiforme patients with different clinical outcomes, \nCurr. Oncol. 29 (2022) 4315\u20134331, https:\/\/doi.org\/10.3390\/curroncol29060345.\n[36] R.T. Powell, A. Olar, S. Narang, G. Rao, E. Sulman, G.N. Fuller, A. Rao, \nIdentification of histological correlates of overall survival in lower grade gliomas \nusing a bag-of-words paradigm: a preliminary analysis based on hematoxylin & \neosin stained slides from the lower grade glioma cohort of the cancer genome \nAtlas, J. Pathol. Inf. 8 (2017), https:\/\/doi.org\/10.4103\/jpi.jpi_43_16.\n[37] M. Nalisnik, D.A. Gutman, J. Kong, L.A.D. Cooper, An interactive learning \nframework for scalable classification of pathology images, in: 2015 IEEE \nInternational Conference on Big Data (Big Data), IEEE, 2015, pp. 928\u2013935, https:\/\/ \ndoi.org\/10.1109\/BigData.2015.7363841.\n[38] N. Nayak, H. Chang, A. Borowsky, P. Spellman, B. Parvin, Classification of tumor \nhistopathology via sparse feature learning, in: Proc IEEE Int Symp Biomed Imaging \n2013, 2013, https:\/\/doi.org\/10.1109\/ISBI.2013.6556499.\n[39] S. Wen, T.M. Kurc, Y. Gao, T. Zhao, J.H. Saltz, W. Zhu, A methodology for texture \nfeature-based quality assessment in nucleus segmentation of histopathology image, \nJ. Pathol. Inf. 8 (2017) 38, https:\/\/doi.org\/10.4103\/jpi.jpi_43_17.\n[40] Y. Chang, F. He, J. Wang, S. Chen, J. Li, J. Liu, Y. Yu, L. Su, A. Ma, C. Allen, Y. Lin, \nS. Sun, B. Liu, J. Javier Otero, D. Chung, H. Fu, Z. Li, D. Xu, Q. Ma, Define and \nvisualize pathological architectures of human tissues from spatially resolved \ntranscriptomics using deep learning, Comput. Struct. Biotechnol. J. 20 (2022) \n4600\u20134617, https:\/\/doi.org\/10.1016\/j.csbj.2022.08.029.\n[41] A.Z. Shirazi, E. Fornaciari, N.S. Bagherian, L.M. Ebert, B. Koszyca, G.A. Gomez, \nDeepSurvNet: deep survival convolutional network for brain cancer survival rate \nclassification based on histopathological images, Med. Biol. Eng. Comput. 58 \n(2020) 1031\u20131045, https:\/\/doi.org\/10.1007\/s11517-020-02147-3.\n[42] Y. Liu, Y. Jia, C. Hou, N. Li, N. Zhang, X. Yan, L. Yang, Y. Guo, H. Chen, J. Li, \nY. Hao, J. Liu, Pathological prognosis classification of patients with neuroblastoma \nusing computational pathology analysis, Comput. Biol. Med. 149 (2022) 105980, \nhttps:\/\/doi.org\/10.1016\/j.compbiomed.2022.105980.\n[43] B. Lessmann, T.W. Nattkemper, V.H. Hans, A. Degenhard, A method for linking \ncomputed image features to histological semantics in neuropathology, J. Biomed. \nInf. 40 (2007) 631\u2013641, https:\/\/doi.org\/10.1016\/j.jbi.2007.06.007.\n[44] O. Attallah, MB-AI-His: histopathological diagnosis of pediatric medulloblastoma \nand its subtypes via AI, Diagnostics 11 (2021) 359, https:\/\/doi.org\/10.3390\/ \ndiagnostics11020359.\n[45] K. Faust, Q. Xie, D. Han, K. Goyle, Z. Volynskaya, U. Djuric, P. Diamandis, \nVisualizing histopathologic deep learning classification and anomaly detection \nusing nonlinear feature space dimensionality reduction, BMC Bioinf. 19 (2018) \n173, https:\/\/doi.org\/10.1186\/s12859-018-2184-4.\n[46] C. Han, H. Yao, B. Zhao, Z. Li, Z. Shi, L. Wu, X. Chen, J. Qu, K. Zhao, R. Lan, \nC. Liang, X. Pan, Z. Liu, Meta multi-task nuclei segmentation with fewer training \nsamples, Med. Image Anal. 80 (2022), https:\/\/doi.org\/10.1016\/j. \nmedia.2022.102481.\n[47] O. Attallah, CoMB-deep: composite deep learning-based pipeline for classifying \nchildhood medulloblastoma and its classes, Front. Neuroinf. 15 (2021), https:\/\/ \ndoi.org\/10.3389\/fninf.2021.663592.\n[48] V.M. Ravi, P. Will, J. Kueckelhaus, N. Sun, K. Joseph, H. Sali\u00b4e, L. Vollmer, \nU. Kuliesiute, J. von Ehr, J.K. Benotmane, N. Neidert, M. Follo, F. Scherer, J. \nM. Goeldner, S.P. Behringer, P. Franco, M. Khiat, J. Zhang, U.G. Hofmann, C. Fung, \nF.L. Ricklefs, K. Lamszus, M. Boerries, M. Ku, J. Beck, R. Sankowski, \nM. Schwabenland, M. Prinz, U. Sch\u00fcller, S. Killmer, B. Bengsch, A.K. Walch, \nD. Delev, O. Schnell, D.H. Heiland, Spatially resolved multi-omics deciphers \nbidirectional tumor-host interdependence in glioblastoma, Cancer Cell 40 (2022) \n639\u2013655.e13, https:\/\/doi.org\/10.1016\/j.ccell.2022.05.009.\n[49] A. Yonekura, H. Kawanaka, V.B.S. Prasath, B.J. Aronow, H. Takase, Automatic \ndisease stage classification of glioblastoma multiforme histopathological images \nusing deep convolutional neural network, Biomed Eng Lett 8 (2018) 321\u2013327, \nhttps:\/\/doi.org\/10.1007\/s13534-018-0077-0.\n[50] R. Lasfar, G. T\u00b4oth, The difference of model robustness assessment using cross- \nvalidation and bootstrap methods, J. Chemom. 38 (2024) e3530, https:\/\/doi.org\/ \n10.1002\/CEM.3530.\n[51] L. Pei, K.A. Jones, Z.A. Shboul, J.Y. Chen, K.M. Iftekharuddin, Deep neural network \nanalysis of pathology images with integrated molecular data for enhanced glioma \nclassification and grading, Front. Oncol. 11 (2021), https:\/\/doi.org\/10.3389\/ \nfonc.2021.668694.\n[52] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, P.A. Heng, DCAN: deep contour-aware \nnetworks for object instance segmentation from histology images, Med. Image \nAnal. 36 (2017) 135\u2013146, https:\/\/doi.org\/10.1016\/j.media.2016.11.004.\n[53] A.Z. Shirazi, M.D. McDonnell, E. Fornaciari, N.S. Bagherian, K.G. Scheer, M. \nS. Samuel, M. Yaghoobi, R.J. Ormsby, S. Poonnoose, D.J. Tumes, G.A. Gomez, \nA deep convolutional neural network for segmentation of whole-slide pathology \nimages identifies novel tumour cell-perivascular niche interactions that are \nassociated with poor survival in glioblastoma, Br. J. Cancer 125 (2021) 337\u2013350, \nhttps:\/\/doi.org\/10.1038\/s41416-021-01394-x.\n[54] R.J. Chen, M.Y. Lu, D.F.K. Williamson, T.Y. Chen, J. Lipkova, Z. Noor, M. Shaban, \nM. Shady, M. Williams, B. Joo, F. Mahmood, Pan-cancer integrative histology- \ngenomic analysis via multimodal deep learning, Cancer Cell 40 (2022) 865\u2013878.e6, \nhttps:\/\/doi.org\/10.1016\/j.ccell.2022.07.004.\n[55] A.A. Bidgoli, S. Rahnamayan, T. Dehkharghanian, A. Riasatian, S. Kalra, M. Zaveri, \nC.J.V. Campbell, A. Parwani, L. Pantanowitz, H.R. Tizhoosh, Evolutionary deep \nfeature selection for compact representation of gigapixel images in digital \npathology, Artif. Intell. Med. 132 (2022) 102368, https:\/\/doi.org\/10.1016\/j. \nartmed.2022.102368.\n[56] X. Li, Q. Tang, J. Yu, Y. Wang, Z. Shi, Microvascularity detection and quantification \nin glioma: a novel deep-learning-based framework, Lab. Invest. 99 (2019) \n1515\u20131526, https:\/\/doi.org\/10.1038\/s41374-019-0272-3.\n[57] N. Bergmann, C. Delbridge, J. Gempt, A. Feuchtinger, A. Walch, L. Schirmer, \nW. Bunk, T. Aschenbrenner, F. Liesche-Starnecker, J. Schlegel, The intratumoral \nheterogeneity reflects the intertumoral subtypes of glioblastoma multiforme: a \nregional immunohistochemistry analysis, Front. Oncol. 10 (2020), https:\/\/doi.org\/ \n10.3389\/fonc.2020.00494.\n[58] F. Liesche-Starnecker, K. Mayer, F. Kofler, S. Baur, F. Schmidt-Graf, J. Kempter, \nG. Prokop, N. Pfarr, W. Wei, J. Gempt, S.E. Combs, C. Zimmer, B. Meyer, \nB. Wiestler, J. Schlegel, Immunohistochemically characterized intratumoral \nheterogeneity is a prognostic marker in human glioblastoma, Cancers 12 (2020) \n2964, https:\/\/doi.org\/10.3390\/cancers12102964.\n[59] L. Garofano, S. Migliozzi, Y.T. Oh, F. D\u2019Angelo, R.D. Najac, A. Ko, B. Frangaj, F. \nP. Caruso, K. Yu, J. Yuan, W. Zhao, A. Luisa Di Stefano, F. Bielle, T. Jiang, P. Sims, \nM.L. Suv`a, F. Tang, X.D. Su, M. Ceccarelli, M. Sanson, A. Lasorella, A. Iavarone, \nPathway-based classification of glioblastoma uncovers a mitochondrial subtype \nwith therapeutic vulnerabilities, Nat. Can. (Ott.) 2 (2021) 141, https:\/\/doi.org\/ \n10.1038\/S43018-020-00159-4.\n[60] M. Martinez-Lage, T.M. Lynch, Y. Bi, C. Cocito, G.P. Way, S. Pal, J. Haller, R.E. Yan, \nA. Ziober, A. Nguyen, M. Kandpal, D.M. O\u2019Rourke, J.P. Greenfield, C.S. Greene, R. \nV. Davuluri, N. Dahmane, Immune landscapes associated with different \nglioblastoma molecular subtypes, Acta Neuropathol Commun 7 (2019) 1\u201312, \nhttps:\/\/doi.org\/10.1186\/S40478-019-0803-6\/FIGURES\/8.\n[61] J. Kang, Z. Ullah, J. Gwak, MRI-based brain tumor classification using ensemble of \ndeep features and machine learning classifiers, Sensors 21 (2021) 1\u201321, https:\/\/ \ndoi.org\/10.3390\/s21062222.\n[62] P. Saha, M.S. Sadi, M.M. Islam, EMCNet: automated COVID-19 diagnosis from X- \nray images using convolutional neural network and ensemble of machine learning \nclassifiers, Inform. Med. Unlocked 22 (2021) 100505, https:\/\/doi.org\/10.1016\/J. \nIMU.2020.100505.\n[63] E. Upschulte, S. Harmeling, K. Amunts, T. Dickscheid, Contour proposal networks \nfor biomedical instance segmentation, Med. Image Anal. 77 (2022) 102371, \nhttps:\/\/doi.org\/10.1016\/J.MEDIA.2022.102371.\n[64] J.P. Thakkar, T.A. Dolecek, C. Horbinski, Q.T. Ostrom, D.D. Lightner, J. \nS. Barnholtz-Sloan, J.L. Villano, Epidemiologic and molecular prognostic review of \nglioblastoma, Cancer Epidemiol. Biomarkers Prev. 23 (2014) 1985\u20131996, https:\/\/ \ndoi.org\/10.1158\/1055-9965.EPI-14-0275\/68016\/AM\/EPIDEMIOLOGIC-AND- \nMOLECULAR-PROGNOSTIC-REVIEW-OF.\n[65] G. Reifenberger, V.P. Collins, Pathology and molecular genetics of astrocytic \ngliomas, J. Mol. Med. (Berl.) 82 (2004) 656\u2013670, https:\/\/doi.org\/10.1007\/s00109- \n004-0564-x.\n[66] O.S. Al-Kadi, A multiresolution clinical decision support system based on fractal \nmodel design for classification of histological brain tumours, Comput. Med. Imag. \nGraph. 41 (2015) 67\u201379, https:\/\/doi.org\/10.1016\/j.compmedimag.2014.05.013.\n[67] H. Xu, L. Liu, X. Lei, M. Mandal, C. Lu, An unsupervised method for histological \nimage segmentation based on tissue cluster level graph cut, Comput. Med. Imag. \nGraph. 93 (2021), https:\/\/doi.org\/10.1016\/j.compmedimag.2021.101974.\n[68] E.I. Papageorgiou, P.P. Spyridonos, D.T. Glotsos, C.D. Stylios, P. Ravazoula, G. \nN. Nikiforidis, P.P. Groumpos, Brain tumor characterization using the soft \ncomputing technique of fuzzy cognitive maps, Applied Soft Computing Journal 8 \n(2008) 820\u2013828, https:\/\/doi.org\/10.1016\/j.asoc.2007.06.006.\n[69] C. Cong, S. Liu, A. Di Ieva, M. Pagnucco, S. Berkovsky, Y. Song, Colour adaptive \ngenerative networks for stain normalisation of histopathology images, Med. Image \nAnal. 102580 (2022), https:\/\/doi.org\/10.1016\/j.media.2022.102580.\n[70] J. Peng, Z. Luo, CS-Net: instance-aware cellular segmentation with hierarchical \ndimension-decomposed convolutions and slice-attentive learning, Knowl. Base \nSyst. 232 (2021), https:\/\/doi.org\/10.1016\/j.knosys.2021.107485.\n[71] M. Komosi\u00b4nski, K. Komosi\u00b4nski, K. Krawiec, Evolutionary weighting of image \nfeatures for diagnosing of CNS tumors, Artif. Intell. Med. (2000), https:\/\/doi.org\/ \n10.1016\/S0933-3657(99)00048-2.\n[72] E. Gangoso, B. Southgate, L. Bradley, S. Rus, F. Galvez-Cancino, N. McGivern, \nE. G\u00fc\u00e7, C.A. Kapourani, A. Byron, K.M. Ferguson, N. Alfazema, G. Morrison, \nV. Grant, C. Blin, I.F. Sou, M.A. Marques-Torrejon, L. Conde, S. Parrinello, \nJ. Herrero, S. Beck, S. Brandner, P.M. Brennan, P. Bertone, J.W. Pollard, S. \nA. Quezada, D. Sproul, M.C. Frame, A. Serrels, S.M. Pollard, Glioblastomas acquire \nmyeloid-affiliated transcriptional programs via epigenetic immunoediting to elicit \nimmune evasion, Cell 184 (2021) 2454\u20132470.e26, https:\/\/doi.org\/10.1016\/j. \ncell.2021.03.023.\n[73] S.E.A. Raza, L. Cheung, M. Shaban, S. Graham, D. Epstein, S. Pelengaris, M. Khan, \nN.M. Rajpoot, Micro-Net: a unified model for segmentation of various objects in \nmicroscopy images, Med. Image Anal. 52 (2019) 160\u2013173, https:\/\/doi.org\/ \n10.1016\/j.media.2018.12.003.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n30 \n\n[74] M.U. Oner, J. Chen, E. Revkov, A. James, S.Y. Heng, A.N. Kaya, J.J.S. Alvarez, \nA. Takano, X.M. Cheng, T.K.H. Lim, D.S.W. Tan, W. Zhai, A.J. Skanderup, W. \nK. Sung, H.K. Lee, Obtaining spatially resolved tumor purity maps using deep \nmultiple instance learning in a pan-cancer study, Patterns 3 (2022), https:\/\/doi. \norg\/10.1016\/j.patter.2021.100399.\n[75] A.A. Mughal, L. Zhang, A. Fayzullin, A. Server, Y. Li, Y. Wu, R. Glass, T. Meling, I. \nA. Langmoen, T.B. Leergaard, E.O. Vik-Mo, Patterns of invasive growth in \nmalignant gliomas\u2014the Hippocampus emerges as an invasion-spared brain region, \nNeoplasia 20 (2018) 643\u2013656, https:\/\/doi.org\/10.1016\/j.neo.2018.04.001.\n[76] S.K. McBrayer, J.R. Mayers, G.J. DiNatale, D.D. Shi, J. Khanal, A.A. Chakraborty, \nK.A. Sarosiek, K.J. Briggs, A.K. Robbins, T. Sewastianik, S.J. Shareef, B. \nA. Olenchock, S.J. Parker, K. Tateishi, J.B. Spinelli, M. Islam, M.C. Haigis, R. \nE. Looper, K.L. Ligon, B.E. Bernstein, R.D. Carrasco, D.P. Cahill, J.M. Asara, C. \nM. Metallo, N.H. Yennawar, M.G. Vander Heiden, W.G. Kaelin, Transaminase \ninhibition by 2-hydroxyglutarate impairs glutamate biosynthesis and redox \nhomeostasis in glioma, Cell 175 (2018) 101\u2013116.e25, https:\/\/doi.org\/10.1016\/j. \ncell.2018.08.038.\n[77] V. Brindha, P. Jayashree, P. Karthik, P. Manikandan, Tumor grading model \nemploying geometric analysis of histopathological images with characteristic \nnuclei dictionary, Comput. Biol. Med. 149 (2022) 106008, https:\/\/doi.org\/ \n10.1016\/j.compbiomed.2022.106008.\n[78] C. Zhong, J. Han, A. Borowsky, B. Parvin, Y. Wang, H. Chang, When machine \nvision meets histology: a comparative evaluation of model architecture for \nclassification of histology sections, Med. Image Anal. 35 (2017) 530\u2013543, https:\/\/ \ndoi.org\/10.1016\/j.media.2016.08.010.\n[79] I. Alzoubi, G. Bao, R. Zhang, C. Loh, Y. Zheng, S. Cherepanoff, G. Gracie, M. Lee, \nM. Kuligowski, K.L. Alexander, M.E. Buckland, X. Wang, M.B. Graeber, An open- \nsource AI framework for the analysis of single cells in whole-slide images with a \nnote on CD276 in glioblastoma, Cancers 14 (2022) 3441, https:\/\/doi.org\/10.3390\/ \ncancers14143441.\n[80] K. Fatima, A. Arooj, H. Majeed, A new texture and shape based technique for \nimproving meningioma classification, Microsc. Res. Tech. 77 (2014) 862\u2013873, \nhttps:\/\/doi.org\/10.1002\/jemt.22409.\n[81] L. Hou, D. Samaras, T.M. Kurc, Y. Gao, J.E. Davis, J.H. Saltz, Patch-based \nconvolutional neural network for whole slide tissue image classification, in: 2016 \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 2016, \npp. 2424\u20132433, https:\/\/doi.org\/10.1109\/CVPR.2016.266.\n[82] L. Jin, F. Shi, Q. Chun, H. Chen, Y. Ma, S. Wu, N.U.F. Hameed, C. Mei, J. Lu, \nJ. Zhang, A. Aibaidula, D. Shen, J. Wu, Artificial intelligence neuropathologist for \nglioma classification using deep learning on hematoxylin and eosin stained slide \nimages and molecular markers, Neuro Oncol. 23 (2021) 44\u201352, https:\/\/doi.org\/ \n10.1093\/neuonc\/noaa163.\n[83] M. Nalisnik, M. Amgad, S. Lee, S.H. Halani, J.E. Velazquez Vega, D.J. Brat, D. \nA. Gutman, L.A.D. Cooper, Interactive phenotyping of large-scale histology \nimaging data with HistomicsML, Sci. Rep. 7 (2017) 14588, https:\/\/doi.org\/ \n10.1038\/s41598-017-15092-3.\n[84] F. Orzan, F. Pagani, M. Cominelli, L. Triggiani, S. Calza, F. De Bacco, D. Medicina, \nP. Balzarini, P.P. Panciani, R. Liserre, M. Buglione, M.M. Fontanella, E. Medico, \nR. Galli, C. Isella, C. Boccaccio, P.L. Poliani, A Simplified Integrated Molecular and \nImmunohistochemistry-Based Algorithm Allows High Accuracy Prediction of \nGlioblastoma Transcriptional Subtypes, vol. 100, Laboratory Investigation, 2020, \npp. 1330\u20131344, https:\/\/doi.org\/10.1038\/s41374-020-0437-0.\n[85] H. Su, F. Xing, X. Kong, Y. Xie, S. Zhang, L. Yang, Robust cell detection and \nsegmentation in histopathological images using sparse reconstruction and stacked \ndenoising autoencoders, Med Image Comput Comput Assist Interv (2015), https:\/\/ \ndoi.org\/10.1007\/978-3-319-24574-4_46.\n[86] T.H. Vu, H.S. Mousavi, V. Monga, G. Rao, U.K.A. Rao, Histopathological image \nclassification using discriminative feature-oriented dictionary learning, IEEE Trans. \nMed. Imag. 35 (2016) 738\u2013751, https:\/\/doi.org\/10.1109\/TMI.2015.2493530.\n[87] F. Xing, Y. Xie, L. Yang, An automatic learning-based framework for robust nucleus \nsegmentation, IEEE Trans. Med. Imag. 35 (2015) 550\u2013566, https:\/\/doi.org\/ \n10.1109\/TMI.2015.2481436.\nC.K. Vong et al.                                                                                                                                                                                                                                 \nComputers in Biology and Medicine 186 (2025) 109642 \n31 \n",
                  "abstract":"1 Abstract\nProblem\nMachine learning (ML)\/Deep learning (DL) techniques have been evolving to solve more complex diseases, but it has been used relatively little in Glioblastoma (GBM) histopathological studies, which could benefit greatly due to the disease's complex pathogenesis.\nAim\nConduct a systematic review to investigate how ML\/DL techniques have influenced the progression of brain tumour histopathological research, particularly in GBM.\nMethods\n54 eligible studies were collected from the PubMed and ScienceDirect databases, and their information about the types of brain tumour\/s used, types of -omics data used with histopathological data, origins of the data, types of ML\/DL and its training and evaluation methodologies, and the ML\/DL task it was set to perform in the study were extracted to inform us of trends in GBM-related ML\/DL-based research.\nResults\nOnly 8 GBM-related studies in the eligible utilised ML\/DL methodologies to gain deeper insights into GBM pathogenesis by contextualising histological data with -omics data. However, we report that these studies have been published more recently. The most popular ML\/DL models used in GBM-related research are the SVM classifier and ResNet-based CNN architecture. Still, a considerable number of studies failed to state training and evaluative methodologies clearly.\nConclusion\nThere is a growing trend towards using ML\/DL approaches to uncover relationships between biological and histopathological data to bring new insights into GBM, thus pushing GBM research forward. Much work still needs to be done to properly report the ML\/DL methodologies to showcase the models\u2019 robustness and generalizability and ensure the models are reproducible.",
                  "cita":"Chun Kiet Vong and Alan Wang and Mike Dragunow and Thomas I-H. Park and Vickie Shim (2025). Brain tumour histopathology through the lens of deep learning: A systematic review.",
                  "palabra_clave":"review",
                  "codigo":"Chun_2025"
         },
         {
                  "titulo":"Early cancer detection using deep learning and medical imaging: A survey",
                  "autor":"Istiak Ahmad and Fahad Alqurashi",
                  "link":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1040842824002713",
                  "ano":"2024",
                  "Texto":"Early cancer detection using deep learning and medical imaging: A survey\nIstiak Ahmad a,b,*,1, Fahad Alqurashi a,2\na Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia\nb School of Information and Communication Technology, Griffith University, Queensland 4111, Australia\nA R T I C L E  I N F O\nKeywords:\nMedical imaging\nCancer detection\nClassification\nSegmentation\nDeep learning\nTransfer learning\nA B S T R A C T\nCancer, characterized by the uncontrolled division of abnormal cells that harm body tissues, necessitates early \ndetection for effective treatment. Medical imaging is crucial for identifying various cancers, yet its manual \ninterpretation by radiologists is often subjective, labour-intensive, and time-consuming. Consequently, there is a \ncritical need for an automated decision-making process to enhance cancer detection and diagnosis. Previously, a \nlot of work was done on surveys of different cancer detection methods, and most of them were focused on specific \ncancers and limited techniques. This study presents a comprehensive survey of cancer detection methods. It \nentails a review of 99 research articles collected from the Web of Science, IEEE, and Scopus databases, published \nbetween 2020 and 2024. The scope of the study encompasses 12 types of cancer, including breast, cervical, \novarian, prostate, esophageal, liver, pancreatic, colon, lung, oral, brain, and skin cancers. This study discusses \ndifferent cancer detection techniques, including medical imaging data, image preprocessing, segmentation, \nfeature extraction, deep learning and transfer learning methods, and evaluation metrics. Eventually, we sum\u00ad\nmarised the datasets and techniques with research challenges and limitations. Finally, we provide future di\u00ad\nrections for enhancing cancer detection techniques.\n1. Introduction\nCancer is an assortment of disorders characterized by unregulated \ncellular proliferation that has the potential to infiltrate and destroy \nnormal tissues. Genetic mutations and environmental triggers cause this \ncondition, potentially impacting many body parts. Over 100 different \nforms of the condition have already been identified. The condition has a \nprofound influence on people, manifesting in intense physical symptoms \nsuch as pain and exhaustion, psychological consequences including \nstress and despair, and substantial social and financial hardships, \nincluding expensive medical expenses and broken relationships. In 2024 \n(Siegel et al., 2024), the American Cancer Society projects 2001,140 \nnew cancer cases and 611,720 cancer deaths in the United States. Cancer \nmortality has declined, averting over 4 million deaths since 1991 due to \nreduced smoking, early detection, and improved treatments, but rising \nincidence rates for several cancers threaten these gains. From \n2015\u20132019, annual incidence rates increased by 0.6 %-1 % for breast, \npancreas, and uterine corpus cancers and by 2 %-3 % for prostate, liver \n(in females), kidney, HPV-associated oral cancers, and melanoma. Sig\u00ad\nnificant disparities persist, with mortality rates for prostate, stomach, \nand uterine corpus cancers being double in Black people compared to \nWhite people and higher rates for liver, stomach, and kidney cancers in \nNative American people. GLOBOCAN (Bray et al., 2024) presents global \ncancer statistics for 2022, reporting nearly 20 million new cases and 9.7 \nmillion deaths, with lung cancer being the most diagnosed and leading \ncause of death. Table 1 lists the major 12 types of cancer statistics from \nthe American Cancer Society (Explore cancer statistics, 2024). The sta\u00ad\ntistics clearly illustrate the importance of early cancer detection system \nrequirements.\nEarly identification of cancer greatly enhances the likelihood of \neffective therapy and prolonged survival by detecting the illness at a \nmore controllable phase. This enables the implementation of more \nefficient and less forceful interventions, resulting in an improved prog\u00ad\nnosis and higher survival rates. Regular screenings and vigilance about \nfirst symptoms are crucial for early cancer detection, eventually \nimproving patient outcomes and lessening the overall impact of the \n* Corresponding author at: Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi \nArabia.\nE-mail address: istiak.ahmad@griffithuni.edu.au (I. Ahmad). \n1 0000\u20130001-9914\u20134116\n2 0000\u20130002-7919\u2013747X\nContents lists available at ScienceDirect\nCritical Reviews in Oncology \/ Hematology\njournal homepage: www.elsevier.com\/locate\/critrevonc\nhttps:\/\/doi.org\/10.1016\/j.critrevonc.2024.104528\nReceived 16 August 2024; Accepted 2 October 2024  \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \nAvailable online 15 October 2024 \n1040-8428\/\u00a9 2024 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license ( http:\/\/creativecommons.org\/licenses\/by\/4.0\/ ). \n\ndisease. Artificial Intelligence (AI) significantly enhanced the smart city \nconcept by improving various sectors, including health, education \n(Ahmad et al., 2021), transportation (Ahmad et al., 2022a), security \n(Alqurashi and Ahmad, 2024), and social implications (Ahmad et al., \n2022b, Ahmad et al., 2022c). Within healthcare, Computer Aided \nDiagnosis (CAD) systems assist in early cancer diagnosis by examining \nmedical images using sophisticated algorithms to identify anomalies \n(Prabhu et al., 2022). They improve screening precision and effective\u00ad\nness by offering a secondary evaluation, minimizing human mistakes, \nand rapidly handling substantial data. Furthermore, CAD systems, \nincluding machine learning (ML) and deep learning (DL), constantly \nlearn and adapt efficiently to new data and imaging methods that help \nradiologists make better judgements and enhance cancer treatment. ML \nalgorithms are capable of processing complex datasets and frequently \noutperform conventional techniques in identifying patterns and abnor\u00ad\nmalities that may indicate malignant changes. DL is a subset of ML that \nuses neural networks to evaluate medical images, including MRIs, CT \nscans, and mammograms, detecting tiny characteristics the human eye \nwould miss. These technologies are crucial because they can handle \nincreasing medical data, lower the risk of incorrect diagnoses, and \nprovide scalable, reliable, and quick solutions for early cancer diagnosis.\n1.1. Related work\nTable 2 lists the summary of an existing survey or related literature \nreview. While Most of the survey papers focused on machine learning or \ndeep learning algorithms, other important topics like medical imaging, \ndataset collection, image preprocessing, segmentation, feature extrac\u00ad\ntion, and evaluation metrics were not discussed. Additionally, the article \nselection process also skips some papers. However, the present study \nprovides a comprehensive discussion of the above topics.\nThe research questions of this study are listed below: \n1. Medical Imaging Data \n11. What are the challenges associated with medical imaging data?\n12. What preprocessing techniques are used to enhance the data \nquality?\n13. What are the benchmark datasets used for cancer detection?\n2. Deep Learning Techniques \n21. What are the main deep-learning techniques used in cancer \ndetection?\n22. What are the most used algorithms for feature extraction?\n23. How can transfer learning be utilized to improve the perfor\u00ad\nmance of deep learning models with limited datasets?\n3. Discussion and Future Directions \n31. What are the challenges and limitations?\n32. What are the future directions?\n1.2. Aim, novelty and contributions\nPrevious survey papers have primarily focused on specific types of \ncancer and limited detection techniques. It covers various techniques, \nincluding medical imaging data, image processing, segmentation, \nfeature extraction, deep learning, and transfer learning methods, along \nwith evaluation metrics. This study provides a comprehensive review of \ncancer detection methods for 12 types of cancer such as breast, cervical, \novarian, prostate, esophageal, liver, pancreatic, colon, lung, oral, brain, \nand skin cancers.\nThe aim of this study is to bridge the existing research gap and \ncontribute to improving cancer detection methods. It also raises eight \nresearch questions and includes a figure illustrating the systematic \nworkflow of the cancer detection process, from input to output.\nThe contributions of this study are outlined as follows: \n\u2022 The study reviews 99 research articles from WoS, IEEE, and Scopus \ndatabases published between 2020 and 2024 and discusses their \ndataset, methods, and output.\n\u2022 We discuss and summarise the benchmark datasets with data source \nlinks, which will enable researchers for further development.\n\u2022 We discuss various cancer detection techniques and corresponding \nresearch studies to provide a comprehensive understanding and \npractical application of these techniques.\n\u2022 We also discuss research challenges and limitations with future di\u00ad\nrections for enhancing cancer detection techniques.\nSection 1 discusses the background of the cancer detection approach, \nrelated survey paper, research motivation, research questions, and \ncontribution to this paper. Section 2 discusses the article collection \nprocess for this study. Section 3 discusses cancer detection techniques, \nincluding medical imaging, image processing, deep learning algorithms, \ntransfer learning, and evaluation metrics. Section 4 discusses cancer \ntypes by introducing each cancer and corresponding statistics, datasets, \nand summary for each cancer detection technique. Section 5 discusses \nthe overall summary of the datasets, cancer detection algorithms, \nresearch challenges and limitations, and future directions. Section 6\nconcludes this study by pointing out major aspects of cancer detection \ntechniques using deep learning and medical imaging.\n2. Methodology and design\nFigure 1 shows the word cloud, which is generated from the articles \nkeywords section. The top 100 keywords are shown in this figure, and \nthe text size is based on the frequency. For example, cancer, learning, \nbreast, detection, feature, imaging, neural, classification, convolutional, \netc., are the most commonly used keywords in research articles.\nThis study collected research articles from the Web of Science (WoS), \nIEEE, and Scopus. We used the name as a variable in the query, and the \nTable 1 \nCancer Statistics - 2024 (Explore cancer statistics, 2024).\nCancer\nEstimated New Cases\nEstimated Deaths\nMale\nFemale\nTotal\nMale\nFemale\nTotal\nBreast\n\u200b\n310,720\n310,720\n\u200b\n42,250\n42,250\nCervical\n\u200b\n13,820\n13,820\n\u200b\n4360\n4360\nOvarian\n\u200b\n19,680\n19,680\n\u200b\n12,740\n12,740\nProstate\n299,010\n\u200b\n299,010\n35,250\n\u200b\n35,250\nEsopha.\n17,690\n4680\n22,370\n12,880\n3250\n16,130\nLiver\n28,000\n13,630\n41,630\n19,120\n10,720\n29,840\nPancr.\n34,530\n31,910\n66,440\n27,270\n24,480\n51,750\nColon\n54,210\n52,380\n106,590\n28,700\n24,310\n53,010\nLung\n116,310\n118,270\n234,580\n65,790\n59,280\n125,070\nOral\n41,510\n16,940\n58,450\n8700\n3530\n12,230\nBrain\n14,420\n10,980\n25,400\n10,690\n8070\n18,760\nSkin\n59,170\n41,470\n100,640\n5430\n2680\n8290\nTotal\n664,850\n634,480\n1299,330\n213,830\n195,670\n409,680\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n2 \n\nname of the cancer will replace it. The following query was used for WoS \nto retrieve the articles: (TI=(Name) AND AB=(\u201cimage\u201d) AND TI=\n(\u201cdetection\u201d)). Further, we used the following criteria to filter the \nresearch articles: article publication duration between 2020 and 2024; \ndocument type: article; research areas: engineering, computer science, \nand medical imaging; language: English; and open access. We applied \nthe following query for IEEE: (\u201cDocument Title\u201d: Name) AND (\u201cAb\u00ad\nstract\u201d: \u201cimage\u201d) AND (\u201cDocument Title\u201d: \u201cdetection\u201d). The publication \nduration was from 2020 to 2024, and the documents were only journals. \nWe used the following query for Scopus database: (TITLE(\u201cName\u201d) AND \nABS(\u201cimage\u201d) AND TITLE(\u201cdetection\u201d)) AND PUBYEAR > 2019 AND \nPUBYEAR < 2025 AND (LIMIT-TO (SRCTYPE,\u201cj\u201d)) AND (LIMIT-TO \n(OA,\u201call\u201d)) AND (LIMIT-TO (SUBJAREA,\u201cCOMP\u201d)) AND (LIMIT-TO \n(DOCTYPE,\u201car\u201d)) AND (LIMIT-TO (LANGUAGE,\u201cEnglish\u201d)). Filtering \ncriteria include a subject area: computer science; document type: article; \nlanguage: English; source type: journal; and open access. Initially, this \nstudy collected a total of 444 articles from the three databases. We got \n308 articles after removing the duplicate articles. Furthermore, we \napplied an eligibility check by using a few inclusion and exclusion \ncriteria to finalise the articles. The inclusion criteria include articles \nusing deep learning algorithms for cancer detection and medical imag\u00ad\ning and issued in first or second quartiles. The exclusion criteria include \nreview articles and articles published in conferences or in the third and \nfourth quartiles. Finally, we have reviewed 99 research articles in this \nstudy. The article selection process is summarised in Table 3.\n3. Cancer detection techniques\nFigure 2 provides a detailed analysis of cancer types and the sys\u00ad\ntematic workflow for detecting cancer using medical imaging and deep \nlearning algorithms. The top section classifies cancers by the human \nbody system: reproductive (breast, cervical, ovarian, prostate), digestive \n(esophageal, liver, pancreatic, colon), respiratory (lung, oral), and \nothers (brain, skin). The bottom section outlines the steps involved in \nTable 2 \nA comparative analysis of existing survey\/literature review.\nRef.\nCY\nCancer\nMI\nPr\nSeg\nFE\nEM\nModels\nBD\nASP\nNoA\nCha.\nFW\nRai (2024) 2024\n2020\u20132023\nlung, breast, skin, brain, colorectal, prostate, \nleukemia\n\u200b\n\u2713\n\u200b\n\u2713\n\u200b\n\u2713\n\u2713\n\u200b\n100+\n\u2713\n\u2713\nYaqoob et al. (2023b)\n2023\n2017\u20132022\nNM\n\u2713\n\u2713\n\u200b\n\u2713\n\u200b\n\u2713\n\u2713\n\u2713\n97\n\u2713\n\u2713\nYaqoob et al. (2023a)\n2023\nNM\nNM\n\u2713\n\u200b\n\u200b\n\u2713\n\u200b\n\u2713\n\u200b\n\u2713\n117\n\u2713\n\u2713\nMurthy and Bethala \n(2023) 2023\n2009\u20132020\nbreast, brain, molecular, prostate, prostate, lung, \ncervical, colon\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u2713\n\u2713\n\u2713\n\u200b\n\u2713\n\u2713\nSharma et al. (2023)\n2023\n2012\u20132023\nbreast, lung, liver, prostate, brain, skin, colon\n\u200b\n\u200b\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u200b\n\u2713\n\u2713\nKaur and Garg (2023)\n2023\nNM\nlung, breast, skin, Brain\n\u2713\n\u2713\n\u2713\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\nZolfaghari et al. \n(2023) 2023\n2002\u20132023\nbreast, prostate, ovarian, leukemia, colon, lung, \nlymphoma, liver\n\u200b\n\u200b\n\u200b\n\u2713\n\u200b\n\u2713\n\u200b\n\u2713\n92\n\u2713\n\u2713\nMaurya et al. (2023)\n2023\n2017\u20132022\nbrain, cervical, breast, skin, lung\n\u200b\n\u200b\n\u200b\n\u200b\n\u2713\n\u2713\n\u2713\n\u200b\n\u200b\n\u2713\n\u2713\nKumar et al. (2022)\n2021\n2009\u20132021\nbreast, prostate, liver, brain, stomach, skin\n\u200b\n\u200b\n\u200b\n\u200b\n\u2713\n\u2713\n\u200b\n\u2713\n185\n\u2713\n\u2713\nMunir et al. (2019)\n2019\nNM\nbreast, lung, brain, skin, prostate\n\u200b\n\u2713\n\u2713\n\u200b\n\u2713\n\u2713\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\nHu et al. (2018) 2018\nNM\nbreast, lung, skin, prostate, brain, colonial\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u2713\n\u2713\n\u200b\n\u200b\n\u2713\n\u2713\nThis Study\n2020\u20132024\nbreast, cervical, ovarian, prostate, esophageal, liver, \ncolon, lung, oral, skin, brain, pancreatic\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n120\n\u2713\n\u2713\nNM = Not Mentioned, CY = Covered Years, MI = Medical Imaging,Pr = Preprocessing, Seg = segmentation, FE = Feature Extraction, EM = Evaluation Metrics, BD =\nBenchmark Dataset, AS = Article Selection Process, NoA = Number of Articles, Cha = Challenges, FW = Future Work\nFig. 1. Word Cloud of 100 Most Used Keywords.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n3 \n\ncancer detection, starting with medical imaging techniques(see Section \n3.1) such as pathology (histological and cytological) and radiology (X- \nray, ultrasound, MRI, CT, PET). Following imaging, the figure describes \nimage preprocessing steps (see Section 3.2), like re-scaling, normaliza\u00ad\ntion, augmentation, noise reduction, and enhancement. Segmentation \ntechniques (see Section 3.3) like U-NET, watershed transform, and \nclustering are then utilized, followed by feature extraction methods (see \nSection 3.4), including GLCM, HOG, and wavelet transform. Various \ndeep learning algorithms (see Section 3.5) such as CNNs, RCNNs, RNNs, \nLSTMs, vision transformers, GANs, and hybrid models are applied, \ninvolving transfer learning (see Section 3.6) with models like YOLO, \nAlexNet, Inception, DenseNet, ResNet, and VGGNet. The final stage in\u00ad\ncludes evaluation metrics (see Section 3.7) like accuracy, precision, \nrecall, F1-score, AUC-ROC, specificity, Kappa, and MCC, ensuring the \nreliability and efficacy of the cancer detection system. This compre\u00ad\nhensive overview helps in understanding the integration of advanced \nimaging and deep learning techniques in cancer detection research.\n3.1. Medical imaging techniques\nMedical imaging is vital in radiology and pathology and is pivotal in \nidentifying and treating many diseases, including cancer. Radiology \n(Kasban et al., 2015) is the field that employs imaging methods to see \ninto the body, whereas pathology is the study of tissues, cells, and organs \nto identify disorders. Some key techniques in pathology are histopa\u00ad\nthology, which studies stained tissue sections to recognise disease at the \ncellular level, and Cytopathology, which studies particular cells from \nbody fluids or fine-needle aspirates. This study focuses on radiology \n(Hussain et al., 2022), including key techniques such as X-rays, \nComputed Tomography (CT) scans, Magnetic Resonance Imaging (MRI), \nultrasound, and Positron Emission Tomography (PET) scans.\n3.1.1. X-radiation (X-Ray)\nX-ray radiography presents numerous advantages, such as being \nnoninvasive, rapid, and painless, which makes it a significant tool for \nmedical and surgical treatment planning and guiding operations like \ncatheter and stent insertions. Nevertheless, it has potential hazards such \nas exposure to ionizing radiation, which may elevate the probability of \ngetting cancer and induce tissue damage such as cataracts, skin \nreddening, and hair loss at high levels of radiation. The clinical appli\u00ad\ncations of X-ray radiography are wide-ranging and include chiropractic \nand dental examinations, projectional radiographs for evaluating frac\u00ad\ntures and lung diseases, and mammography for breast screening.\n3.1.2. Computed tomography Scan (CT)\nCT creates comprehensive cross-sectional images of the body using \nX-rays and computer processing. It is non-invasive, fast, painless, and \nprovides a global vein image with high spatial resolution. It can discern \nminor physical density changes without invasive artery catheter im\u00ad\nplantation. CT concerns include exposure to ionizing radiation, a lack of \nreal-time information, and difficulties identifying intra-luminal abnor\u00ad\nmalities. It requires contrast, which may cause allergies and toxicity, and \nhas inferior soft tissue contrast resolution. CT evaluates body compo\u00ad\nnents, diagnoses illnesses, injuries, and anomalies, plans and guides \nTable 3 \nResearch Articles Collections.\nCancers\nWoS\nScopus\nIEEE\nReviewed\nBreast\n57\n120\n39\n29\nCervical\n10\n16\n3\n8\nOvarian\n1\n2\n0\n3\nProstate\n7\n10\n6\n11\nEsophageal\n2\n3\n2\n3\nLiver\n5\n8\n2\n3\nPancreatic\n3\n5\n1\n3\nColon\n5\n7\n1\n4\nLung\n16\n39\n7\n14\nOral\n5\n5\n2\n6\nBrain\n1\n3\n1\n3\nSkin\n13\n32\n5\n12\nTotal\n125\n250\n69\n142\nTotal (Initial Identification)\n444\nTotal (Without Duplicate)\n308\nTotal (Reviewed After Eligibility check)\n99\nFig. 2. Systematic Workflow for Cancer Detection.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n4 \n\nsurgeries, and monitors cancer treatment.\n3.1.3. Magnetic resonance imaging (MRI)\nMRI generates comprehensive images of bodily tissues using mag\u00ad\nnetic and radio frequency fields and monitors body chemistry. MRI has \nseveral advantages: it is non-invasive, painless, radiation-free, high- \nresolution, and operator-independent. Its downsides include limited \nsensitivity, extensive scan and post-processing durations, a large probe \nneed, no real-time information, and difficulties identifying intra-luminal \nabnormalities. MRI is used to diagnose liver and gastrointestinal disor\u00ad\nders, brain and spinal cord abnormalities, tumours, cysts, other abnor\u00ad\nmalities, joint injuries, pelvic discomfort in women, and unhealthy \ntissues.\n3.1.4. Ultrasonogram (US)\nUltrasonography employs high-frequency sound waves to generate \nmedical images by reflecting off tissues. It detects flow variations and \nabnormalities within and outside lumens without ionizing radiation, is \nnoninvasive, painless, and provides high-resolution, real-time data. \nWhile it lacks defined criteria, is operator-dependent, and is time- \nconsuming, it can monitor fetal growth, imagine head and neck struc\u00ad\ntures, and check solid abdominal organs, including the liver, pancreas, \nand kidneys, making it useful in cancer diagnosis. This technique helps \ndetect tumours, cysts, and other anomalies, aiding in early cancer \ndetection and surveillance.\n3.1.5. Positron emission tomography (PET)\nPET employs radiotracers to observe metabolic processes in the \nbody, making it an efficient cancer screening tool. PET scans are \nnoninvasive and give real-time functional information, detecting aber\u00ad\nrant metabolic activity linked to cancer. This imaging approach is useful \nfor identifying tiny tumours, determining metastasis, and measuring \ncancer therapy efficacy. PET scans employ radioactive chemicals and \nexpose patients to ionizing radiation, but their capacity to detect \nmetabolic changes at the cellular level benefits them over CT and MRI, \nwhich give structural information. PET\/CT improves cancer identifica\u00ad\ntion and surveillance by connecting metabolic activity with anatomical \nfeatures.\n3.2. Preprocessing\n3.2.1. Gaussian filter\nThe operation is performed by assembling the image with a Gaussian \nfunction, which provides greater weights to pixels closer to the centre of \nthe convolution kernel and lesser weights to those further away. This \nleads to a phenomenon known as a blurring effect, which reduces the \npresence of high-frequency noise and small fluctuations in pixel in\u00ad\ntensity. Gaussian filters are very efficient when decreasing random \nnoise, and preserving edge integrity is crucial. Consequently, they are \nessential in tasks such as edge recognition, image improvement, and \npreparing for further analytic stages like segmentation or feature \nextraction. Equation (1) shows the two-dimensional Gaussian function. \nGauss(x, y) =\n1\n2\u03c0\u03c32e\u2212x2+y2\n2\u03c32\n(1) \n3.2.2. Gabor filter\nThe Gabor filter (Obayya et al., 2023) is a kind of linear filter that \nuses a Gabor function as its kernel to collect data on the frequency and \ndirection of local visual elements such as edges and textures. It reprents \nGaussian envelop modulated by sinusoidal plane wave. Equation (2) \nshows the Gabor filter (Hammouche et al., 2022), where fru is the center \nfrequency, and m, and n denotes the ratio of center frequency and \ngaussian envelope size. \u03b8v represents the orientation. \n\u03b4(x, y)\n=\nfr2\nu\n\u03c0mne\n((\nfr2\nu\nm2\n)\nx\u02b92+\n(\nfr2\nu\nn2\n)\ny\u02b92\n)\n)\nej2\u03c0frux\u02b9\nx\u02b9\n=\nxcos\u03b8v + ysin\u03b8v\ny\u02b9\n=\n\u2212xsin\u03b8v + ycos\u03b8v\n(2) \n3.2.3. Median enhanced wiener filter (MEWF)\nAlqarafi et al. (2024) implemented MEWF for noise reduction for \nskin cancer detection. The MEWF technique focuses on the Weiner filter, \nwhich transforms the mask matrix pixel values with median values. This \neffectively reduces the noise in degraded images. The MEWF is shown in \nEquation (3) (Park et al., 2024), where the mean is replaced with median \nvalue; f denotes the degraded image; Iwf, and Imewf denote wiener filter \nand MEWF; \u03b8, and \u03b8 represent the mean and median values of the pixels; \n\u03b1, and \u03b2 represent standard deviation of pixels and noise. \nIwf(x, y)\n=\n\u03b8 + \u03b12 \u2212\u03b22\n\u03b12\n(f(x, y) \u2212\u03b8)\nImewf(x, y)\n=\n\u03b8 + \u03b12 \u2212\u03b22\n\u03b12\n(f(x, y) \u2212\u03b8)\n(3) \n3.2.4. Median filter\nThe median filter is a non-linear technique that removes noise from \nvisual data. Median filters are often used in CAD because of their ca\u00ad\npacity to maintain image sharpness, edges, and clarity while minimizing \nnoise.\n3.2.5. Bilateral filter\nBiF[I]m = 1\nWm\n\u2211\nn\u03f5S\nG\u03b1S(|m \u2212n|) \u2217G\u03b1r(|Im \u2212In|) \u2217I\n(4) \nA bilateral filter is applied to the skin lesion (Midasala et al., 2024) to \nenhance the image in the spatial domain and improve overall contrast. \nThis filter works on the whole image and enhances the histogram. \nAdaptive histogram equalization (AHE) differs from bilateral filters by \ndividing the image into smaller tiles, usually 8\u00d78 pixels. AHE then \nequalizes the histogram within each tile, improving the visibility of \nlesion borders. AHE utilizes contrast-limiting techniques to mitigate \nexcessive contrast and noise. The Equation (4) for the bilateral filter has \nfurther elements, namely the normalization factor and range weight. In \nthis particular situation, the symbol I is used to represent the input skin \nlesion image, G\u03b1S is used to denote the spatial extent of the kernel, and \nG\u03b1r is used to represent the lowest amplitude of an edge. The normali\u00ad\nzation factor (Wm) ensures that only pixels with intensities identical to \nthe centre pixel are blurred, preserving crisp intensity transitions. This is \nachieved using the Gaussian space weight (G\u03b1S(|m \u2212\nn|)) and Gaussian \nrange weight (G\u03b1r(|Im \u2212\nIn|))\n3.2.6. Entropy filter\nThe entropy filter represents a statistical metric that quantifies the \nunpredictability level in an image\u2019s pixel values. Equation (5) provides \nan equation where p is the scaled histogram count for this image \n(Alzubaidi et al., 2021). \nEntropy = \u2212\n\u2211\n(mlog2(m))\n(5) \n3.2.7. Other filters\nKavitha et al. (2023) employed an auto-correlogram, Color Layout \nFilter (CLF), and Binary Pyramid Patterns Filter (BPPF) to remove skin \nhair from the image for skin cancer detection. Thanh et al. (2020)\nimplemented Adaptive principal curvature to enhance the skin hair \ncontrast more reasonably than the maximum principal curvature and \ngradient magnitude.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n5 \n\n3.2.8. Image augmentation\nData augmentation is essential in diagnosing cancer for several rea\u00ad\nsons. Addressing the issue of limited annotated medical imaging datasets \nis critical in medical research, given privacy concerns, the high cost of \nacquiring and labelling data, and the scarcity of specific cancer cate\u00ad\ngories. Data augmentation is a technique that artificially expands the \nvariety and quantity of the training dataset to minimize overfitting, \nenhance the model\u2019s generalization ability, and improve the accuracy \nand resilience of DL models. This ensures that the models can identify \nand categorize malignant tissues, even when confronted with novel and \nunfamiliar images. Image enhancement methods include changing the \nshape of an image (such as rotating, scaling, translating, and flipping) \nand its intensity (by modifying contrast, brightness, and adding noise). \nThey also include more complex techniques such as random cropping, \nelastic deformations, and creating images using generative adversarial \nnetworks (GANs). These strategies replicate many real-world changes in \nmedical images, allowing the models to learn consistent traits and \nenhance their ability to identify cancer in varied imaging settings and \npatient groups.\n3.3. Segmentation\nImage segmentation is the process of splitting an image into separate \nsections to locate and extract significant elements. Segmentation is vital \nfor accurately demarcating malignancies from healthy tissue using deep \nlearning algorithms like CNN (Li et al., 2018; Sun et al., 2019a; Sun \net al., 2019b), which can precisely identify and isolate malignant areas, \nincluding tumour size, shape, and location. This capability greatly \nsupports early cancer detection.\n3.3.1. U-Net\nThe U-Net (Ronneberger et al., 2015) is a fully convolutional \nnetwork (FCN) that was designed for image segmentation. The archi\u00ad\ntecture of U-Net is bidirectional, where the encoder is responsible for \nextracting significant characteristics, while the decoder is symmetric \nand growing, facilitating localization by upconvolution. The framework \nconsists of three main components: downsampling, bottlenecking, and \nupsampling. Every block consists of a convolutional layer, an activation \nfunction, and a max pooling function. U-Net doubles the features after \neach pooling layer to accomplish image segmentation. The upsampling \nlayer of the model consists of a deconvolution layer and an activation \nfunction. Due to the fully convolutional nature, the upsampling contains \nmany feature mappings. The absence of completely linked layers in \nU-Net allows the model to be flexible and capable of processing images \nof varying sizes.\n3.3.2. Watershed transform\nThe watershed transformation is a region-based technique that rep\u00ad\nresents an image as a topographical map, where bright regions corre\u00ad\nspond to high altitudes and dark regions correspond to low altitudes. \nWater is distributed via local low points, where it accumulates in \ncatchment basins and is divided by dams to prevent blending, creating \ndifferent areas called watersheds. The quantity of segmented objects is \ncontingent upon the quantity of local minima, often resulting in exces\u00ad\nsive segmentation as a result of a multitude of spurious minima. In order \nto alleviate this issue, automated markers are used to provide guidance \nthroughout the segmentation process (Sadad et al., 2020).\n3.3.3. Kernel fuzzy C-means\nHuaping et al. (2021) proposed a kernel-based fuzzy C-means clus\u00ad\ntering approach for skin cancer image segmentation. They evaluate the \ndistance of the data points from the centre of the cluster using this \nequation: kernerFunction = exp(\u2212x\u2212y2\n\u03c1\n). They implemented the following \nalgorithms to get the final clusters: The algorithm started by imple\u00ad\nmenting Kernel Fuzzy C-means to cluster a set of objects and generate a \nU membership matrix. The number of closest neighbours t is determined \nfor each pair of elements xi and xj. If xi and xj are not t closest neigh\u00ad\nbours, the weight Wij is set to 0. If they are from the same cluster, then \nWij is set to 0. Otherwise, Wij is calculated using the following formula: \nexp(ln2 \u00d7 (ui\u2a01uj)), where \u2a01 denotes the exclusive OR indicating \noverlap between two fuzzy sets. A diagonal matrix D is then formulated \nand normalized, where Dij = \u2211n\nj=1Wij. The number of eigenvectors \ngreater than L = D\u22121\u22152WD1\u22152 is determined, forming the matrix P = [p1, \np2, \u2026. , pk], which is normalized to create matrix Y. Each row of Y \nrepresents a space Rk point, and final clustering is generated (Huaping \net al., 2021).\n3.4. Feature extraction\nThe fundamental concept behind feature extraction is to include \ndistinct characteristics that distinguish one input pattern from another \npattern to streamline the original data. The pre-processed image sepa\u00ad\nrates textural, intensity, and shape features to derive distinctive fea\u00ad\ntures. The textual features include contrast, energy, correlation, and \nhomogeneity; intensity features include mean, variance, and standard \ndeviation; and shape features include area, perimeter, and circularity.\n3.4.1. Histogram of oriented gradients (HOG)\nHOG (Histogram of Oriented Gradients) features are used in com\u00ad\nputer vision to detect and localize objects by analyzing the differential \nintensity of local gradients or edge directions. In mammographic im\u00ad\nages, each region of interest (ROI) is divided into non-overlapping cells, \nand for each cell, gradients (HX, HY) in the x and y directions are \ncalculated (Equation (6)). The magnitude and orientation of these gra\u00ad\ndients are then computed (Equation (6)), with orientations ranging from \n\u2212180\u2013180 degrees (signed) or 0\u2013360 degrees (unsigned). Afterwards, \nthe Bin is calculated as the number of Bins multiplied by orientations \nand divided by 360. Finally, features are computed as a count of the \nfrequency of each value in the image array of Bin using the magnitude as \nweight. \nHX = d\ndxf(x, y)\nHY = d\ndyf(x, y)\nmagnitude,\n\u20d2\u20d2\u20d2\u20d2H\n\u20d2\u20d2\u20d2\u20d2=\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\nH2\nX + H2\nY\n\u221a\norientation, \u03b8(x, y) = tanhH2\nY\nH2\nX\n(6) \n3.4.2. Gray level co-occurrence matrix (GLCM)\nThe Gray Level Co-occurrence Matrix (GLCM) (Midasala et al., 2024) \nis a commonly used method in cancer screening to examine the texture \nof medical images. The GLCM is a statistical technique that quantifies \nthe spatial correlation between pixels. It does this by determining the \nfrequency with which pairs of pixel values occur in a defined spatial \nconnection and organizing these frequencies into a matrix. Various \ntexture properties may be derived from this matrix, including contrast, \ncorrelation, homogeneity, and energy (See Equation (7)). The \u201cContrast\u201d \ncharacteristic represents the disparity in intensity between neighbouring \npixels. The \u201cHomogeneity\u201d characteristic quantifies the consistency of \ntexture. The term \u201ccorrelation\u201d quantifies the extent of the linear asso\u00ad\nciation between pixels. The term \u201cEnergy\u201d represents the level of \nsharpness and detail in the images. Higher \u201cEnergy\u201d levels correspond to \nmore intricate textures. These features present significant insights into \nthe structural formations that dominate the image, which are crucial for \nidentifying between healthy and malignant cells. \nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n6 \n\nContrast =\n\u2211\nN\u22121\nx,y=0\nSx,y(x \u2212y)2\nCorrelation =\n\u2211\nN\u22121\nx,y=0\nSx,y\n\u23a1\n\u23a2\u23a3\n(x \u2212\u03bca)(y \u2212\u03bcy)\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\n(\u03c32\nx)(\u03c32\ny)\n\u221a\n\u23a4\n\u23a5\u23a6\nHomogeneity =\n\u2211\nN\u22121\nx,y=0\nSx,y\n1 + (x \u2212y)2\nEnergy =\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\n\u2211\nN\u22121\nx,y=0\nS2\nx,y\n\u221a\n\u221a\n\u221a\n\u221a\n(7) \n3.4.3. Redundant discrete wavelet transform (RDWT)\nThe Redundant Discrete Wavelet Transform (RDWT) is an effective \nmethod for extracting features in image processing and is very valuable \nin healthcare fields like cancer diagnosis. RDWT, unlike conventional \nDWT, does not include down-sampling in the transformation process, \ntherefore maintaining the spatial resolution of the image at various \nscales. This leads to a representation that is not affected by shifts, which \nis essential for effectively presenting the intricate characteristics of \nmedical imaging. Midasala et al. (2024) used multilevel feature \nextraction using GLCM and RDWT for skin cancer detection and \nclassification.\n3.4.4. Other feature extraction methods\nObayya et al. (2023) employed GhostNet with AFAO-based hyper\u00ad\nparameter tuning for lung and colon cancer detection. Alqarafi et al. \n(2024) used a multi-scale Graph Convolution Network (M-GCN) for \nfeature extraction. Kumar et al. (2024) implemented a Spatial Gray-level \nDependency Matrix (SGLD) for feature extraction from the brain tumour \nimages.\n3.5. Deep learning algorithms\n3.5.1. Convolutional neural network\nConvolutional Neural Networks (CNNs) (O\u2019shea and Nash, 2015) are \npopular for cancer detection and segmentation because they extract \nuseful data and identify patterns. Medical imaging CNN architectures \nare intended to handle MRI, CT, and histopathology images. Cancer \ndetection CNNs include numerous layers that can independently discern \ntumour edges, textures, and forms. After these layers, pooling layers \nreduce spatial dimensions and computational effort while keeping \nimportant characteristics. CNNs use fully linked layers with upsampling \nand skip connections, like the U-Net architecture, to accurately distin\u00ad\nguish malignant areas. Training on annotated medical datasets allows \nthese networks to discriminate between benign and malignant tissues. \nCNNs can precisely find and classify tumours, improving cancer diag\u00ad\nnosis, treatment planning, and disease progression. Automatic hyper\u00ad\nparameter tuning algorithms, such as Grey Wolf Optimization (GWO) \n(Mohakud and Dash, 2022), Particle Swarm Optimization (PSO), Ge\u00ad\nnetic Algorithm (GA), and Chimp Optimization Algorithm (COA) \n(Marzouk et al., 2022), are used to optimize CNN hyper-parameters. \nHybrid optimization algorithms, like PSOBER (Myriam et al., 2023), \nwhich is a combination of PSO and Al-Biruni Earth Radius (BER) algo\u00ad\nrithms, can be used for CNN optimization. Lu et al. (2021) proposed \nmetaheuristic optimization (i.e., marine predators algorithm(MPA)) \nwith CNN to enhance the accuracy of lung cancer detection.\n3.5.2. Regional CNN\nRegional Convolutional Neural Networks (R-CNNs) are sophisticated \nCNN designs designed to identify and segment an object\u2019s specific re\u00ad\ngions of interest (ROIs). The original R-CNN employs selective search to \ngenerate region suggestions, which are then processed by a CNN to \nextract features. Subsequently, machine learning algorithms are used for \nclassification. Fast R-CNN (Girshick, 2015) enhances speed by using an \nRoI pooling layer to analyze the full image simultaneously. Faster \nR-CNN (Ren et al., 2015) incorporates a Region Proposal Network (RPN) \nto produce proposals directly from feature maps, enhancing efficiency. \nMask R-CNN (He et al., 2017) enhances Faster R-CNN by including an \nadditional component that predicts segmentation masks, enabling ac\u00ad\ncurate tumour segmentation. The R-CNN versions are very efficient in \ndetecting cancer and accurately identifying and localizing tumours \nusing different imaging techniques. This improves diagnostic precision \nand aids in focused treatment planning.\n3.5.3. YOLO\nYOLO (You Only Look Once) (Ragab et al., 2024) is a real-time object \nidentification architecture that has shown its efficacy in cancer diagnosis \nin medical imaging due to its rapidity and precision. YOLO approaches \nobject recognition as a singular regression task, directly predicting \nbounding boxes and class likelihoods for objects inside the whole picture \nin a single iteration. This single-stage design, including multiple con\u00ad\nvolutional layers, simultaneously processes the whole image, signifi\u00ad\ncantly reducing computation time compared to conventional multi-stage \napproaches. The image is partitioned into a grid by the architecture, \nwhere each grid cell has the task of predicting a certain number of \nbounding boxes, confidence scores, and class probabilities. Due to its \nhigh efficiency and real-time object detection capabilities, YOLO is \nwell-suited for rapidly evaluating large collections of medical images.\n3.5.4. Generative adversial networks\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2020) \ninclude two neural networks: the generator and the discriminator. The \ngenerator generates artificial data samples, while the discriminator as\u00ad\nsesses these samples to differentiate between authentic and generated \ndata. The adversarial process facilitates the enhancement of the gener\u00ad\nator\u2019s outputs to the point where they become indistinguishable from \nauthentic data. GANs are crucial for cancer diagnosis because of their \ncapacity to generate synthetic medical imaging of superior quality. \nThese images may be used to enhance the accuracy of training datasets \nand diagnostic models. GANs possess a structural design that allows \nthem to learn about intricate patterns in data distributions. This ability \nmakes them very efficient in producing lifelike tumour images, which \nmay then be used to enhance the precision of CNNs in identifying and \nisolating malignant areas. In addition, GANs may contribute to \nadvancing data augmentation methods, which can improve the resil\u00ad\nience and applicability of cancer detection algorithms. The capacity to \ngenerate a wide range of quality medical images significantly improves \nthe ability to detect diseases early, develop treatment strategies, and \nimprove the overall quality of care for cancer patients.\n3.5.5. Vision transformer\nThe core function of Vision Transformer (ViT) (Dosovitskiy et al., \n2020) is a self-attention mechanism (Vaswani et al., 2017) that un\u00ad\nderstands the context and accesses previous information. The Vision \nTransformer (ViT) operates through a sequence of key steps: it first di\u00ad\nvides the input image into fixed-size patches, each linearly transformed \ninto a vector to form patch embeddings. Positional encodings are added \nto these embeddings to provide spatial context. The model\u2019s core com\u00ad\nprises multiple encoder layers, each containing multi-head self-attention \n(see Equation (8) Vaswani et al., 2017) and feedforward neural net\u00ad\nworks. The self-attention mechanism captures relationships between \npatches, allowing the model to focus on important regions by computing \nweighted sums of patch embeddings. Following this, feedforward neural \nnetworks introduce non-linearity and learn complex relationships. Layer \nnormalization and residual connections stabilize and enhance each \nsub-layer\u2019s outputs, ensuring effective training and preventing vanish\u00ad\ning gradients. Sriwastawa and Arul Jothi (2024) implemented vision \ntransformer variants for breast cancer detection using Breakhis and IDC \ndatasets. Some vision transformer variants are CvT, CrossFormer, \nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n7 \n\nCrossViT, etc. The related code for each variant can be found on Github.1\nMultiHead(M, N, P) = Concat(head1, \u2026., headh)WO\nwhere, headi = Attention(MWM\ni , NWN\ni , PWP\ni )\nAttention(M, N, P) = softmax(MNT\n\u0305\u0305\u0305\u0305\u0305\u0305\ndN\n\u221a\n)P\n(8) \n3.6. Transfer learning\n3.6.1. VGGNet\nThe VGGNet (Sengupta et al., 2019) architecture is known for its \nstraightforward design, which includes the incorporation of compact 3 \n\u00d7 3 convolutional filters. The VGG16 and VGG19 are two prominent \nversions of the VGGNet, which differ in terms of their quantity of \ntrainable parameters and layers. VGG16 is composed of sixteen weight \nlayers, with thirteen being convolutional layers (2 layers of 64 filters, 2 \nlayers of 128 filters, 3 layers of 256 filters, 3 layers of 512 filters, and 3 \nlayers of 512 filters) and three being fully connected (FC) layers (4096, \n4096, 1000 units). VGG-19 is more extensive than VGG-16, boasting a \ntotal of 16 convolutional layers (2 layers of 64 filters, 2 layers of 128 \nfilters, 4 layers of 256 filters, 4 layers of 512 filters, and 4 layers of 512 \nfilters) and three fully connected (FC) layers (4096, 4096, 1000 units). \nBoth VGG16 and VGG19 have 5 Max-Pooling layers (2\u00d72) after each set \nof convolutional layers. Although VGGNet is known for its simplicity, it \nrequires a significant amount of processing resources due to its \ncomplexity and dependence on 3 \u00d7 3 filters. Because of its simple \ncomposition, it has been thoroughly investigated in the field of medical \nimage classification.\n3.6.2. ResNet\nResNet (Residual Networks) (Zagoruyko and Komodakis, 2016) is a \ndeep CNN that uses residual learning to enable the training of very deep \nnetworks. This is achieved by using skip connections, which effectively \naddress the issue of the vanishing gradient problem. ResNet-50 com\u00ad\nprises 50 layers, organized in a sequence of residual blocks, each \nincluding three convolutional layers. ResNet-101 enhances this design \nby including 101 layers using a larger quantity of residual blocks. \nResNet-152 is an extension of the ResNet model that increases the depth \nof the network to 152 layers.\n3.6.3. Inception\nInceptionNetV1 (Szegedy et al., 2016), or GoogLeNet (Szegedy et al., \n2015; Al-Huseiny and Sajit, 2021), is a complex CNN structure that \nintroduced the inception module, a pivotal advancement to enhance \ncomputing efficiency while maintaining superior performance. The \ninception module conducts convolutions using various filter sizes (1\u00d71, \n3\u00d73, and 5\u00d75) and a 3\u00d73 max pooling operation simultaneously, \ncombining their results along the depth dimension. This enables the \nnetwork to record many spatial elements and sizes concurrently, \nenhancing its ability to recognize intricate picture patterns. The first \nInceptionNet, comprising 22 layers, effectively decreased the parameter \ncount compared to conventional CNNs due to using 1\u00d71 convolutions \nthat reduce dimensionality before performing more computationally \nintensive procedures. Later iterations, such as InceptionV2, V3, and V4, \nincluded improvements and enhancements, such as factorized convo\u00ad\nlutions and regularization algorithms, resulting in better accuracy and \nefficiency. The modular and scalable architecture of InceptionNet has \nhad a significant impact, leading to breakthroughs in developing deep \nlearning models for many computer vision applications. The modular \nand scalable architecture of InceptionNet has significantly impacted the \ndevelopment of deep learning models for a range of computer vision \ntasks.\n3.6.4. DenseNet\nDenseNet (Iandola et al., 2014) is a deep learning architecture that \naims to enhance the flow of information and gradients within the \nnetwork. In contrast to CNN, which only has direct connections to its \nsubsequent layers, DenseNet establishes direct connections between \neach layer and every other layer in a feed-forward manner. This implies \nthat the feature maps from previous layers are utilized as inputs for each \nlayer, and their outputs are transmitted to all subsequent layers. This \nhighly interconnected pattern improves the reuse of features and ad\u00ad\ndresses the issue of the gradient vanishing problem, making it easier to \ntrain \nextremely \ndeep \nnetworks. \nDenseNet \narchitectures, \nlike \nDenseNet-121, DenseNet-169, and DenseNet-201, are represented by \ntheir depth (number of layers) and dense blocks, establishing connec\u00ad\ntions between layers. The connectivity between layers in DenseNets \nresults in better parameter utilization and enhanced performance, \nmaking them a powerful tool for image classification and computer \nvision tasks.\n3.6.5. AlexNet\nAlexNet transformed image processing by introducing a ground\u00ad\nbreaking deep convolutional neural network architecture. AlexNet \n(Krizhevsky et al., 2017) is composed of a total of eight layers. These \nlayers include five convolutional layers and three fully connected layers. \nThe architecture includes max-pooling layers following specific con\u00ad\nvolutional layers to decrease spatial dimensions and computational \nload. AlexNet utilizes ReLU activations to introduce non-linearity, \nresulting in faster training and addressing the vanishing gradient prob\u00ad\nlem. In addition, dropout is used in the fully connected layers to prevent \noverfitting. An important breakthrough of AlexNet was the incorpora\u00ad\ntion of data augmentation and GPU acceleration to effectively manage \nthe computational requirements of training on extensive datasets such as \nImageNet.\n3.6.6. MobileNet\nMobileNet is a collection of compact deep neural network structures \nfor efficient implementation in mobile and embedded vision tasks. \nMobileNet utilizes depthwise separable convolutions to divide the \nconvolution process into two steps: a depthwise convolution and a \npointwise convolution. This approach effectively decreases the number \nof parameters and computing expenses when compared to conventional \nconvolutions. The MobileNet evolution comprises multiple versions: \nMobileNetV1 (Howard et al., 2017), which proposed depthwise sepa\u00ad\nrable convolutions; MobileNetV2 (Sandler et al., 2018), which incor\u00ad\nporated inverted residuals and linear bottlenecks to enhance \nperformance and efficiency; and MobileNetV3 (Howard et al., 2019), \nwhich integrated advancements such as squeeze-and-excitation modules \nand neural architecture search (NAS) to improve performance further \nand decrease latency. MobileNet topologies are very advantageous for \ncancer detection because they excel at precise image analysis on devices \nwith limited resources. This allows for real-time processing and diag\u00ad\nnosis in clinical situations with restricted computing capabilities.\n3.7. Evaluation metrics\nEvaluation metrics are used to evaluate the model\u2019s performance by \nestimating the accuracy of a model on unseen data samples. Four basic \nconcepts of evaluating metrics are true positive, true negative, false \npositive, and false negative. These terms are described below: \n\u2022 True Positive (TP): predicted as true and indeed true.\n\u2022 True Negative (TN): predicted as false and indeed false.\n\u2022 False Positive (FP): predicted as true and indeed false.\n\u2022 False Negative (FN): predicted as false and indeed true.\n1 https:\/\/github.com\/lucidrains\/vit-pytorch\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n8 \n\n3.7.1. Accuracy\nAccuracy is the proportion of accurately predicted cases (true posi\u00ad\ntives and true negatives) out of all instances. It quantifies the overall \naccuracy of the system\u2019s predictions. \nAccuracy =\nTP + TN\nTP + TN + FP + FN\n(9) \nThe accuracy of a model that accurately predicts 80 out of 100 ma\u00ad\nlignant photos and detects 90 out of 100 non-cancerous images would be \ncalculated as (80 + 90) \/ 200 = 0.85 or 85 %. This suggests the model \naccurately predicts outcomes with an 85 % success rate.\n3.7.2. Precision\nPrecision is the quotient obtained by dividing the number of correct \npositive predictions by the sum of correct positive predictions and \nincorrect positive predictions. It quantifies the model\u2019s precision in \ncorrectly detecting positive cases only. \nPrecision =\nTP\nTP + FP\n(10) \nGiven a segmentation model that detects 100 areas as malignant, \nwith 80 of them being really cancerous and 20 being false positives, the \nprecision will be calculated as 80 \/ (80 + 20) = 0.8 or 80 %. This in\u00ad\ndicates that the model accurately identifies 80 % of the locations as \nmalignant or diseased.\n3.7.3. Recall \/ sensitivity \/ true positive rate\nRecall, also known as sensitivity, is the proportion of correct positive \npredictions to the total number of positive predictions, including both \ntrue positives and false negatives. The metric quantifies the model\u2019s \ncapacity to accurately detect all relevant positive cases. \nRecall =\nTP\nTP + FN\n(11) \nIf there are 100 malignant areas in the dataset and the model accu\u00ad\nrately detects 80 (missing 20), the recall would be calculated as 80 \/ (80 \n+ 20) = 0.8 or 80 %. This suggests the model accurately detects 80 % of \nthe real malignant areas.\n3.7.4. F1-score\nThe F1-score is a statistical measure that calculates the harmonic \nmean of accuracy and recall, thereby balancing the two measurements. \nIt is advantageous when there is an uneven distribution of classes in the \ndataset. \nF1 \u2212score = 2\u22c5Precision\u22c5Recall\nPrecision + Recall\n(12) \nGiven the precision of 80 % and recall of 80 % from the previous \ninstances, the F1-score can be computed as 2 * (0.8 * 0.8) \/ (0.8 + 0.8) =\n0.80 or 80 %.\n3.7.5. Specificity \/ true negative rate\nSpecificity refers to the accuracy of accurately classifying the nega\u00ad\ntive class. \nspecificity =\nTN\nTN + FP\n(13) \n3.7.6. False positive rate and false negative rate\nThe False Negative Rate (FNR) indicates the percentage of instances \nbelonging to the positive class that were wrongly identified by the al\u00ad\ngorithm. The false positive rate (FPR) quantifies the proportion of in\u00ad\nstances from the negative class that the algorithm mistakenly classified \nas positive. It is preferable to have a greater TNR and a lower FPR to \naccurately categorize instances of the negative class. \nFPR = 1 \u2212specificity =\nFP\nTN + FP\n(14) \nFNR =\nFN\nTP + FN\n(15) \n3.7.7. AUC-ROC curve\nThe AUC-ROC quantifies the model\u2019s capacity to differentiate be\u00ad\ntween positive and negative classifications. The ROC curve graphically \nrepresents the relationship between the true positive rate (recall) and \nthe false positive rate (1-specificity) at different threshold values.\nA cancer detection model with an AUC-ROC value of 0.95 demon\u00ad\nstrates exceptional discriminatory power in distinguishing between \nmalignant and non-cancerous areas. This implies that the model has a \nstrong capability to identify both positive and negative cases accurately. \nA model that is considered flawless has an Area Under the Curve (AUC) \nvalue of 1.0, whereas a model that lacks discriminating power has an \nAUC value of 0.5.\n3.7.8. Kappa score\nCohen\u2019s Kappa (Cohen, 1960) was first introduced to measure the \nlevel of consensus between two analysts who evaluated the same group \nof individuals using a nominal scale with two or more categories. The \nmetric is often used for binary classification issues. \nk =\n2\u22c5(TP\u22c5TN \u2212FP\u22c5FN)\n(TP + FP)\u22c5(TP + FN)\u22c5(TP + FN)\u22c5(TN + FN)\n(16) \n3.7.9. Matthews correlation coefficient (MCC)\nTo address imbalanced datasets, the Matthews correlation coefficient \noffers an alternative method to calculate the Pearson product-moment \ncorrelation coefficient between actual and projected values using a \ncontingency matrix (Chicco and Jurman, 2020). The lowest possible \nvalue of MCC is \u2212\n1, and the highest possible value is +1. \nMCC = (TP\u22c5TN) \u2212(FP\u22c5FN)\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\nA\u22c5B\u22c5C\u22c5D\n\u221a\nA = (TP + FP), B = (TP + FN)\nC = (TN + FP), D = (TN + FN)\n(17) \n3.7.10. Jaccard index and dice coefficient\nJaccard Index and Dice Coefficient are the most used evaluation \nmetrics for segmentation. Jaccard index, also known as Intersection- \nover-Union (IoU) or Jaccard similarity coefficient (see Equation (18)). \nDice Coefficient (DC), also known as or S\u00f8rensen-Dice index (see \nEquation (19)). \nJaccard Index =\nTP\nTP + FP + FN\n(18) \nDice Coefficient =\n2TP\n2TP + FP + FN\n(19) \n4. Case study of cancer detection\nThis section discusses the case study of cancer, including each cancer \ndataset and related research works. Figure 3 shows the 12 types of \ncancers in medical imaging images, including breast cancer (Fig. 3a), \ncervical cancer (fig. 3b), ovarian cancer (fig. 3c), prostate cancer (fig. \n3d), esophageal cancer (fig. figu3e), liver cancer (fig. figu3f), pancreatic \ncancer (fig. figu3g), colon cancer (fig. 3h), lung cancer (fig. figu3i), oral \ncancer (fig. figu3j), skin cancer (fig. figu3k), and brain cancer (fig. \nfigu3l). The Cancer Imaging Archive (TCIA2) include most of the cancer \n2 https:\/\/wiki.cancerimagingarchive.net\/\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n9 \n\ndataset for detection and segmentation (Clark et al., 2013). This study \nused the following acronyms in Tables 4, 5, 6, 7, and 8 to show the \nexisting research outcomes: acc=accuracy, pr=precision, re=recall, and \nf1=f1-score.\n4.1. Reproductive system\n4.1.1. Breast cancer\nBreast cancer is defined as the uncontrolled growth of breast cells \nthat results in tumour formation. These tumours may spread and \npotentially lead to death. In 2022, it resulted in a total of 670,000 deaths \nworldwide. Women account for approximately 50 % of breast cancer \noccurrences because of their gender and age (Breast cancer, 2024). Men \nmay also be affected by breast cancer; globally, males account for \n0.5\u20131 % of cases (Aygin and Yaman, 2022). Consequently, early detec\u00ad\ntion of this cancer is critical for its treatment. Radiologists diagnose \nbreast cancer using the most accurate procedure known as mammog\u00ad\nraphy. Apart from that, other screening tests are breast MRIs and ul\u00ad\ntrasounds, which diagnose breast cancer.\nBreast cancer dataset. Some mammogram image datasets are MIAS \n(Suckling et al., 2015),3 Mini-MIAS,4 INBreast (Moreira et al., 2012),5\nDDSM (Heath et al., 1998), and CBIS-DDSM (Lee et al., 2017). The \nMini-MIAS dataset is a subset of the UK-based Mammographic Image \nAnalysis Society (MIAS) (Suckling et al., 2015) database. It consists of \n322 images for 161 cases, each with extensive annotations concerning \nabnormalities. This dataset is highly helpful for developing and testing \nfundamental algorithms for breast cancer diagnosis. INBreast is a dataset \ncomprising 410 full-field digital mammograms from 115 cases. It pro\u00ad\nvides high-resolution pictures with precise annotations, making it \nwell-suited for developing sophisticated algorithms and performing \ntasks that require accurate detection and classification of lesions. Digital \nDatabase for Screening Mammography (DDSM) (Heath et al., 1998) \ncontained 55,890 images, including 14 % benign and 86 % malignant. \nCBIS-DDSM6 is an enhanced and carefully selected version of the DDSM. \nIt consists of 2620 studies that have detailed annotations at the pixel \nlevel for masses and calcifications. This extensive dataset enables the \ntraining of powerful algorithms and the evaluation of their performance, \nhence promoting the creation of dependable computer-aided diagnosis \nsystems. Li et al. (2021a) employed three datasets, including BreakHis \n(Spanhol et al., 2015),7 BACH (Aresta et al., 2019), and PUIH (Yan et al., \n2020). The BreaKHis dataset consists of 7909 histopathological images \nthat include three colour channels (RGB) and were captured at four \ndifferent magnification levels: 40x, 100x, 200x, and 400x. The BACH \ndataset consists of 400 RGB images, whereas the PUIH dataset has 4020 \nFig. 3. Different Types of Cancer Image.\n3 https:\/\/www.repository.cam.ac.uk\/items\/b6a97f0c-3b9b-40ad-8f18-3d1 \n21eef1459\n4 https:\/\/www.kaggle.com\/datasets\/kmader\/mias-mammography\n5 https:\/\/www.kaggle.com\/datasets\/ramanathansp20\/inbreast-dataset\n6 https:\/\/wiki.cancerimagingarchive.net\/pages\/viewpage.action?page \nId=22516629\n7 https:\/\/www.kaggle.com\/datasets\/ambarish\/breakhis\/data\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n10 \n\nRGB images. However, the specific zoom levels for these images have \nnot been disclosed. The BreaKHis dataset has images with a resolution of \n700 \u00d7 460 pixels, whereas the BACH and PUIH datasets provide images \nwith a resolution of 2048 \u00d7 1536 pixels.\nThere are some B-mode breast ultrasound (BUS) imaging databases, \nsuch as a private dataset from Fudan University Shanghai Cancer Center \n(FUSCC) (Wang et al., 2024). The FUSCC dataset contains BUS images \nthat were obtained from an overall 176 individuals, with 89 patients \nhaving benign tumours and 87 patients having malignant malignancy. \nEvery patient was assigned a video file, and 10 images were randomly \nextracted from each ultrasound video. There were a total of 890 benign \nand 870 malignant samples. The Mindary Resona7 ultrasonic scanner \nwas used to scan all the samples. The scanning was done using the L11\u20133 \nlinear-array probe. An expert sonologist marked a rectangular region of \ninterest (ROI) on each BUS image to show the extent of the lesion. \nTable 4 \nReproductive System: Breast Cancer (2020\u20132024).\nYear & Ref.\nDataset\nMethodology\nResult\n2024 Shah et al. \n(2024)\nDDSM\nemployed DCGAN to \ngenerate synthetic \nmammogram images \nand validated by \nradiologists to assess the \nperceptual quality and \nrealism\nt-statistic was 6.35 \nwith a p-value <\n0.001\nSriwastawa and \nArul Jothi \n(2024)\nBreakHis, \nIDC\nused vision transformer \nvariants: ViT, PiT, CvT, \nCrossFormer, CrossViT, \nNesT, MaxViT, and \nSepViT\nacc: 91.57 % \n(BreakHis), 91.8 % \n(IDC)\nAnas et al. \n(2024)\nINbreast, \nCBIS- \nDDSM, BNS\nYOLOV5 for mass \ndetection and \nclassification and Mask \nR-CNN for tumour \nborders and size \ndetection\nFPR: 0.049 %, FNR: \n0.029 %, MCC: \n92.02 %\nAdmass et al. \n(2024)\nAFG\nproposed locally \npreserving projection \ntransformation function \nto ensure local features \nwere retained into the \nGoogle inception \nnetwork\nacc: 99.81 %, re: \n96.48 %\nKhan et al. \n(2024)\nDDSM\nemployed Decentralized \nFederated Learning to \nenable collaborative \nlearning, Ant Colony \nOptimization for \nhyperparameter fine- \ntuning, and Neural \nArchitecture Search for \noptimizing NN \narchitecture\nacc: 93.0 %, re: \n92.6 %, specificity: \n93.0 % (Benign vs. \nMalignant)\nWang et al. \n(2024)\nFUSCC, \nBUSI, BLUI\nproposed a two-stage \ntraining strategy with D- \nNet, C-Net, and F-Net: \nD-Net and C-Net trained \non fully annotated \nimages with ROI-and \nimage labels and a self- \ndistillation mechanism \nsqueezes knowledge \nfrom the F-net into the \nD-Net and C-Net to fine- \ntune them.\nacc: 92.46 \u00b1 1.78, \nre: 92.86 \u00b1 2.38, \nspecificity: 92.27 \n\u00b1 2.17 (BUSI)\nTeoh et al. \n(2024)\nmini-MIAS, \nINBreast, \nCBIS-DDSM\nproposed ensemble \noptimization (AlexNet, \nGoogLeNet, VGG16, \nResNet\u221250) to detect \nand localize areas of \nmicrocalcifications\navg. confidence \nlevel: 0.9305 and \n0.8859 for \nabnormal and \nnormal cases\nZeng et al. \n(2024)\nBreaKHis\nimproved ResNet \narchitecture \n(FastLeakyResNet-CIR)\nacc: 98.94 %\n2023 Jafari and \nKarami (2023)\nRSNA, \nDDSM, \nMIAS\nemployed pre-trained \nCNN model for feature \nextraction and ML \nalgorithms for \nclassification\nacc: 92 % (RSNA), \n94.5 % (MIAS), \n96 % (DDSM)\nSharmin et al. \n(2023)\nIDC\nproposed resnet50v2 \nand ensemble-based ML \nmethods\nacc: 95 %, pr: \n94.86 %, re: \n94.32 %, f1: \n94.57 %\nBoudouh and \nBouakkaz \n(2023)\nMini-MIAS, \nDDSM, \nCMMD\nXception, InceptionV3, \nResNet101V2, \nResNet50V2, AlexNet, \nVGG16, VGG19\nacc: 99.9 % \n(ResNet50V2), \n99.54 % \n(InceptionV3)\nStrelcenia and \nPrakoonwit \n(2023)\nWBCD\nKullback-Leibler \nDivergence Conditional \nGAN (K-CGAN)\nacc: 90.86 %, pr: \n97.62 %, re: \n77.36 %, f1: \n86.31 %\nTable 4 (continued)\nYear & Ref. \nDataset \nMethodology \nResult\nFuentes-Fino \net al. (2023)\nINbreast, \nCBIS- \nDDSM, CRC\nAlexNet, DenseNet, and \nMobileNet\nacc: 69 % \n(mobileNet)\nAsadi and \nMemon (2023)\nINbreast, \nBCDR, \nWBCD\nIntegrate U-Net for \nsegmentation and \nResNet50 for \nclassification\nacc: 98.61 %, f1: \n98.41 %\nKashyap (2023)\nBreakHis, \nBreCaHad\nproposed stochastic \ndilated residual ghost \nmodel for cancer \ndetection\nAUC: 96.15\nGade et al. \n(2023)\ninfrared \nimage\n2D empirical wavelet \ntransform was used for \nmultiscale analysis\n99.54 %\nChen et al. \n(2023)\nPrivate\nemployed wavelet \ndecomposition and \nweighted recurrence \nnetwork\nAUC: 0.96\nToma et al. \n(2023)\nBreaKHis\nResNext\u221250, \nResNext\u2212101, DPN131, \nDenseNet\u2212169 and \nNASNet-A\nacc: 99.8 % \n(ResNext\u221250)\n2022 Gon\u00e7alves \net al. (2022)\nInfrared \nimage\nVGG\u221216, ResNet\u221250 \nand DenseNet\u2212201\nf1: 92 % \n(VGG\u221216), 90 % \n(ResNet\u221250)\nAzevedo et al. \n(2022)\nBCDR\nproposed quantum \nneural networks using \ntransfer learning\nacc: 84 %\nAl Husaini et al. \n(2022)\nDMR\nemployed inception V3, \ninception V4, and \nmodified inception MV4\nacc: 99.748 %\nDey et al. (2022)\nDMR\nemployed prewitt and \nroberts for edge \ndetection and \nDenseNet121\nacc: 98.8 %\nLu et al. (2022)\nUSI\npre-trained ReNet\u221218 \nwith spatial attention \nmechanism\nacc: 94.10 %, pr: \n98.14 %, f1: \n96.50 %\nAlruwaili and \nGouda (2022)\nMIAS\nResNet50 and NasNet- \nmobile network\nacc: 89.5 % \n(ResNet50)\n2021 \nS\u00b4anchez-Cauce \net al. (2021)\nDMR\nproposed multi-input \nmodel integrating \nthermal images and \nclinical data\nacc: 97 %, \nAUCROC: 0.99\nLi et al. (2021a)\nBreakHis, \nBACH, \nPUIH\nmulti-view attention- \nguided multiple \ninstance detection \nnetwork\nAUC:0.99 \n(BreakHis), 0.98 \n(BACH),00.98 \n(PUIH)\nSaber et al. \n(2021)\nMIAS\nVGG16, VGG19, \nResNet50, InceptionV3, \nInceptionV2ResNet\nacc: 98.96 %, re: \n97.83 %, pr: \n97.35 %, f1: \n97.66 %\n2020 Zheng \net al. (2020)\nTCIA\nCNN with Adaboost\nacc: 97.2 %, re: \n98.3 %, specificity: \n96.5 %\nWang et al. \n(2020)\nprivate\n3D convolutional \nnetwork\nsensitivity: 95 %\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n11 \n\nAnother private dataset was AFG (Admass et al., 2024), which was \ndeveloped by Assosa General Hospital, Felege-Hiwot Hospital, and \nGondar University Hospital. Some public datasets like BUSI \n(Al-Dhabyani et al., 2020) and BLUI (Ardakani et al., 2023) are also \navailable for breast cancer detection. The BUSI dataset was gathered \nfrom the Baheye Hospital and consisted of data collected in 2018 from \n600 female patients aged between 25 and 75 years old. The dataset has \n780 images with an average of 500 \u00d7 500 pixels. The images are cate\u00ad\ngorized into normal, benign, and malignant. Given that this particular \ndataset only includes labels and masks at the image level to segment \nlesions. The BLUI dataset was collected from the Shahid Beheshti Uni\u00ad\nversity of Medical Sciences. The dataset has a total of 232 BUS images, \nconsisting of 109 benign cases and 123 malignant instances. All images \nwere annotated based on the histopathologic research findings, which \nincluded fine needle aspiration, core needle, or open biopsies. The ROI \nin every image was marked by a radiologist with 20 years of expertise.\nRadiological Society of North America (RSNA)8 provided 54,713 \nimages in Digital Imaging and Communications in Medicine (DICOM) \nformat from roughly 11,000 patients. The CRC (Calderon-Ramirez et al., \n2022) dataset comprises 87 cases of patients aged between 40 and 90 \nyears old, obtained from the private clinic of Chavarria Estrada Medical \nImaging in Costa Rica. Silva et al. (2014) proposed an infrared image \ndataset for breast cancer detection (Gade et al., 2023; Gon\u00e7alves et al., \n2022). The University of Pennsylvania and the Cancer Institute of New \nJersey have presented a histopathological image dataset (Cruz-Roa \net al., 2014) comprising 279 whole slide images (WSI) of breast cancer \nspecimens obtained from 162 women. University Hospital Ant\u02c6onio \nPedro (HUAP) of the Federal Fluminense University of Brazil provided a \nMastology Research (DMR) (Al Husaini et al., 2022; Dey et al., 2022) \ndatabase for thermal images to detect breast cancer. The invasive Ductal \nCarcinoma (IDC)9 dataset consists of 162 whole slide pictures (WSIs), \nwhich were enlarged at 40x. From these images, a total of 277,524 \npatches were recovered, including 198,738 negative patches and 78,786 \npositive patches. Admass (Admass et al., 2024) collected breast ultra\u00ad\nsound images from Assosa General Hospital, Gondar University Hospi\u00ad\ntal, and Felege-Hiwot Hospital. Wisconsin Breast Cancer Database \n(WBDC)10 has 699 records of Fine Needle Aspirates (FNA) generated \nTable 5 \nReproductive System Cancer (2020\u20132024).\nYear & Ref.\nDataset\nMethodology\nResult\nCervical\n2024 Fahad et al. (2024)\nSipakMed, Herlev\nemployed random forest feature importance and information gain as \nfeature ranking techniques, GCN was introduced for cervical cell types \nprediction\nacc: 99.11 % (SipakMed), \n98.18 % (Herlev)\n2023 Mazroa et al. (2023)\nHerlev\nCLAHE was used for contrast enhancement, LeNet model for feature \nextraction, and Attention-based Long Short-Term Memory with \nImproved Bald Eagle Search Optimization (IBESO) for classification.\nacc: 99.26 %, pr: 97.51 %, re: \n97.11 %, f1: 97.29 %\nSahoo et al. (2023)\nSipakmed, Mendeley \nLBC\nproposed fuzzy rank-based ensemble approach by integrating three pre- \ntrained models (VGG, Inception and ResNet) for feature extraction and \nprediction\nacc: 97.18 % (Sipakmed), f1: \n97.16 %(Sipakmed)\n2022 Ji et al. (2022)\n71 flim images\nproposed unsupervised machine learning method for classification\nre: 90.9 %, specificity: 100 %\nDevi et al. (2022)\nHerlev\nproposed segmentation technique based on neutrosophic graph cuts \n(NGCS)\nacc: 99.42 %, re: 98.52 %, \nspecificity: 99.42 %\nElakkiya et al. (2022)\nMobileODT\nproposed faster small-object detection neural networks (fsod-gan) for \nclassification\nacc: 99 %\n2021 Pirovano et al. (2021)\nprivate dataset\na dataset simulating regions of Pap smears was created for training and \ntesting the classifier. loss function, named classification under \nregression constraint, was used to train the region classifier\nacc: 95 %\nBhatt et al. (2021)\nSipakmed, Herlev\nConv Net with Transfer Learning was used for feature extraction and \nGradient-weighted Class Activation Mapping (Grad-CAM) was used for \nvisual localization of lesions in images\nacc: 99.70 %, pr: 99.70 %, re: \n99.72 %, f-beta: 99.63 %, kapp: \n99.31 %\nOvarian\n2023 Mukhedkar et al. (2023)\nPelvic-CT image\nproposed Bi-LSTM with CNN to enhance learning and employed Lion \nwith Grey Wolf Optimization (LGWO) for efficient feature selection\nacc: 98 %, pr: 93 %, re: 99.7 %, \nf1: 98 %\nPaayas and Annamalai (2023)\nUBC-OCEAN\nDenseNet121\nacc: 99.6 %, f1: 100 %\n2022 Schwartz et al. (2022)\nOCT recordings\nemployed VGG-supported Feed-forward Network and 3D CNN to \ncapture spatial relationships in data\nAUC: 0.81 \u00b1 0.037\nProstate\n2023 Nishio et al. (2023)\nPanda\nemployed EfficientNet B3 and Label distribution learning\nacc: 40.7 %\nBashkanov et al. (2023)\n1074 mpMRI\n3D nnU-Net Architecture was used for anisotropy in MRI data. Cancer \nfindings were categorized into coarse, medium, and fine granularities.\nLesion-wise partial FROC-AUC: \n1.94\nWilson et al. (2023)\n1028 biopsy cores\nSelf-Supervised Learning (SSL) is implemented to leverage the \nabundance of unlabeled micro-ultrasound data\nAUROC: 91 %\n2021 Duran-Lopez et al. (2021)\nHematoxylin, Eosin- \nstained slides\nCNN is used to obtain patch-level features and aggregated these using \ncustom wide and DNN model\nacc: 94.24 %, re: 98.87 %\nQian et al. (2021)\nProstateX, \nPROMISE12\nemployed a 3D prostate cancer lesion segmentation network based on \nfocal Tversky loss\nTPR: 91.82 %\nIqbal et al. (2021)\nCarcinoma Images\napplied LSTM and ResNet\u2212101 and compared with Hand-Crafted \nFeatures (texture, morphology, and GLCM) and ML algorithms\nacc for GLCM with KNN-cosine: \n99.07 %\nPinckaers et al. (2021)\n4712 biopsy\nproposed ResNet34 for feature extraction\nAUC: 0.909\nSaha et al. (2021)\n1950 bpMRI\ndeep attention mechanisms were proposed to enhance detection \nperformance, and decoupled residual classifier is used to reduce false \npositive\nAUROC: 0.882 \u00b1 0.030\nde Vente et al. (2021)\nProstateX\u22122\n2D U-Net was used to lesion segmentation maps and ensemble approach \nimplemented to enhance the accuracy\nWeighted Kappa: 0.13 \u00b1 0.27 \n(lesion-wise)\n2020 Duran-Lopez et al. (2020)\nprivate dataset\nCNN is employed for cancer detection\nacc: 99.98 %, f1: 99.98 %, AUC: \n0.999\nSedghi et al. (2020)\n145 biopsy cores\nproposed fully convolutional networks (FCN) based on U-Net and U-Net \nwith attention gates\nAUC: 0.76\n8 https:\/\/kaggle.com\/competitions\/rsna-breast-cancer-detection\n9 https:\/\/www.kaggle.com\/datasets\/paultimothymooney\/breast-histopatho \nlogy-images\n10 https:\/\/archive.ics.uci.edu\/dataset\/17\/breast+cancer+wisconsin+diagnos \ntic\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n12 \n\nfrom breast tissue. Every record has nine properties, consisting of 239 \n(35.0 %) malignant and 444 (65.0 %) benign samples. Lu et al. (2022)\nUltrasound Images Dataset (USI)11 for breast cancer detection.\n4.1.2. Cervical cancer\nCervical cancer is the fourth most prevalent disease among women \nglobally, with around 660,000 new cases projected for 2022 (Cervical \ncancer, 2024). In the same year, 94 % of the 350,000 fatalities caused by \ncervical cancer took place in low- and middle-income countries. The \ndiscrepancy may be attributed to inequitable availability of vaccines, \nscreening, and treatment, along with elevated HIV prevalence and so\u00ad\ncioeconomic variables, including gender biases and poverty. Women \nwho have HIV have six times greater chance of developing cervical \ncancer, which accounts for 5 % of all cases.\nCervical cancer dataset. Plissiti et al. proposed a SIPaKMeD (Plissiti et al., \n2018) dataset for cervical cancer detection. SIPaKMeD12 is a publicly \navailable dataset containing 4049 cell images that have been annotated \nand classified into five categories. There are two categories for normal \ncells (superficial-intermediate and parabasal cells), two categories for \naberrant but non-malignant cells (koilocytes and dyskeratotic cells), and \none category for benign (metaplastic) cells. The dataset includes two \nkinds of images: whole images and cropped cell core data. The cropped \ndata represent a subset of the whole picture extracted from the cell \nnucleus. The Herlev (Marinakis et al., 2009b; Marinakis et al., 2009a) \ndataset comprises around 917 cervical cell images obtained at the Uni\u00ad\nversity Hospital of Herlev using a digital camera and microscope. The \nHerlev dataset consists of seven distinct types of cervical cells: superfi\u00ad\ncial squamous, severe dysplasia, columnar epithelium, moderate \ndysplasia, carcinoma, mild dysplasia, and intermediate squamous, with \n150, 146, 192, and 182 images corresponding to cancer, moderate \ndysplasia, severe dysplasia, and mild dysplasia, respectively. Mobi\u00ad\nleODT13 is a Kaggle competition dataset for cervical cancer screening. \nTable 6 \nDigestive System Cancer (2020\u20132024).\nYear & \nRef.\nDataset\nMethodology\nResult\nEsophageal\n2023 \nHosseini \net al. \n(2023)\nNA\na systematic review \npaper of esophageal \ncancer detection\nNA\n2020 \nNakano \net al. \n(2020)\nendoscopic \nimages\nNarrowband \nImaging (NBI) and \nFlexible Spectral \nImaging Color \nEnhancement (FICE) \nwas used to enhance \ncapillary patterns in \ncancerous areas and \nLugol \nChromoendoscopy \nfor cancer detection\nacc: \n92.17 %, re: \n96.6 %, \nspecificity: \n91.6 %\nXue et al. \n(2020)\nendoscopic \nimages\nimplemented \nImproved Empirical \nWavelet Transform \n(IEWT) for feature \nextraction to lower- \nfrequency \ncomponent, and \nDeep Learning- \nbased Complex \nEmpirical Wavelet \nTransform (DL- \nCEWT) for high- \nfrequency \ncomponents\nacc: more \nthan 95 %\nLiver\n2023 \nNapte \net al. \n(2023)\nLiTS\nimplemented \ndouble-stage \nGaussian filtering \nfor image \nenhancement, Edge \nStrengthening \nParallel UNet (ESP- \nUNet) to bypass the \nu-seg and o-seg for \nliver segmentation \nand DCNN for \nclassification\nacc: \n98.60 %\n2022 \nOthman \net al. \n(2022)\nLiTS17, 3D- \nIRCADb\u221201\nproposed two hybrid \nmodel: DeeplapV3 +\nResNet\u221250, and \nVGG\u221216 +\nResNet\u221250V2 + U- \nNet + LSTM\nacc: 99.5 %, \npr: 86.4 %, \nre: 97.9 %\n2020 \nDong et al. \n(2020)\ncustom \ndataset\nproposed \nHybridized Fully \nConvolutional \nNeural Network for \nliver tumour \ndetection and \nsegmentation\nacc: 99 %\nPancreatic\n2023 Naga \nRamesh \net al. \n(2023)\nCT images\nHarris Hawks \nOptimization (HHO) \nwith debseNet121 \nwas used for feature \nextraction, and \nSparrow Search \nAlgorithm (SSA) was \nused with CNN- \nBiLSTM for \nadjusting \nhyperparameter\nacc: \n99.26 %, re: \n99.26 %, \nspecificity: \n99.26 %\n2021 Li \net al. \n(2021b)\nRaman \nspectroscopic\nimplemented CNN \nfor pancreatic \ncancer detection\nacc: over \n95 %\nColon\n2024 \nAlqahtani \net al. \n(2024)\nLC25000\nmedian filtering \nused for noise \nremoval, Improved \nWater Strider \nAlgorithm (IWSA) \nacc: \n99.41 %, pr: \n98.52 %, re: \n98.51 %, f1: \n98.51 %\nTable 6 (continued)\nYear & \nRef. \nDataset \nMethodology \nResult\nfor optimizing \nhyperparameters of \nMobileNetV2 to \nextract the features, \nand convolutional \nautoencoder for \nclassification\n2023 Haq \net al. \n(2023)\nHT\u221229\napplied ResNet\u221250 \nto detect and count \nof colon cancer\nacc: 95.3 %\n2022 Sakr \net al. \n(2022)\nLC25000\nproposed \nConvolutional \nNeural Network \n(CNN) for colon \ncancer detection\nacc: \n99.50 %\nTalukder \net al. \n(2022)\nLC25000\nimplemented \nVGG16, VGG19, \nDenseNet169, \nDenseNet201 for \nfeature extraction, \nand RF, SVM, LR, \nMLP, XGB, and LGB \nto evaluate the \nmodel accuracy\nacc: \n99.05 % \n(lung), \n100 % \n(colon), \n99.30 % \n(both)\n11 https:\/\/www.kaggle.com\/datasets\/aryashah2k\/breast-ultrasound-images \n-dataset\n12 https:\/\/www.kaggle.com\/datasets\/prahladmehandiratta\/cervical-cance \nr-largest-dataset-sipakmed\n13 https:\/\/www.kaggle.com\/c\/intel-mobileodt-cervical-cancer-screening \n\/overview\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n13 \n\nTable 7 \nRespiratory System Cancer (2020\u20132024).\nYear & Ref.\nDataset\nMethodology\nResult\nLung\n2024 Nair and \nJerome (2024)\nPrivate \ndataset\nCombined PET and \nMRI images at the \npixel level, applied \nAdaptive Tee Seed \nOptimization for \nselecting optimal \nfusion parameters \nand Deep Extreme \nLearning Machine \nas a classifier\nacc: 97.23 %\nNoaman et al. \n(2024)\nLC25000, \nBreakHis\nDenseNet201 for \nfeature extraction, \nand KNN, SVM, \nLGBM, catboost, \nXGBoost, DT, RF, \nMultinomialNB for \nclassification.\nacc: 99.68 % \n(LC), acc: \n94.808 % (Br.)\n2023 Malik \net al. (2023)\nLC chest X- \nray\nProposed CDC Net \n(a CNN \nincorporating \nresidual network \nconcepts and \ndilated \nconvolution) for \nCOVID\u221219, lung \ncancer, \npneumothorax, \ntuberculosis, and \npneumonia \nclassification\nacc: 99.39 %, \nre: 98.13 %, \npr: 99.42 %, \nAUC: 0.9953\nObayya et al. \n(2023)\nBiomedical \nimages\nGabor Filtering \napplied image \npreprocessing, \nGhostNet for \nfeature extraction, \nAdaptive Fuzzy \nArtificial Optimizer \nto adjust GhostNet \nhyperparameters, \nand Tuna Swarm \nAlgorithm \ncombined with \nEcho State Network \nfor lung and colon \ncancer detection\nacc: 99.33 %\nChikkalingaiah \net al. (2023)\nLIDC\nmedian filter for \nnoise removal, \nGLCM for feature \nextraction, and \nSVM, RF, KNN, and \nDT are used for \nclassification\nacc: 99.32 % \n(SVM)\n2022 Jain et al. \n(2022)\nLC25000, \nNLST, NCI\nKernel Principal \nComponent \nAnalysis integrated \nwith CNN for \nfeature extraction, \nand Fast Deep Belief \nNeural Network for \nclassification\nacc: 97.1 % \n(LZ2500), \n98 %(NLST), \n97.5 % (NCI)\nLiberini et al. \n(2022)\nregenerate \nPET\/CT\nimage \nreconstructed using \nBlock Sequential \nRegularized \nExpectation \nMaximization with \ndifferent \u03b2-values \nand Ordered Subset \nExpectation \nMaximization\nhighest \ndetectability \nfor BSREM200 \nand \nBSREM300\nBhattacharjee \net al. (2022)\nLIDC-IDRI\noptimized random \nforest with K-means \nvisualization\nacc: 92.14 %\nTable 7 (continued)\nYear & Ref. \nDataset \nMethodology \nResult\n2021 Alzubaidi \net al. (2021)\n1000 CT\nten different feature \ntypes are used, \nincluding Gabor \nfilter, Histogram of \nOriented Gradients \n(HOG), and hear \nwavelet and six ML \nalgorithms\nacc: 97 %, re: \n96 %, and \nspecificity: \n97 %\nLu et al. (2021)\nRIDER\nMPA optimization \nalgorithm with \nCNN to enhance the \naccuracy and \ncompared with \nResNet\u221218, \nGoogLeNet, \nAlexNet, and \nVGG\u221219\nacc: 93.4 %, \nre: 98.4 %, \nspecificity: \n97.1 %\n2020 \nRajagopalan \nand Babu \n(2020)\nJSRT\nImplemented a \nMassive ANN-based \nsoft tissue \ntechnique and \nRecognizes nodule \ncandidates from X- \nray images for \nfeature extraction \nand classification\nacc: 72.96 % \n(with soft \ntissue), \n66.76 % \n(without)\nChenyang and \nChan (2020)\nLUNA16, \nLIDC-IDRI\nproposed 3- \nd encoder-decoder \narchitecture for \nautomatic cancer \nand nodules \ndetection\nacc: 90.29 %, \nre: 88.79 %, \nspecificity: \n91.78 %\nMasood et al. \n(2020b)\nLIDC\nMultidimensional \nRegion-based Fully \nConvolutional \nNetwork (MRFCN) \nfor detection and \nclassification\nacc: 97.91 %, \nre: 98.1 %\nMasood et al. \n(2020a)\nLUNA16, \nANODE09, \nLIDC-IDRI\nproposed 3d deep \nconvolutional \nneural network and \napplied median \nintensity projection \nand multi-region \nproposal network \n(mrpn) for \nautomatic selection \nof potential 3D \nregion-of-interests\nacc: 98.51 %, \nre: 98.4 %, \nspecificity: \n92 %, auroc: \n92 %\nOral\n2024 Deo et al. \n(2024)\nH&E dataset\nensemble of DL \nmodels (ResNet50 \nand DenseNet201)\nacc: 92 %\n2023 Myriam \net al. (2023)\nLT dataset\nCNN and DBN with \nparticle swarm and \nal-biruni earth \nradius optimization\nacc: 97.35 %\n2022 Marzouk \net al. (2022)\nLT dataset\nFuzzy-based \ncontrast \nenhancement, \nDenseNet\u2212169 for \nfeature extraction \nand COA \noptimization \nalgorithms +\nAutoencoder for \nclassification\nacc: 90.08 %\nShamim et al. \n(2022)\nprivate \ndataset\npre-trained models, \nincluding AlexNet, \nGoogLeNet, Vgg19, \nResNet50, \nInceptionv3 and \nSqueezeNet are \nused to detect \nbinary and five \ntypes of tongue \nlesions\nbinary acc: \n97.5 % \n(vgg19), \nmulti-class \nacc: 96.7 % \n(resnet50)\n(continued on next page)\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n14 \n\nThe dataset has three cervix types, including type 1 (completely ecto\u00ad\ncervical and fully visible), type 2 (has endocervical component and fully \nvisible), and type 3 (has endocervical component and not fully visible). \nMendeley Liquid-based cytology (LBC) (Hussain, 2019) comprises pap \nsmear images from 460 patients captured in 40x magnification.\n4.1.3. Ovarian cancer\nOvarian cancer (Ovarian, fallopian tube, and primary peritoneal \ncancer, 2024) is a malignant tumor that originates in the ovaries, which \nare an integral element of the female reproductive system. It mostly \naffects aging women between 50 and 70. It is more frequent in persons \nwith a family history of ovarian or breast cancer, genetic abnormalities \nsuch as BRCA1 or BRCA2, or who have never been pregnant. Common \nsymptoms of ovarian cancer often include chronic bloating, abdominal \npain, impaired appetite or early satiety, and frequent or urgent urine. \nThese symptoms are often inconspicuous and might be misinterpreted as \nless severe illnesses, resulting in a delayed diagnosis. Timely identifi\u00ad\ncation is essential since ovarian cancer is often detected in a late stage, \nwhen it has already metastasized, making treatment more challenging. \nTimely detection greatly enhances the chances of survival and treatment \nresults, underscoring the need for routine screens and knowledge of \nsymptoms.\nOvarian cancer dataset. UBC-OCEAN (Paayas and Annamalai, 2023)14\ncomprises five distinct subtypes of ovarian cancer: high-grade serous \ncarcinoma, low-grade serous, endometrioid, clear-cell ovarian carci\u00ad\nnoma, and mucinous carcinoma. There are two distinct classifications of \nimages: whole slide images (WSI) and tissue microarray (TMA). The WSI \nis magnified by a factor of 20 and might be of considerable size. The \nTMAs have a lower size, around 4,000x4000 pixels, but they are \nmagnified at a factor of 40x.\n4.1.4. Prostate cancer\nThe prostate is a male reproductive system that comprises three \ndistinct regions: the transition zone, central zone, and peripheral zone, \nwith the latter two forming around 95 % of its total volume. The ma\u00ad\njority of prostate cancer cases, about 70 %, are discovered in the pe\u00ad\nripheral zone, while just 2.5 % of cases happen within the transition \nzone. Although the transition zone is the smallest area, it has a 20 % \nlikelihood of getting prostate cancer (McNeal et al., 1988). Timely \ndetection greatly enhances the chances of survival and treatment results, \nunderscoring the need for routine screens and knowledge of symptoms.\nProstate cancer dataset. ProstateX (Litjens et al., 2014a) is a collabora\u00ad\ntive medical image detection competition sponsored by the American \nAssociation of Physicists in Medicine (AAPM), the International Society \nfor Optics and Photonics (SPIE), and the National Cancer Institute (NCI). \nThe ProstateX collection comprises MR images with several sequences, \nincluding T2-weighted (T2 W) images, proton density-weighted (PD-W) \nimages, dynamic enhancement (DCE) images, and diffusion-weighted \nTable 7 (continued)\nYear & Ref. \nDataset \nMethodology \nResult\n2021 Yan et al. \n(2021)\n62 \nhealthy\u221262 \npatients\nvariations of \nEmission Filters and \n264 classifiers \nformed from the \ncombination of four \nquantization \nmethods, two \nexcitation sources, \nand 30 spectral \nbands\nacc: 82.85 %, \nrecall: \n96.15 %\n2020 Welikala \net al. (2020)\nMEMOSA\nResNet101 for \nimage classification \nand Faster R-CNN \nfor object detection\nf1: 87.07 % & \n41.18 % for \nclassification \n& detection\nTable 8 \nOther System (2020\u20132024).\nYear & Ref.\nDataset\nMethodology\nResult\nSkin\n2024 Midasala et al. (2024)\nISIC 2020\nBilateral filter to remove noise; multilevel (low-level, texture, and colour) feature \nextraction using GLCM, RDWT and K-means clustering (KMC) for segmenting.\nacc: 99.18 %, fpr: 99.25 %, \npf: 99.04 %, MCC: 99.84 %\nNaeem et al. (2024)\nISIC 2019\nDL models and Handcrafted methods for feature extraction, and CNN for \nclassification\nacc: 97.81 %, pr: 98.31 %, re: \n97.89 %, f1: 98.10 %\nAlqarafi et al. (2024)\nMed-node, ISIC, \nDermIs\nMEWF for noise reduction, M-GCN for feature extraction, Tri-level feature fusion \nmodule and sigmoid function for classification\nacc: 98.78 %, re: 97.99, f1: \n98.84 %\n2023 Kavitha et al. (2023)\nISIC 2018\nauto correlogram methods, color layout filter, binary pyramid pattern filter, \nensemble models used for detection\nacc: 90.96 %, pr: 91 %, re: \n91 %, f1: 0.91 %\n2022 Fraiwan and Faouri (2022)\nHAM10000\nthirteen transfer learning model used for classification\nacc: 82.9 %\nMohakud and Dash (2022)\nISIC\nGWO algorithm used to optimize CNN hyper-parameters and compared with PSO \nand GA\nacc: 98.33 %\nImran et al. (2022)\nISIC\nensemble learning technique using VGG, capsnet, resnet\nacc: 93.5 %, re: 87 %, \nspecificity: 94 %\n2021 Wang and Hamian (2021)\nSIIM-ISIC\nThermal Exchange Optimization (DTEO) algorithm-based deep belief network \n(DBN) for classification\nacc: 92.65 %, re: 91.18 %, \nspecificity: 89.70 %\nHuaping et al. (2021)\nACS, PH2\nfeature extracted from Kernel fuzzy c-means based ROI segmentation and SVM is \nused for classification\nacc: 64.19 %, 70 % (PH2)\n2020 Wei et al. (2020)\nISIC 2016\nproposed a lightweight semantic segmentation model using U-Net architecture \nwith MobileNetV1 and DenseNet121 for high-precision lesion area segmentation\nacc: 96.2 %, ji: 86.7 %, dc: \n92.3 %\nAshraf et al. (2020)\nDermIS and \nDermQuest\nImproved k-means algorithm to extract ROIs and enhance the training set by \naugmenting ROI images. CNN-based transfer learning was used for classification.\nacc(dermis): 97.9 %, acc \n(dermquest): 97.4 %\nThanh et al. (2020)\nISIC\nAdaptive principal curvature for image preprocessing, color normalization for \nskin lesion segmentation, and ABCD rule for feature extraction\nacc: 96.6 %, dice: 93.9 %, \njaccard: 88.7 %\nBrain\n2024 Kumar et al. (2024)\nMRI scans \n(private)\nmean grey-level approach for image segmentation and Hara-lick\u2019s feature \nequations and spatial gray-level dependency matrix (SGLD) for statistical feature \nanalysis to detect brain tumour\nacc: 97 %\n2023 Mercaldo et al. (2023)\nBr35H\nimplemented VGG16, ResNet50, AlexNet, and MobileNet and class activation \nmapping (CAM) to detect and highlight associated areas\nacc: (97.83 % - 99.67 %)\n2020 Florimbi et al. (2020)\nHSI data\nLeverages GPU technology to handle the large volume of data and complex \ncalculations required for HSI\nclassify large image dataset \nunder 3 s\n14 https:\/\/www.kaggle.com\/competitions\/UBC-OCEAN\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n15 \n\n(DW) images. The label file given by ProstateX contains the precise \ncoordinates of the lesion and information on whether it is benign or \ncancerous. In ProstateX, two sets of patients are included: 204 with 330 \nlesions in the training set and 142 with 208 lesions in the testing set. The \nProstateX-2 (Abraham and Nair, 2018) challenge train set has 99 pa\u00ad\ntients and 112 lesions, and the test set has 63 patients and 70 lesions. \nThe International Medical Image Processing Committee conducted the \nPROMISE12 (Litjens et al., 2014b) challenge in 2012, focusing on \nprostate segmentation. The dataset comprises 50 training samples with a \nT2-weighted sequence of the prostate MR image and its related prostate \nmask, as well as 30 testing samples. Panda (Prostate Cancer Grade \nAssessment) (Bulten et al., 2022; Tolkach et al., 2023)15 challenge \ncomprises a total of 10,616 WSIs, which were obtained from Radboud \nUniversity Medical Center and Karolinska Institute, totalling 5160 and \n5456 WSIs images, respectively. Duran-Lopez et al. (2021) used He\u00ad\nmatoxylin and Eosin (H&E)-stained slides (158 normal WSIs and 174 \nmalignant WSIs) for prostate cancer detection.\n4.1.5. Summary of reproductive system cancer\nTable 4 summarises the list of research articles related to breast \ncancer, and Table 5 lists the summary of other reproductive system \ncancer detection studies.\n4.2. Digestive system\n4.2.1. Esophageal cancer\nEsophageal cancer is a malignant tumour originating in the esoph\u00ad\nagus, the anatomical structure connecting the neck to the stomach. \nPrimarily afflicting the elderly, especially males, this condition is often \nlinked to risk factors such as smoking, excessive alcohol intake, and \npersistent acid reflux. Common symptoms include dysphagia, angina, \nunintended weight loss, and a persistent cough. It is categorized into \nesophageal adenocarcinoma and esophageal squamous cell carcinoma \n(Esophageal cancer, 2024). It is projected that in the coming years, the \nrate of this disease, which is now the seventh most frequent cancer \nworldwide and the sixth largest cause of cancer-related mortality, will \nrise by 140 % (Stabellini et al., 2022).\nEsophageal cancer dataset. Xue et al. (2020) presented a robust method \nfor detecting esophageal cancer using an improved empirical wavelet \ntransform (IEWT) on images obtained from endoscopic procedures. The \nprocess involved converting endoscopic images into the L*x*y* colour \nspace and creating a fusion image using the x* and y* components. This \nfusion image was then analyzed using a mix of wavelet transforms, \nincluding a lower-frequency element from IEWT and high-frequency \nelements from a Deep Learning-based Complex Empirical Wavelet \nTransformation (DL-CEWT). The technique rigorously determined \nfractal sizes through box interpolation to identify abnormal areas based \non their fractal dimensions. This proposed method significantly \nenhanced the precision and effectiveness of cancer detection compared \nto established methods such as SVM, RF, and CNN. Nakano et al. (2020)\nintroduced a minimally intrusive method for differentiating between \nbenign and malignant tissues using low-concentration Lugol staining \nand narrowband light. The suggested endoscopic system, which collects \nRGB images synced with narrowband light, had encouraging outcomes \nin studies conducted with resected material. While more validation and \nsystem modifications remain necessary, this technique could potentially \nimprove the precision and sensitivity of esophageal cancer detection in \nclinical settings.\n4.2.2. Liver cancer\nLiver cancer is a malignancy originating in the liver cells and \ntypically impacts people with chronic liver illnesses like hepatitis or \ncirrhosis. It can happen in adults and children. Common symptoms of \nthis condition include decreased appetite, discomfort in the upper \nabdomen, feelings of nausea, and the manifestation of jaundice (Liver \nand bile duct cancer, 2024). Timely identification is important for liver \ncancer as it significantly improves the efficacy of therapy, elevates \nsurvival rates, and permits less intrusive procedures. The estimated \nnumber of new cases of liver cancer and deaths in 2024 are 41,630 and \n29,840, respectively (Cancer stat facts: Liver and intrahepatic bile duct \ncancer, 2024).\nLiver cancer dataset. The LiTS (Bilic et al., 2023; Chlebus et al., 2018) \nchallenge comprises 131 contrast-enhanced abdomen CT images ob\u00ad\ntained from 7 different clinical institutes. Expert radiologists have con\u00ad\nducted liver and tumour reference annotations to accompany the CT \nimages. The horizontal resolution varies between 0.5 and 1.0 mm, while \nthe vertical thickness spans from 0.7 to 5.0 mm. The dataset comprises \n908 lesions, with 63 % having the longest axial diameter of 10 mm or \nmore. The 3D-IRCADb01 (Soler et al., 2010) dataset comprises 20 3D CT \nimages obtained from 20 distinct patients, with liver tumours present in \n75 % of the cases. Each image has dimensions of 512 \u00d7 512 pixels and \nexhibits varying average liver densities ranging from 40 to 135. The \nimages with labels were partitioned into segments using the DICOM \nformat.\n4.2.3. Pancreatic cancer\nPancreatic cancer is a malignant tumour originating in the pancreas \ntissues. According to the National Cancer Institute (Cancer stat facts: \nPancreatic cancer, 2024), the estimated number of new cases of \npancreatic cancer and deaths in 2024 are 66,440 and 51,750, \nrespectively.\nPancreatic cancer dataset. Naga Ramesh et al. (2023) employed Harris \nHawks Optimization (HHO) with debseNet121 for feature extraction \nand Sparrow Search Algorithm (SSA) with CNN-BiLSTM for adjusting \nhyperparameters. They used a CT image dataset containing 250 \npancreatic tumours and 250 non-pancreatic tumour images. Li et al. \n(2021b) used CNNs to improve the recognition of pancreatic cancer via \nthe use of spontaneous Raman spectroscopy. The work entailed the ex\u00ad\namination of 1D and 2D Raman images of pancreatic tissue in a mouse \nmodel to assess the efficacy of CNNs in detecting malignant areas. The \nauthors emphasized the significance of visualizing crucial features from \nthe CNN models, as it facilitates the precise identification of malignancy. \nThe findings indicate that using CNN-assisted Raman spectroscopy may \ngreatly enhance the detection of pancreatic cancer, implying a hopeful \nmethod for early diagnosis and improved treatment results.\n4.2.4. Colon cancer\nColorectal cancer is a common and deadly disease, especially in \npeople over 50. The most common symptoms are constipation, \nabdominal pain, diarrhea, and sudden weight loss. Early detection, \nthrough screenings and a healthy lifestyle, can significantly reduce the \nrisk and improve outcomes. By 2040 (Colon cancer, 2023), projections \nindicate a sharp rise in the global burden of colorectal cancer, with 3.2 \nmillion new cases and 1.6 million deaths annually, underscoring the \nneed for effective detection and prevention strategies.\nColon cancer dataset. The histopathological images, LC2500016\n(Alqahtani et al., 2024; Sakr et al., 2022; Talukder et al., 2022), \ncomprise two distinct datasets, one for lung cancer and one for colon \ncancer. The dataset includes 25,000 images, of which 10,000 are related \n15 https:\/\/www.kaggle.com\/competitions\/prostate-cancer-grade-assessment\n16 https:\/\/www.kaggle.com\/datasets\/andrewmvd\/lung-and-colon-cancer-h \nistopathological-images\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n16 \n\nto colon cancer and 15,000 to lung cancer. The lung cancer dataset has \nthree distinct categories of cells: adenocarcinomas, squamous cell car\u00ad\ncinomas, and benign tissue. The colon cancer dataset has two distinct \ncategories of cells: adenocarcinomas and benign tissue. The LC25000 \ndataset was created using a portion of HIPAA-compliant information \nand validated references, incorporating 750 lung tissue samples (250 \nadenocarcinomas, 250 squamous cell carcinomas, and 250 benign tis\u00ad\nsue) and 500 colon tissue samples (250 adenocarcinomas and 250 \nbenign tissue). These samples were then augmented to produce a total of \n25,000 images.\nHaq et al. (2023) introduced the HT-29 cells dataset to detect, clas\u00ad\nsify, and quantify colon cancer cells via image processing and deep \nlearning methods. The HT-29 cell lines were obtained from Procell Life \nScience and Technology Co. Ltd. The dataset consisted of 566 images \nclassified into two categories: benign and malignant. The ResNet50 \nmodel was used for classification, resulting in a validation accuracy of \n95.3 %.\n4.2.5. Summary of digestive system cancer\nTable 6 lists the summary of digestive system cancer detection \nstudies.\n4.3. Respiratory system\n4.3.1. Lung cancer\nLung cancer is an uncontrolled cell growth in the lungs that causes \nsignificant death and disability. The major types of carcinoma are non- \nsmall cell carcinoma (NSCLC), defined as a moderate growth rate, and \nsmall cell carcinoma (SCLC), defined as a rapid growth rate. Lung cancer \ncontinues to be the primary cause of cancer-related mortality world\u00ad\nwide, resulting in about 1.8 million deaths in 2020 (GLOBOCAN 2020). \nThe basic symptoms of this cancer include a persistent cough, chest pain, \nand breathing difficulty (Lung cancer, 2023).\nLung cancer dataset. The LC25000 (Noaman et al., 2024) dataset com\u00ad\nprises 25,000 histological images for lung and colon cancer detection. \nMalik et al. (2023) used an LC dataset (Shiraishi et al., 2000) that in\u00ad\ncludes around 20,000 chest X-rays and CT scan images. However, they \nonly used LC chest X-ray images with pneumothorax, COVID-19, and \ntuberculosis images for multi-classification. Alzubaidi et al. (2021) used \n1000 CT images collected from the TCIA (Clark et al., 2013) database, \nincluding 500 normal and 500 abnormal cases.\nThe Lung Image Database Consortium and Image Database Resource \nInitiative (LIDC-IDRI) (Armato et al., 2011) is the most used helical CT \nimages database for lung cancer detection (Bhattacharjee et al., 2022; \nChenyang and Chan, 2020; Masood et al., 2020a; Masood et al., 2020b; \nChikkalingaiah et al., 2023). This database, stored in DICOM format, has \na total of 125 GB and includes 1018 CT scans and 7371 nodules, with \nannotations by four radiologists available in Extensible Markup Lan\u00ad\nguage (XML) files. Nodules fall into three categories: (a) nodules that \nmeasure between 3 mm and 30 mm are called nodules; (b) nodules \nsmaller than 3 mm are called non-benign nodules; and (c) non-nodules \nthat are 3 mm or larger and do not exhibit nodule characteristics.\nReference Image Database to Evaluate Therapy Response (RIDER17) \n(Lu et al., 2021) dataset developed by the Cancer Research and Pre\u00ad\nvention Foundation (CRPF) and NCI\u2019s centre with the support of RSNA \nin 2004. This dataset comprises CT images obtained from several lung \nCT scans (\u201ccoffee break experiment\u201d) of 32 participants diagnosed with \nlung cancer. Furthermore, it also includes a database of identified le\u00ad\nsions, each of which underwent 2 CT tests using the same machine and \nimaging technique.\nJapanese Society of Radiological Technology (JSRT18) (Rajagopalan \nand Babu, 2020) contains 233 images including 93 normal and 142 \nnodule x-ray images. LUNA16 (Setio et al., 2017; Chenyang and Chan, \n2020; Masood et al., 2020a) is a subset of the LIDC-IDRI dataset. \nANODE09 (Van Ginneken et al., 2010; Masood et al., 2020a) comprises \n55 CT scans, including five annotations accomplished by three pro\u00ad\nfessionals comprising 31 normal and 39 nodules. (Jain et al., 2022) \nemployed three datasets, including LC2500, NLST, and NCI, for lung \ncancer detection. The NLST (National Lung Screening Trial) dataset in\u00ad\ncludes 215 tiles of size 512 \u00d7 512 chosen from original high-resolution \nhistopathological images, with an experienced pathologist manually \nannotating 83,245 nuclei objects. The NCI Genomic dataset comprises \nfreely available lung cancer images utilised to study the automatic \nclassification of immediate tumours and solid tissue normal using 459 \nand 1175 eosin-stained histopathology images. Additionally, a set of 567 \nand 608 WSI is utilized to classify primary tumours.\n4.3.2. Oral cancer\nThe incidence rates of oral cancer have slightly increased for both \ngenders between the mid-2000s and 2015\u20132019. The age-adjusted \noccurrence rate for Black males decreased from 31.5 per 100,000 in \n1987\u201312.8, while for Black women it decreased from 12.8 to 4.9. In \ncontrast, the frequency of occurrences among individuals of White \nethnicity increased from 10.6 in 2004\u201318.4 in the period of 2015\u20132019 \n(Oral cancer incidence (new cases) by age, race, and gender, 2024).\nOral cancer dataset. Rahman et al. (2020) proposed an archive that \nencompasses a collection of 1224 histological images detailing the oral \ncavity, organized into two distinct categories based on resolution. The \ninitial category comprises 89 images depicting normal epithelium \nalongside 439 images of Oral Squamous Cell Carcinoma (OSCC), each \nmagnified at 100x. Meanwhile, the subsequent category includes 201 \nimages of normal epithelium and 495 images of OSCC, with these \nspecimens magnified at 400x. These images were captured utilizing a \nLeica ICC50 HD microscope and applied to H&E stained tissue slides \n(Deo et al., 2024), which were prepared by healthcare experts from a \nsample pool of 230 patients. Myriam et al. (2023) used Kaggle Lips and \nTongue (LT)19 dataset, comprises 87 malignant images and 44 benign \nimages, resulting in a total of 131 images. They got 97.35 % accuracy for \ncancer detection. Marzouk et al. (2022) also used the LT dataset and \nachieved 90.35 % accuracy.\nCancer Research Malaysia originated a mobile application named \nMobile Mouth Screening Anywhere (MEMOSA) (Haron et al., 2020) to \ndevelop annotated oral session images. Welikala et al. (2020) collected \n2155 oral cavity images from 1085 people with and without lessons. The \nhighest and lowest pixels were 5472x3648, and 119x142 pixels, \nrespectively. They used MEMOSA to annotate these collected images.\n4.3.3. Summary of REspiratory System Cancer\nTable 7 lists the summary of Respiratory system cancer detection \nstudies.\n4.4. Other system\n4.4.1. Brain cancer\nAccording to the National Cancer Institute (Cancer stat facts: Brain \nand other nervous system cancer, 2024), the estimated number of new \ncases of brain and other nervous system cancer is 25,400, and the esti\u00ad\nmated number of deaths is 18,760 in 2024. The five years (2014\u20132020) \nrelative survival rate of brain cancer is 33.4 %. Approximately 1 million \nAmericans have a primary brain tumour, with 94,390 new cases ex\u00ad\npected in 2023, with 59 % occurring in females and 41 % in males (Brain \ntumor facts, 2024).\n17 https:\/\/wiki.cancerimagingarchive.net\/display\/Public\/RIDER+Lung+CT\n18 http:\/\/db.jsrt.or.jp\/eng.php\n19 https:\/\/www.kaggle.com\/code\/shivam17299\/oral-cancer-lips-and-tongue- \nimages-dataset\/data\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n17 \n\nBrain cancer dataset. Br35H20(Mercaldo et al., 2023) contains a total of \n3000 images including 1500 tumorous MRI images and 1500 \nnon-tumorous images. Other studies employed private datasets (MRI \nscans Kumar et al., 2024, and HSI Florimbi et al., 2020) for brain tumour \ndetection.\n4.4.2. Skin cancer\nAccording to the National Cancer Institute (Cancer stat facts: Mela\u00ad\nnoma of the skin, 2024), the estimated number of new cases of skin \ncancer is 100,640, and the estimated number of deaths is 8290 in 2024. \nThe five-year (2014\u20132020) relative survival rate of skin cancer is \n94.1 %.\nSkin cancer dataset. The International Skin Imaging Collaboration \n(ISIC)21 (Mohakud and Dash, 2022; Imran et al., 2022; Thanh et al., \n2020) challenges have emerged as a catalyst for study in the domain of \nmelanoma categorization (Cassidy et al., 2022). The provider offers \ndigitized, high-quality skin lesion images with professional annotations \nand metadata. From 2016\u20132020, the ISIC datasets progressively \nexpanded, with ISIC 2016 (Gutman et al., 2016; Wei et al., 2020) con\u00ad\ntaining 1279 images (900 train, 379 test, with two classes: benign and \nmalignant), ISIC 2017 (Codella et al., 2018) having 2600 images (2000 \ntrain, 600 test, with three classes: melanoma, seborrheic keratosis, and \nothers), ISIC 2018 (Codella et al., 2019; Kavitha et al., 2023) including \n11,527 images (10,015 train, 1512 test, with seven classes: melanoma, \nmelanocytic nevus, basal cell carcinoma, actinic keratosis, benign \nkeratosis, dermatofibroma, vascular lesion), ISIC 2019 (Combalia et al., \n2019; Naeem et al., 2024) comprising 33,569 images (25,331 train, \n8238 test, with eight classes: ISIC 2018 classes and squamous cell car\u00ad\ncinoma), and ISIC 2020 (Rotemberg et al., 2021; Midasala et al., 2024) \nencompassing 44,108 images (33,126 train, 10,982 test, with nine \nclasses). Additionally, SIIM-ISIC22 is another version of ISIC datasets.\nHAM1000023(Fraiwan and Faouri, 2022) comprises 11,526 images \nincluding 10,015 training images, and 1511 test images from ISIC \ndataset. Dermis and DermQuest24 (Alqarafi et al., 2024; Ashraf et al., \n2020) comprises 206 dermoscopic images. American Cancer Society \n(ACS)25 (Skin cancer, 2024; Huaping et al., 2021) database comprises 68 \npairs of XLM and TLM images. The PH226 (Mendon\u00e7a et al., 2013; \nHuaping et al., 2021) contains 200 dermoscopic images (40 melanomas, \n80 nevi, and 80 atypical nevi). MED-NODE27 comprises 170 images (100 \nnevi and 70 melanoma cases).\n4.4.3. Summary of other system cancer\nTable 8 lists the summary of brain tumour and skin cancer detection \nstudies.\n5. Research summary and discussion\nThis section discusses the research summary and responds to the \nstated research questions (RQ). The responses to the research questions \nare as follows:\nRQ 1. a: The challenges associated with medical data are imbalanced \ndatasets, labelling errors, and limited data availability 5.1.1.\nRQ 1. b: The preprocessing techniques include several filtering ap\u00ad\nproaches (Gaussian filter, median filter, bilateral filter, etc.), \naugmentation, noise removal, etc. The details are discussed in Sec\u00ad\ntion 3.2.\nRQ:1. c: The benchmark datasets for cancer detection are discussed \nin Section 4. The breast cancer detection dataset includes DDSM \n(Shah et al., 2024; Khan et al., 2024; Jafari and Karami, 2023; \nBoudouh and Bouakkaz, 2023) CBIS-DDSM (Teoh et al., 2024; \nFuentes-Fino et al., 2023), INBreast (Anas et al., 2024; Teoh et al., \n2024; Fuentes-Fino et al., 2023; Asadi and Memon, 2023), MIAS \n(Jafari and Karami, 2023; Alruwaili and Gouda, 2022; Saber et al., \n2021), mini-MIAS (Teoh et al., 2024; Boudouh and Bouakkaz, 2023), \nIDC (Sharmin et al., 2023), BreaKHis (Zeng et al., 2024; Kashyap, \n2023; Toma et al., 2023; Li et al., 2021a), CMMD (Boudouh and \nBouakkaz, 2023), WBCD (Strelcenia and Prakoonwit, 2023; Asadi \nand Memon, 2023), BCDR (Asadi and Memon, 2023; Azevedo et al., \n2022), and DMR (Al Husaini et al., 2022; Dey et al., 2022; \nS\u00b4anchez-Cauce et al., 2021). Cervical cancer detection datasets are \nSipakMed (Fahad et al., 2024; Sahoo et al., 2023; Bhatt et al., 2021), \nand Herlev (Fahad et al., 2024; Mazroa et al., 2023; Devi et al., 2022; \nBhatt et al., 2021). Ovarian cancer detection datasets are \nUBC-OCEAN (Paayas and Annamalai, 2023) and Pelvic-CT \n(Mukhedkar et al., 2023). Prostate cancer detection datasets are \npanda (Nishio et al., 2023), ProstateX (Qian et al., 2021), ProstateX-2 \n(de Vente et al., 2021), and PROMISE12 (Qian et al., 2021). The liver \ncancer detection datasets are LiTS (Napte et al., 2023; Othman et al., \n2022) and 3D-IRCADb-01 (Othman et al., 2022). The Colon cancer \ndetection datasets are LC25000 (Sakr et al., 2022; Talukder et al., \n2022) and HT-29 (Haq et al., 2023). Lung cancer detection datasets \nare LC25000 (Noaman et al., 2024; Jain et al., 2022), LIDC-IDRI \n(Chikkalingaiah et al., 2023; Bhattacharjee et al., 2022; Chenyang \nand Chan, 2020; Masood et al., 2020b; Masood et al., 2020a), \nLUNA16 (Chenyang and Chan, 2020; Masood et al., 2020a), JSRT \n(Rajagopalan and Babu, 2020), and RIDER (Lu et al., 2021). The oral \ncancer detection datasets are the LT dataset (Myriam et al., 2023; \nMarzouk et al., 2022), the H&E dataset (Deo et al., 2024), and \nMEMOSA (Welikala et al., 2020). The skin cancer datasets are IISC \n(Midasala et al., 2024; Naeem et al., 2024; Alqarafi et al., 2024; \nKavitha et al., 2023; Mohakud and Dash, 2022; Imran et al., 2022; \nWei et al., 2020; Thanh et al., 2020), HAM10000 (Fraiwan and \nFaouri, 2022), ACS, PH2, DermIS and DermQuest (Alqarafi et al., \n2024; Ashraf et al., 2020), and Med-Node (Alqarafi et al., 2024). \nBrain cancer detection datasets are Br35H (Mercaldo et al., 2023) \nand HSI (Florimbi et al., 2020).\nRQ 2. a: The most used deep learning algorithms are CNN (Wang \net al., 2024; Mukhedkar et al., 2023, Schwartz et al., 2022; \nDuran-Lopez et al., 2021; Duran-Lopez et al., 2020; Naga Ramesh \net al., 2023; Sakr et al., 2022; Malik et al., 2023; Myriam et al., 2023; \nMohakud and Dash, 2022), LSTM (Mazroa et al., 2023), BiLSTM \n(Mukhedkar et al., 2023; Naga Ramesh et al., 2023; Jain et al., 2022), \nRCNN (Welikala et al., 2020), GAN (Shah et al., 2024; Strelcenia and \nPrakoonwit, 2023; Elakkiya et al., 2022), and vision transformer \n(Sriwastawa and Arul Jothi, 2024). The details are discussed in \nSection 3.5.\nRQ 2. b: The most used feature extraction algorithms are GLCM \n(Midasala et al., 2024), HOG (Alzubaidi et al., 2021), RDWT \n(Midasala et al., 2024) and wavelet transform (Xue et al., 2020; \nAlzubaidi et al., 2021). The details are discussed in Section 3.4.\nRQ 2. c: Leveraging pre-trained models through transfer learning can \nsubstantially bolster the performance of deep learning models in \ndetecting cancer, particularly when working with limited datasets, \nleading to more accurate and reliable results. The most used pre- \ntrained models are VGG16 (Teoh et al., 2024; Boudouh and Bouak\u00ad\nkaz, 2023; Gon\u00e7alves et al., 2022; Saber et al., 2021; Sahoo et al., \n2023; Talukder et al., 2022; Mercaldo et al., 2023), VGG19 (Boudouh \nand Bouakkaz, 2023; Saber et al., 2021; Talukder et al., 2022; Lu \n20 https:\/\/www.kaggle.com\/datasets\/ahmedhamada0\/brain-tumor-detection\n21 https:\/\/challenge.isic-archive.com\/data\n22 https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/overview\n23 https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/D \nVN\/DBW86T\n24 https:\/\/www.dermis.net\/dermisroot\/en\/home\/index.htm\n25 https:\/\/www.cancer.org\/cancer\/skin-cancer.html\n26 https:\/\/www.fc.up.pt\/addi\/ph2%20database.html\n27 https:\/\/www.cs.rug.nl\/~imaging\/databases\/melanoma_naevi\/\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n18 \n\net al., 2021; Shamim et al., 2022), ResNet50 (Teoh et al., 2024; Zeng \net al., 2024; Sharmin et al., 2023; Boudouh and Bouakkaz, 2023; \nAsadi and Memon, 2023; Gon\u00e7alves et al., 2022; Alruwaili and \nGouda, 2022; Saber et al., 2021; Haq et al., 2023; Deo et al., 2024; \nShamim et al., 2022; Mercaldo et al., 2023), ResNet152, Dense\u00ad\nNet121 (Fuentes-Fino et al., 2023; Dey et al., 2022; Paayas and \nAnnamalai, 2023; Wei et al., 2020), DenseNet169 (Toma et al., 2023; \nTalukder et al., 2022; Marzouk et al., 2022), DenseNet201 \n(Gon\u00e7alves et al., 2022; Talukder et al., 2022; Noaman et al., 2024; \nDeo et al., 2024), InceptionV3 (Boudouh and Bouakkaz, 2023; Al \nHusaini et al., 2022; Saber et al., 2021; Shamim et al., 2022), \nXception (Boudouh and Bouakkaz, 2023), ResNet101 (Boudouh and \nBouakkaz, 2023), GoogleNet (Admass et al., 2024; Teoh et al., 2024; \nSahoo et al., 2023; Lu et al., 2021; Shamim et al., 2022), AlexNet \n(Teoh et al., 2024; Boudouh and Bouakkaz, 2023; Fuentes-Fino et al., \n2023; Lu et al., 2021; Shamim et al., 2022; Mercaldo et al., 2023), \nMobileNetV2 (Fuentes-Fino et al., 2023; Alqahtani et al., 2024; Wei \net al., 2020; Mercaldo et al., 2023), YOLO (Anas et al., 2024), etc. \nThe details are discussed in Section 3.6.\nRQ 3. a: The details of the research challenges and limitations are \ndiscussed in Section 5.1.\nRQ 3. b: The details of the future directions are discussed in Section \n5.2.\n5.1. Challenges and limitations\n5.1.1. Data quality and quantity\nImbalanced Datasets: Medical imaging datasets commonly display \nan imbalanced distribution of classes, with a higher prevalence of non- \ncancerous images compared to cancerous ones. This disproportion can \nresult in biased models that inadequately address minority classes. In the \ncontext of lung cancer detection using CT scans, non-cancerous images \nfrequently outnumber cancerous ones to a significant degree. This \nimbalance significantly impacts model training, leading to a notably \nhigh FNR. Labeling Errors: Accurate annotation of medical images is \nabsolutely essential, but it presents a significant challenge due to inter- \nobserver variability and the requirement for expert radiologists. Accu\u00ad\nrate annotation of microcalcifications and masses in breast cancer \ndetection using mammograms demands the expertise of experienced \nradiologists. Any discrepancies between annotators can significantly \nimpact model performance. Limited Availability: Access to substantial \nand diverse datasets is limited by privacy concerns and the proprietary \nnature of medical data. For instance, obtaining high-quality MRI scans \nfor brain tumour detection is hindered by the high cost and privacy \nconcerns, posing a significant challenge in compiling large datasets for \ntraining deep learning models.\n5.1.2. Model generalization\nOverfitting: Deep learning models, especially CNNs, are prone to \noverfitting, especially when they are trained on small or homogeneous \ndatasets. For example, a deep learning model trained on a limited \ndataset of skin lesion images may achieve good performance on the \ntraining data but struggle to generalize to new images, leading to poor \nperformance on unseen cases. Domain Adaptation: It\u2019s imperative to \nrecognize that models trained on data from a particular institution or \ndemographic are unlikely to perform optimally on data from another, \nwhich underscores the critical need for robust domain adaptation \ntechniques. For example, a model trained to detect liver cancer using CT \nscans from one hospital will likely falter when dealing with CT scans \nfrom another hospital due to discrepancies in imaging protocols and \npatient demographics.\n5.1.3. Complexities in segmentation\nVariability in Tumor Appearance: Tumors can vary significantly in \nsize, shape, and texture, making accurate segmentation challenging. \nExample: In prostate cancer detection using MRI, tumours can vary \ngreatly in size, shape, and contrast, making accurate segmentation \nchallenging. Boundary Delineation: Precisely delineating tumour \nboundaries is difficult due to factors like image noise, low contrast, and \nthe presence of artifacts. Example: Glioblastoma segmentation in brain \nMRIs is difficult due to the infiltrative nature of the tumour, which can \nblend with surrounding brain tissues, complicating boundary \ndelineation.\n5.1.4. Interpretability and trustworthiness\nBlack-Box Nature: While deep learning models have achieved high \naccuracy in cancer detection, segmentation, and classification, their lack \nof interpretability makes it challenging to understand the reasoning \nbehind their predictions, which is crucial for clinical decision-making. \nExample: In colorectal cancer detection using colonoscopy images, the \ndeep learning model\u2019s decision-making process can be opaque, making \nit difficult for clinicians to understand why certain polyps are classified \nas malignant. Explainability: Providing understandable explanations \nfor model predictions is essential for clinical acceptance but remains a \nsignificant challenge. Example: For pancreatic cancer detection using CT \nscans, providing clear explanations for the model\u2019s predictions is crucial \nfor gaining clinician trust and ensuring the model\u2019s integration into the \ndiagnostic process.\n5.1.5. Computational and infrastructure challenges\nHigh Computational Costs: Training deep learning models on large \nmedical imaging datasets requires substantial computational resources, \nincluding powerful GPUs and significant memory. Example: Training a \ndeep learning model for whole-slide imaging in histopathology to detect \nbreast cancer involves processing gigapixel images, requiring substan\u00ad\ntial computational resources. Data Storage: Medical imaging data, such \nas high-resolution scans, requires large storage capacities and efficient \ndata management systems. Example: High-resolution PET scans for \ndetecting metastasis in various cancers require large storage capacities, \nnecessitating efficient data management solutions. Integration with \nExisting Systems: Integrating deep learning models into existing clin\u00ad\nical workflows and electronic health record (EHR) systems is a complex \ntask, requiring interoperability and seamless data exchange. Example: \nIntegrating a deep learning model for prostate cancer detection into \nexisting radiology workflows and EHR systems can be challenging, \nrequiring significant efforts for interoperability and seamless data \nexchange.\n5.2. Future directions\n5.2.1. Development of robust and interpretable models\nHybrid Models: Combining deep learning with traditional machine \nlearning and statistical methods to enhance interpretability. Example: \nCombining CNNs with traditional image processing techniques for bet\u00ad\nter interpretability in detecting liver cancer from ultrasound images. \nExplainable AI (XAI): Investing in research to develop methods that \nprovide clear, understandable explanations for model predictions. \nExample: Developing XAI methods to provide heatmaps highlighting \nregions of interest in mammograms for breast cancer detection, helping \nradiologists understand model predictions.\n5.2.2. Enhancement of data availability and quality\nData Augmentation and Synthesis: Utilizing advanced data \naugmentation techniques and synthetic data generation to overcome \ndata scarcity and imbalance. Example: Using GANs to create synthetic \nCT scans for lung cancer, improving model training by augmenting \nlimited datasets. Collaborative Data Sharing: Encouraging collabora\u00ad\ntive efforts and frameworks for secure data sharing across institutions to \nbuild larger, more diverse datasets. Example: Establishing secure data- \nsharing frameworks across hospitals to build a larger, more diverse \ndataset for brain tumour detection using MRI.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n19 \n\n5.2.3. Improving model generalization\nTransfer Learning: Leveraging transfer learning to adapt pre- \ntrained models to new datasets with limited labelled data. Example: \nApplying transfer learning from a pre-trained model on general medical \nimages to improve performance in detecting skin cancer from dermo\u00ad\nscopic images. Domain Adaptation Techniques: Developing advanced \ndomain adaptation methods to ensure models perform well across \ndifferent populations and imaging settings. Example: Developing \ndomain adaptation methods to ensure a liver cancer detection model \ntrained on one type of CT scanner generalizes well to images from \ndifferent scanners.\n5.2.4. Advanced segmentation techniques\nMulti-Modal Imaging: Combining information from multiple im\u00ad\naging modalities (e.g., MRI, CT, PET) to improve segmentation accuracy. \nExample: Combining PET and CT scans for better segmentation of lung \ntumours, leveraging the complementary information from both modal\u00ad\nities. 3D Segmentation: Developing and refining 3D segmentation al\u00ad\ngorithms to better capture the spatial characteristics of tumours. \nExample: Developing 3D segmentation algorithms to capture the spatial \ncharacteristics of brain tumours from MRI, providing more accurate and \ncomprehensive tumour delineation.\n5.2.5. Integration into clinical practice\nClinical Trials and Validation: Conducting extensive clinical trials \nto validate the effectiveness and safety of deep learning models in real- \nworld settings. Example: Conducting clinical trials to validate the \neffectiveness and safety of a deep learning model for colorectal cancer \ndetection using colonoscopy videos in real-world settings.\n5.2.6. Addressing ethical and privacy concerns\nFederated Learning: Implementing federated learning approaches \nto train models on decentralized data without compromising patient \nprivacy. Example: Implementing federated learning approaches to train \nmodels on decentralized breast cancer mammogram data from multiple \ninstitutions without compromising patient privacy. Ethical Guidelines: \nEstablishing comprehensive ethical guidelines for the development and \ndeployment of AI in medical imaging to ensure patient safety and data \nsecurity. Example: Establishing ethical guidelines for the deployment of \nAI in detecting pancreatic cancer from CT scans, ensuring patient safety \nand data security.\n6. Conclusion\nThe integration of deep learning algorithms into medical imaging for \ncancer detection, segmentation, and classification presents significant \nadvancements in diagnostic accuracy, efficiency, and patient care. This \nsurvey highlights the notable successes in detecting various cancers, \nincluding lung, breast, prostate, brain, and skin cancers, while also \nidentifying key research challenges and inherent limitations. Addressing \nissues such as data quality, model generalization, interpretability, and \ncomputational demands is essential for further progress. Future research \nshould focus on developing robust, interpretable models, enhancing \ndata availability, improving model generalization, and ensuring ethical \nand privacy considerations are met. By tackling these challenges and \nfocusing on these future directions, the field can achieve more accurate, \nreliable, and clinically applicable solutions, ultimately revolutionizing \ncancer diagnosis and treatment and leading to better patient outcomes.\nConflict of Interest Statement\nThe authors declare that the research was conducted without any \ncommercial or financial relationships that could be construed as a po\u00ad\ntential conflict of interest.\nAuthor Contributions\nIstiak Ahmad: Conceptualization, methodology, software, valida\u00ad\ntion, formal analysis, investigation, resources, data curation, wri\u00ad\nting\u2014original \ndraft, \nvisualization, \nfunding \nacquisition. \nFahad \nAlqurashi: Conceptualization, validation, formal analysis, investiga\u00ad\ntion, resources, writing\u2014review and editing, supervision, project \nadministration. All authors have read and agreed to the published \nversion of the manuscript.\nFunding statement\nThe authors acknowledge with thanks the technical and financial \nsupport from the Deanship of Scientific Research (DSR) at the King \nAbdulaziz University (KAU), Jeddah, Saudi Arabia, under Grant No. \n(IFPIP: 1327\u2013611\u20131443).\nDeclaration of Competing Interest\nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper.\nReferences\nAbraham, B., Nair, M.S., 2018. Computer-aided classification of prostate cancer grade \ngroups from mri images using texture features and stacked sparse autoencoder. \nComput. Med. Imaging Graph. 69, 60\u201368.\nAdmass, W.S., Munaye, Y.Y., Salau, A.O., 2024. Integration of feature enhancement \ntechnique in google inception network for breast cancer detection and classification. \nJ. Big Data 11 (1), 78.\nAhmad, I., AlQurashi, F., Abozinadah, E., Mehmood, R., 2021. A novel deep learning- \nbased online proctoring system using face recognition, eye blinking, and object \ndetection techniques. Int. J. Adv. Comput. Sci. Appl. 12 (10).\nAhmad, I., Alqurashi, F., Abozinadah, E., Mehmood, R., 2022a. Deep journalism and \ndeepjournal v1. 0: a data-driven deep learning approach to discover parameters for \ntransportation. Sustainability 14 (9), 5711.\nI. Ahmad, F. AlQurashi, and R. Mehmood.Machine and deep learning methods with \nmanual and automatic labelling for news classification in bangla language. arXiv \npreprint arXiv:2210.10903, 2022b.\nI. Ahmad, F. AlQurashi, and R. Mehmood.Potrika: Raw and balanced newspaper datasets \nin the bangla language with eight topics and five attributes. arXiv preprint arXiv: \n2210.09389, 2022c.\nAl Husaini, M.A.S., Habaebi, M.H., Gunawan, T.S., Islam, M.R., Elsheikh, E.A.A., \nSuliman, F.M., 2022. Thermal-based early breast cancer detection using inception \nv3, inception v4 and modified inception mv4. Neural Comput. Appl. 34 (1), \n333\u2013348.\nAl-Dhabyani, W., Gomaa, M., Khaled, H., Fahmy, A., 2020. Dataset of breast ultrasound \nimages. Data Brief. 28, 104863.\nAl-Huseiny, M.S., Sajit, A.S., 2021. Transfer learning with googlenet for detection of lung \ncancer. Indones. J. Electr. Eng. Comput. Sci. 22 (2), 1078\u20131086.\nAlqahtani, H., Alabdulkreem, E., Alotaibi, F.A., Alnfiai, M.M., Singla, C., Salama, A.S., \n2024. Improved water strider algorithm with convolutional autoencoder for lung \nand colon cancer detection on histopathological images. IEEE Access 12, 949\u2013956.\nAlqarafi, A., Khan, A.A., Mahendran, R.K., Al-Sarem, M., Albalwy, F., 2024. Multi-scale \ngc-t2: Automated region of interest assisted skin cancer detection using multi-scale \ngraph convolution and tri-movement based attention mechanism. Biomed. Signal \nProcess. Control 95, 106313.\nAlqurashi, F., Ahmad, I., 2024. Scientometric analysis and knowledge mapping of \ncybersecurity. Int. J. Adv. Comput. Sci. Appl. 15 (3).\nAlruwaili, M., Gouda, W., 2022. Automated breast cancer detection models based on \ntransfer learning. Sensors 22 (3), 876.\nAlzubaidi, M.A., Otoom, M., Jaradat, H., 2021. Comprehensive and comparative global \nand local feature extraction framework for lung cancer detection using ct scan \nimages. IEEE Access 9, 158140\u2013158154.\nAnas, M., Haq, I.U., Husnain, G., Faraz Jaffery, S.A., 2024. Advancing breast cancer \ndetection: Enhancing yolov5 network for accurate classification in mammogram \nimages. IEEE Access 12, 16474\u201316488.\nArdakani, A.A., Mohammadi, A., Mirza-Aghazadeh-Attari, M., Acharya, U.R., 2023. An \nopen-access breast lesion ultrasound image database: Applicable in artificial \nintelligence studies. Comput. Biol. Med. 152, 106438.\nAresta, G., Ara\u00fajo, T., Kwok, S., Chennamsetty, S.S., Safwan, M., Alex, V., Marami, B., \nPrastawa, M., Chan, M., Donovan, M., et al., 2019. Bach: Grand challenge on breast \ncancer histology images. Med. Image Anal. 56, 122\u2013139.\nArmato III, S.G., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer, C.R., Reeves, A.P., \nZhao, B., Aberle, D.R., Henschke, C.I., Hoffman, E.A., et al., 2011. The lung image \ndatabase consortium (lidc) and image database resource initiative (idri): a completed \nreference database of lung nodules on ct scans. Med. Phys. 38 (2), 915\u2013931.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n20 \n\nAsadi, B., Memon, Q., 2023. Efficient breast cancer detection via cascade deep learning \nnetwork. Int. J. Intell. Netw. 4, 46\u201352.\nAshraf, R., Afzal, S., Rehman, A.U., Gul, S., Baber, J., Bakhtyar, M., Mehmood, I., \nSong, O.-Y., Maqsood, M., 2020. Region-of-interest based transfer learning assisted \nframework for skin cancer detection. IEEE Access 8, 147858\u2013147871.\nAygin, D., Yaman, \u00a8O., 2022. Breast cancer in men: Risk factors, treatment options, \nquality of life: Systematic review. Clin. Exp. Health Sci. 12 (1), 257\u2013267.\nAzevedo, V., Silva, C., Dutra, I., 2022. Quantum transfer learning for breast cancer \ndetection. Quantum Mach. Intell. 4 (1), 5.\nBashkanov, O., Rak, M., Meyer, A., Engelage, L., Lumiani, A., Muschter, R., Hansen, C., \n2023. Automatic detection of prostate cancer grades and chronic prostatitis in \nbiparametric mri. Comput. Methods Prog. Biomed. 239, 107624.\nBhatt, A.R., Ganatra, A., Kotecha, K., 2021. Cervical cancer detection in pap smear whole \nslide images using convnet with transfer learning and progressive resizing. PeerJ \nComput. Sci. 7, e348.\nBhattacharjee, A., Murugan, R., Goel, T., 2022. A hybrid approach for lung cancer \ndiagnosis using optimized random forest classification and k-means visualization \nalgorithm. Health Technol. 12 (4), 787\u2013800.\nBilic, P., Christ, P., Li, H.B., Vorontsov, E., Ben-Cohen, A., Kaissis, G., Szeskin, A., \nJacobs, C., Mamani, G.E.H., Chartrand, G., et al., 2023. The liver tumor \nsegmentation benchmark (lits). Med. Image Anal. 84, 102680.\nBoudouh, S.S., Bouakkaz, M., 2023. Breast cancer: toward an accurate breast tumor \ndetection model in mammography using transfer learning techniques. Multimed. \nTools Appl. 82 (22), 34913\u201334936.\nBrain tumor facts. National Brain Tumor Society, \u3008https:\/\/braintumor.org\/brain-tumor \ns\/about-brain-tumors\/brain-tumor-facts\/\u3009.2024, Accessed: 05-07-2024.\nBray, F., Laversanne, M., Sung, H., Ferlay, J., Siegel, R.L., Soerjomataram, I., Jemal, A., \n2024. Global cancer statistics 2022: Globocan estimates of incidence and mortality \nworldwide for 36 cancers in 185 countries. CA: a Cancer J. Clin. 74 (3), 229\u2013263.\nBreast cancer.World Health Organization, \u3008https:\/\/www.who.int\/news-room\/fact-sheets \n\/detail\/breast-cancer\u3009, 2024.Accessed: 05-07-2024.\nBulten, W., Kartasalo, K., Chen, P.-H.C., Str\u00a8om, P., Pinckaers, H., Nagpal, K., Cai, Y., \nSteiner, D.F., Van Boven, H., Vink, R., et al., 2022. Artificial intelligence for \ndiagnosis and gleason grading of prostate cancer: the panda challenge. Nat. Med. 28 \n(1), 154\u2013163.\nCalderon-Ramirez, S., Murillo-Hernandez, D., Rojas-Salazar, K., Elizondo, D., Yang, S., \nMoemeni, A., Molina-Cabello, M., 2022. A real use case of semi-supervised learning \nfor mammogram classification in a local clinic of costa rica. Med. Biol. Eng. Comput. \n60 (4), 1159\u20131175.\nCancer stat facts: Brain and other nervous system cancer. National Cancer Institute, \u3008htt \nps:\/\/seer.cancer.gov\/statfacts\/html\/brain.html\u3009.2024, Accessed: 05-07-2024.\nCancer stat facts: Liver and intrahepatic bile duct cancer. National Cancer Institute, \u3008htt \nps:\/\/seer.cancer.gov\/statfacts\/html\/livibd.html\u3009.2024.Accessed: 05-07-2024.\nCancer stat facts: Melanoma of the skin. National Cancer Institute, \u3008https:\/\/seer.cancer. \ngov\/statfacts\/html\/melan.html\u3009.2024, Accessed: 05-07-2024.\nCancer stat facts: Pancreatic cancer. National Cancer Institute, \u3008https:\/\/seer.cancer.gov\/s \ntatfacts\/html\/pancreas.html\u3009.2024, Accessed: 05-07-2024.\nCassidy, B., Kendrick, C., Brodzicki, A., Jaworek-Korjakowska, J., Yap, M.H., 2022. \nAnalysis of the isic image datasets: Usage, benchmarks and recommendations. Med. \nImage Anal. 75, 102305.\nCervical cancer. World Health Organization, \u3008https:\/\/www.who.int\/news-room\/fact-sh \neets\/detail\/cervical-cancer\u3009, 2024.Accessed: 05-07-2024.\nChen, C.-B., Wang, Y., Fu, X., Yang, H., 2023. Recurrence network analysis of \nhistopathological images for the detection of invasive ductal carcinoma in breast \ncancer. IEEE\/ACM Trans. Comput. Biol. Bioinforma. 20 (5), 3234\u20133244.\nChenyang, L., Chan, S.-C., 2020. A joint detection and recognition approach to lung \ncancer diagnosis from ct images with label uncertainty. IEEE Access 8, \n228905\u2013228921.\nChicco, D., Jurman, G., 2020. The advantages of the matthews correlation coefficient \n(mcc) over f1 score and accuracy in binary classification evaluation. BMC Genom. \n21, 1\u201313.\nChikkalingaiah, S., RaoHari Prasad, S.A.P., Uggregowda, L.D., 2023. Classification \ntechniques using gray level co-occurrence matrix features for the detection of lung \ncancer using computed tomography imaging. Int. J. Electr. Comput. Eng. (2088- \n8708) 13 (5).\nChlebus, G., Schenk, A., Moltz, J.H., van Ginneken, B., Hahn, H.K., Meine, H., 2018. \nAutomatic liver tumor segmentation in ct with fully convolutional neural networks \nand object-based postprocessing. Sci. Rep. 8 (1), 15497.\nClark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S., Phillips, S., \nMaffitt, D., Pringle, M., et al., 2013. The cancer imaging archive (tcia): maintaining \nand operating a public information repository. J. Digit. Imaging 26, 1045\u20131057.\nN.C.F. Codella, D. Gutman, M.E. Celebi, B. Helba, M.A. Marchetti, S.W. Dusza, A. Kalloo, \nK. Liopyris, N. Mishra, H. Kittler, et al. Skin lesion analysis toward melanoma \ndetection: A challenge at the 2017 international symposium on biomedical imaging \n(isbi), hosted by the international skin imaging collaboration (isic).In: 2018 IEEE \n15th international symposium on biomedical imaging (ISBI 2018), 168-172.IEEE, \n2018.\nN. Codella, V. Rotemberg, P. Tschandl, M.E. Celebi, S. Dusza, D. Gutman, B. Helba, A. \nKalloo, K. Liopyris, M. Marchetti, et al. Skin lesion analysis toward melanoma \ndetection 2018: A challenge hosted by the international skin imaging collaboration \n(isic).arXiv preprint arXiv:1902.03368, 2019.\nCohen, J., 1960. A coefficient of agreement for nominal scales. Educ. Psychol. Meas. 20 \n(1), 37\u201346.\nColon cancer. World Health Organization, \u3008https:\/\/www.who.int\/news-room\/fact-sheet \ns\/detail\/colorectal-cancer\u3009, 2023.Accessed: 05-07-2024.\nM. Combalia, N.C.F. Codella, V. Rotemberg, B. Helba, V. Vilaplana, O. Reiter, C. Carrera, \nA. Barreiro, A.C. Halpern, S. Puig, et al. Bcn20000: Dermoscopic lesions in the wild. \narXiv preprint arXiv:1908.02288, 2019.\nCruz-Roa, A., Basavanhally, A., Gonz\u00b4alez, F., Gilmore, H., Feldman, M., Ganesan, S., \nShih, N., Tomaszewski, J., Madabhushi, A., 2014. Automatic detection of invasive \nductal carcinoma in whole slide images with convolutional neural networks. volume \n9041 Medical Imaging 2014: Digital Pathology. SPIE, 904103. volume 9041. \nde Vente, C., Vos, P., Hosseinzadeh, M., Pluim, J., Veta, M., 2021. Deep learning \nregression for prostate cancer detection and grading in bi-parametric mri. IEEE \nTrans. Biomed. Eng. 68 (2), 374\u2013383.\nDeo, B.S., Pal, M., Panigrahi, P.K., Pradhan, A., 2024. An ensemble deep learning model \nwith empirical wavelet transform feature for oral cancer histopathological image \nclassification. Int. J. Data Sci. Anal. 1\u201318.\nDevi, M.A., Sheeba, J.I., Joseph, K.S., 2022. Neutrosophic graph cut-based segmentation \nscheme for efficient cervical cancer detection. J. King Saud. Univ. -Comput. Inf. Sci. \n34 (1), 1352\u20131360.\nDey, S., Roychoudhury, R., Malakar, S., Sarkar, R., 2022. Screening of breast cancer from \nthermogram images by edge detection aided deep transfer learning model. \nMultimed. Tools Appl. 81 (7), 9331\u20139349.\nDong, X., Zhou, Y., Wang, L., Peng, J., Lou, Y., Fan, Y., 2020. Liver cancer detection using \nhybridized fully convolutional neural network based on deep learning framework. \nIEEE Access 8, 129889\u2013129898.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. \nDehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: \nTransformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020.\nDuran-Lopez, L., Dominguez-Morales, J.P., Conde-Martin, A.F., Vicente-Diaz, S., Linares- \nBarranco, A., 2020. Prometeo: A cnn-based computer-aided diagnosis system for wsi \nprostate cancer detection. IEEE Access 8, 128613\u2013128628.\nDuran-Lopez, L., Dominguez-Morales, J.P., Gutierrez-Galan, D., Rios-Navarro, A., \nJimenez-Fernandez, A., Vicente-Diaz, S., Linares-Barranco, A., 2021. Wide & deep \nneural network model for patch aggregation in cnn-based prostate cancer detection \nsystems. Comput. Biol. Med. 136, 104743.\nElakkiya, R., Subramaniyaswamy, V., Vijayakumar, V., Mahanti, A., 2022. Cervical \ncancer diagnostics healthcare system using hybrid object detection adversarial \nnetworks. IEEE J. Biomed. Health Inform. 26 (4), 1464\u20131471.\nEsophageal cancer. National Cancer Institute, \u3008https:\/\/www.cancer.gov\/types\/eso \nphageal\u3009.2024 Accessed: 05-07-2024.\nExplore cancer statistics. American Cancer Society, \u3008https:\/\/cancerstatisticscenter.can \ncer.org\/#\/\u3009, 2024.Accessed: 10-07-2024.\nFahad, N.M., Azam, S., Montaha, S., Hossain Mukta, Md.S., 2024. Enhancing cervical \ncancer diagnosis with graph convolution network: Ai-powered segmentation, feature \nanalysis, and classification for early detection. Multimed. Tools Appl. 1\u201325.\nFlorimbi, G., Fabelo, H., Torti, E., Ortega, S., Marrero-Martin, M., Callico, G.M., \nDanese, G., Leporati, F., 2020. Towards real-time computing of intraoperative \nhyperspectral imaging for brain cancer detection using multi-gpu platforms. IEEE \nAccess 8, 8485\u20138501.\nFraiwan, M., Faouri, E., 2022. On the automatic detection and classification of skin \ncancer using deep transfer learning. Sensors 22 (13), 4963.\nFuentes-Fino, R., Calder\u00b4on-Ram\u00edrez, S., Dom\u00ednguez, E., L\u00b4opez-Rubio, E., Elizondo, D., \nMolina-Cabello, M.A., 2023. An uncertainty estimator method based on the \napplication of feature density to classify mammograms for breast cancer detection. \nNeural Comput. Appl. 35 (30), 22151\u201322161.\nGade, A., Dash, D.K., Kumari, T.M., Ghosh, S.K., Tripathy, R.K., Pachori, R.B., 2023. \nMultiscale analysis domain interpretable deep neural network for detection of breast \ncancer using thermogram images. IEEE Trans. Instrum. Meas. 72, 1\u201313.\nR. Girshick.Fast r-cnn.In: Proceedings of the IEEE international conference on computer \nvision, 1440-1448, 2015.\nGon\u00e7alves, C.B., Souza, J.R., Fernandes, H., 2022. Cnn architecture optimization using \nbio-inspired algorithms for breast cancer detection in infrared images. Comput. Biol. \nMed. 142, 105205.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \nCourville, A., Bengio, Y., 2020. Generative adversarial networks. Commun. ACM 63 \n(11), 139\u2013144.\nD. Gutman, N.C.F. Codella, E. Celebi, B. Helba, M. Marchetti, N. Mishra, and A. Halpern. \nSkin lesion analysis toward melanoma detection: A challenge at the international \nsymposium on biomedical imaging (isbi) 2016, hosted by the international skin \nimaging collaboration (isic).arXiv preprint arXiv:1605.01397, 2016.\nHammouche, R., Attia, A., Akhrouf, S., Akhtar, Z., 2022. Gabor filter bank with deep \nautoencoder based face recognition system. Expert Syst. Appl. 197, 116743.\nHaq, I., Mazhar, T., Asif, R.N., Ghadi, Y.Y., Saleem, R., Mallek, F., Hamam, H., 2023. \nA deep learning approach for the detection and counting of colon cancer cells (ht-29 \ncells) bunches and impurities. PeerJ Comput. Sci. 9, e1651.\nHaron, N., Zain, R.B., Ramanathan, A., Abraham, M.T., Liew, C.S., Ng, K.G., Cheng, L.C., \nHusin, R.B., Chong, S.M.Y., Thangavalu, L.A.P., et al., 2020. m-health for early \ndetection of oral cancer in low-and middle-income countries. Telemed. e-Health 26 \n(3), 278\u2013285.\nK. He, G. Gkioxari, P., Doll\u00b4ar, and R. Girshick.Mask r-cnn.In: Proceedings of the IEEE \ninternational conference on computer vision, 2961-2969, 2017.\nHeath, M., Bowyer, K., Kopans, D., Kegelmeyer Jr, P., Moore, R., Chang, K., \nMunishkumaran, S., 1998. Current status of the digital database for screening \nmammography. In: Digital Mammography: Nijmegen, 1998. Springer, pp. 457\u2013460.\nHosseini, F., Asadi, F., Emami, H., Ebnali, M., 2023. Machine learning applications for \nearly detection of esophageal cancer: a systematic review. BMC Med. Inform. Decis. \nMak. 23 (1), 124.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n21 \n\nA.G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, \nand H. Adam.Mobilenets: Efficient convolutional neural networks for mobile vision \napplications.arXiv preprint arXiv:1704.04861, 2017.\nA. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, \nV. Vasudevan, et al. Searching for mobilenetv3.In: Proceedings of the IEEE\/CVF \ninternational conference on computer vision, 1314-1324, 2019.\nHu, Z., Tang, J., Wang, Z., Zhang, K., Zhang, L., Sun, Q., 2018. Deep learning for image- \nbased cancer detection and diagnosis- a survey. Pattern Recognit. 83, 134\u2013149.\nHuaping, J., Junlong, Z., Norouzzadeh Gil Molk, A.M., 2021. Skin cancer detection using \nkernel fuzzy c-means and improved neural network optimization algorithm. Comput. \nIntell. Neurosci. 1, 9651957.\nHussain, S., Mubeen, I., Ullah, N., UdDinShah, S.S., Khan, B.A., Zahoor, M., Ullah, R., \nKhan, F.A., Sultan, M.A., 2022. Modern diagnostic imaging technique applications \nand risk factors in the medical field: a review. BioMed. Res. Int. 2022 (1), 5164970.\nHussain, E., 2019. Liquid based cytology pap smear images for multi-class diagnosis of \ncervical cancer. Data Brief. 4.\nF. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and K. Keutzer.Densenet: \nImplementing efficient convnet descriptor pyramids.arXiv preprint arXiv:1404.1869, \n2014.\nImran, A., Nasir, A., Bilal, M., Sun, G., Alzahrani, A., Almuhaimeed, A., 2022. Skin cancer \ndetection using combined decision of deep learners. ieee Access 10 (october), \n118198\u2013118212.\nIqbal, S., Siddiqui, G.F., Rehman, A., Hussain, L., Saba, T., Tariq, U., Abbasi, A.A., 2021. \nProstate cancer detection using deep learning and traditional techniques. IEEE \nAccess 9, 27085\u201327100.\nJafari, Z., Karami, E., 2023. Breast cancer detection in mammography images: A cnn- \nbased approach with feature selection. Information 14 (7), 410.\nJain, D.K., Lakshmi, K.M., Varma, K.P., Ramachandran, M., Bharati, S., 2022. Lung \ncancer detection based on kernel pca-convolution neural network feature extraction \nand classification by fast deep belief neural network in disease management using \nmultimedia data sources. Comput. Intell. Neurosci. 1, 3149406.\nJi, M., Zhong, J., Xue, R., Su, W., Kong, Y., Fei, Y., Ma, J., Wang, Y., Mi, L., 2022. Early \ndetection of cervical cancer by fluorescence lifetime imaging microscopy combined \nwith unsupervised machine learning. Int. J. Mol. Sci. 23 (19), 11476.\nKasban, H., El-Bendary, M.A.M., Salama, D.H., 2015. A comparative study of medical \nimaging techniques. Int. J. Inf. Sci. Intell. Syst. 4 (2), 37\u201358.\nKashyap, R., 2023. Stochastic dilated residual ghost model for breast cancer detection. \nJ. Digit. Imaging 36 (2), 562\u2013573.\nKaur, C., Garg, U., 2023. Artificial intelligence techniques for cancer detection in medical \nimage processing: A review. Mater. Today.: Proc. 81, 806\u2013809.\nKavitha, P., Ayyappan, G., Jayagopal, P., Mathivanan, S.K., Mallik, S., Al-Rasheed, A., \nAlqahtani, M.S., Soufiene, B.O., 2023. Detection for melanoma skin cancer through \naccf, bppf, and clf techniques with machine learning approach. BMC Bioinforma. 24 \n(1), 458.\nKhan, S., Nosheen, F., AliNaqvi, S.S., Jamil, H., Faseeh, M., AliKhan, M., Kim, D.-H., \n2024. Bilevel hyperparameter optimization and neural architecture search for \nenhanced breast cancer detection in smart hospitals interconnected with \ndecentralized federated learning environment. IEEE Access 12, 63618\u201363628.\nKrizhevsky, A., Sutskever, I., Hinton, G.E., 2017. Imagenet classification with deep \nconvolutional neural networks. Commun. ACM 60 (6), 84\u201390.\nKumar, Y., Gupta, S., Singla, R., Hu, Y.-C., 2022. A systematic review of artificial \nintelligence techniques in cancer prediction and diagnosis. Arch. Comput. Methods \nEng. 29 (4), 2043\u20132070.\nKumar, A., Singh, A.K., Singh, A., Kumar, V., Prakash, S., Tiwari, P.K., 2024. An efficient \nframework for brain cancer identification using deep learning. Multimed. Tools \nAppl. 1\u201330.\nLee, R.S., Gimenez, F., Hoogi, A., Miyake, K.K., Gorovoy, M., Rubin, D.L., 2017. \nA curated mammography data set for use in computer-aided detection and diagnosis \nresearch. Sci. data 4 (1), 1\u20139.\nLi, X., Chen, H., Qi, X., Dou, Q., Fu, C.-W., Heng, P.-A., 2018. H-denseunet: hybrid \ndensely connected unet for liver and tumor segmentation from ct volumes. IEEE \nTrans. Med. Imaging 37 (12), 2663\u20132674.\nLi, G., Li, C., Wu, G., Ji, D., Zhang, H., 2021a. Multi-view attention-guided multiple \ninstance detection network for interpretable breast cancer histopathological image \ndiagnosis. IEEE Access 9, 79671\u201379684.\nLi, Z., Li, Z., Chen, Q., Ramos, A., Zhang, J., Boudreaux, J.P., Thiagarajan, R., Bren- \nMattison, Y., Dunham, M.E., McWhorter, A.J., et al., 2021b. Detection of pancreatic \ncancer by convolutional-neural-network-assisted spontaneous raman spectroscopy \nwith critical feature visualization. Neural Netw. 144, 455\u2013464.\nLiberini, V., Pizzuto, D.A., Messerli, M., Orita, E., Gr\u00fcnig, H., Maurer, A., Mader, C., \nHusmann, L., Deandreis, D., Kotasidis, F., et al., 2022. Bsrem for brain metastasis \ndetection with 18f-fdg-pet\/ct in lung cancer patients. J. Digit. Imaging 35 (3), \n581\u2013593.\nLitjens, G., Debats, O., Barentsz, J., Karssemeijer, N., Huisman, H., 2014a. Computer- \naided detection of prostate cancer in mri. IEEE Trans. Med. Imaging 33 (5), \n1083\u20131092.\nLitjens, G., Toth, R., Van De Ven, W., Hoeks, C., Kerkstra, S., Van Ginneken, B., \nVincent, G., Guillard, G., Birbeck, N., Zhang, J., et al., 2014b. Evaluation of prostate \nsegmentation algorithms for mri: the promise12 challenge. Med. Image Anal. 18 (2), \n359\u2013373.\nLiver and bile duct cancer. National Cancer Institute, \u3008https:\/\/www.cancer.gov\/types\/ \nliver\u3009.2024.Accessed: 05-07-2024.\nLu, X., Nanehkaran, Y.A., Fard, M.K., 2021. A method for optimal detection of lung \ncancer based on deep learning optimized by marine predators algorithm. Comput. \nIntell. Neurosci. 2021 (1), 3694723.\nLu, S.-Y., Wang, S.-H., Zhang, Y.-D., 2022. Safnet: A deep spatial attention network with \nclassifier fusion for breast cancer detection. Comput. Biol. Med. 148, 105812.\nLung cancer.World Health Organization, \u3008https:\/\/www.who.int\/news-room\/fact-sheet \ns\/detail\/lung-cancer\u3009, 2023.Accessed: 05-07-2024.\nMalik, H., Anees, T., Din, M., Naeem, A., 2023. Cdc_net: Multi-classification \nconvolutional neural network model for detection of covid-19, pneumothorax, \npneumonia, lung cancer, and tuberculosis using chest x-rays. Multimed. Tools Appl. \n82 (9), 13855\u201313880.\nMarinakis, Y., Marinaki, M., Dounias, G., Jantzen, J., Bjerregaard, B., 2009a. Intelligent \nand nature inspired optimization methods in medicine: the pap smear cell \nclassification problem. Expert Syst. 26 (5), 433\u2013457.\nMarinakis, Y., Dounias, G., Jantzen, J., 2009b. Pap smear diagnosis using a hybrid \nintelligent scheme focusing on genetic algorithm based feature selection and nearest \nneighbor classification. Comput. Biol. Med. 39 (1), 69\u201378.\nMarzouk, R., Alabdulkreem, E., Dhahbi, S., Nour, M.K., Duhayyim, M.A., Othman, M., \nHamza, M.A., Motwakel, A., Yaseen, I., Rizwanullah, M., 2022. Deep transfer \nlearning driven oral cancer detection and classification model. Comput., Mater. \nContin. 73 (2).\nMasood, A., Yang, P., Sheng, B., Li, H., Li, P., Qin, J., Lanfranchi, V., Kim, J., Feng, D.D., \n2020a. Cloud-based automated clinical decision support system for detection and \ndiagnosis of lung cancer in chest ct. IEEE J. Transl. Eng. Health Med. 8, 1\u201313.\nMasood, A., Sheng, B., Yang, P., Li, P., Li, H., Kim, J., Feng, D.D., 2020b. Automated \ndecision support system for lung cancer detection and classification via enhanced \nrfcn with multilayer fusion rpn. IEEE Trans. Ind. Inform. 16 (12), 7791\u20137801.\nMaurya, S., Tiwari, S., Mothukuri, M.C., Tangeda, C.M., Nandigam, R.N.S., Addagiri, D. \nC., 2023. A review on recent developments in cancer detection using machine \nlearning and deep learning models. Biomed. Signal Process. Control 80, 104398.\nMazroa, A.A., Ishak, M.K., Aljarbouh, A., Mostafa, S.M., 2023. Improved bald eagle \nsearch optimization with deep learning-based cervical cancer detection and \nclassification. IEEE Access 11, 135175\u2013135184.\nMcNeal, J.E., Redwine, E.A., Freiha, F.S., Stamey, T.A., 1988. Zonal distribution of \nprostatic adenocarcinoma: correlation with histologic pattern and direction of \nspread. Am. J. Surg. Pathol. 12 (12), 897\u2013906.\nT. Mendon\u00e7a, P.M. Ferreira, J.S. Marques, A.R.S. Marcal, and J. Rozeira.Ph2 - a \ndermoscopic image database for research and benchmarking.In: 2013 35th Annual \nInternational Conference of the IEEE Engineering in Medicine and Biology Society \n(EMBC), 5437-5440, 2013.\nMercaldo, F., Brunese, L., Martinelli, F., Santone, A., Cesarelli, M., 2023. Explainable \nconvolutional neural networks for brain cancer detection and localisation. Sensors \n23 (17), 7614.\nMidasala, V.D., Prabhakar, B., Chaitanya, J.K., Sirnivas, K., Eshwar, D., Kumar, P.M., \n2024. Mfeuslnet: Skin cancer detection and classification using integrated ai with \nmultilevel feature extraction-based unsupervised learning. Eng. Sci. Technol., Int. J. \n51, 101632.\nMohakud, R., Dash, R., 2022. Designing a grey wolf optimization based hyper-parameter \noptimized convolutional neural network classifier for skin cancer detection. J. King \nSaud. Univ. -Comput. Inf. Sci. 34 (8), 6280\u20136291.\nMoreira, I.C., Amaral, I., Domingues, I., Cardoso, A., Cardoso, M.J., Cardoso, J.S., 2012. \nInbreast: toward a full-field digital mammographic database. Acad. Radiol. 19 (2), \n236\u2013248.\nMukhedkar, M., Rohatgi, D., Vuyyuru, V.A., Ramakrishna, K.V.S.S., Baker El-Ebiary, Y. \nA., Asir Daniel, V.A., 2023. Feline wolf net: A hybrid lion-grey wolf optimization \ndeep learning model for ovarian cancer detection. Int. J. Adv. Comput. Sci. Appl. 14 \n(9).\nMunir, K., Elahi, H., Ayub, A., Frezza, F., Rizzi, A., 2019. Cancer diagnosis using deep \nlearning: a bibliographic review. Cancers 11 (9), 1235.\nMurthy, N.S., Bethala, C., 2023. Review paper on research direction towards cancer \nprediction and prognosis using machine learning and deep learning models. \nJ. Ambient Intell. Humaniz. Comput. 14 (5), 5595\u20135613.\nMyriam, H., Abdelhamid, A.A., El-Kenawy, E.-S.M., Ibrahim, A., Eid, M.M., Jamjoom, M. \nM., Khafaga, D.S., 2023. Advanced meta-heuristic algorithm based on particle swarm \nand al-biruni earth radius optimization methods for oral cancer detection. IEEE \nAccess 11, 23681\u201323700.\nNaeem, A., Anees, T., Khalil, M., Zahra, K., Naqvi, R.A., Lee, S.-W., 2024. Snc_net: Skin \ncancer detection by integrating handcrafted and deep learning-based features using \ndermoscopy images. Mathematics 12 (7), 1030.\nNagaRamesh, J.V., Abirami, T., Gopalakrishnan, T., Narayanasamy, K., Ishak, M.K., \nKarim, F.K., Mostafa, S.M., Allakany, A., 2023. Sparrow search algorithm with \nstacked deep learning based medical image analysis for pancreatic cancer detection \nand classification. IEEE Access 11, 111927\u2013111935.\nNair, L.V., Jerome, S.A., 2024. Optimized pixel level image fusion for lung cancer \ndetection over mri and pet image. Multimed. Tools Appl. 1\u201323.\nNakano, K., Saito, Y., Kurabuchi, Y., Ohnishi, T., Ota, S., Uesato, M., Haneishi, H., 2020. \nDesign of multiband switching illumination with low-concentration lugol stain for \nesophageal cancer detection. IEEE Access 8, 216043\u2013216054.\nNapte, K.M., Mahajan, A., Urooj, S., 2023. Automatic liver cancer detection using deep \nconvolution neural network. IEEE Access 11, 94852\u201394862.\nNishio, M., Matsuo, H., Kurata, Y., Sugiyama, O., Fujimoto, K., 2023. Label distribution \nlearning for automatic cancer grading of histopathological images of prostate cancer. \nCancers 15 (5), 1535.\nNoaman, N.F., Kanber, B.M., Smadi, A.A., Jiao, L., Alsmadi, M.K., 2024. Advancing \noncology diagnostics: Ai-enabled early detection of lung cancer through hybrid \nhistological image analysis. IEEE Access 12, 64396\u201364415.\nObayya, M., Arasi, M.A., Alruwais, N., Alsini, R., Mohamed, A., Yaseen, I., 2023. \nBiomedical image analysis for colon and lung cancer detection using tuna swarm \nalgorithm with deep learning model. IEEE Access 11, 94705\u201394712.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n22 \n\nOral cancer incidence (new cases) by age, race, and gender. National Institute of Dental \nand Craniofacial Research, \u3008https:\/\/www.nidcr.nih.gov\/research\/data-statistics\/ \noral-cancer\/incidence\u3009.2024.Accessed: 05-07-2024.\nK. O\u2019shea, R. Nash.An introduction to convolutional neural networks.arXiv preprint \narXiv:1511.08458, 2015.\nOthman, E., Mahmoud, M., Dhahri, H., Abdulkader, H., Mahmood, A., Ibrahim, M., \n2022. Automatic detection of liver cancer using hybrid pre-trained models. Sensors \n22 (14), 5429.\nOvarian, fallopian tube, and primary peritoneal cancer. National Cancer Institute, \n\u3008https:\/\/www.cancer.gov\/types\/ovarian\u3009.2024.Accessed: 06-07-2024.\nP. Paayas and R. Annamalai.Ocean - ovarian cancer subtype classification and outlier \ndetection using densenet121.In: 2023 Seventh International Conference on Image \nInformation Processing (ICIIP), 827-831, 2023.\nPark, J., Song, S., Kang, S.-H., Lee, Y., 2024. Performance evaluation of improved \nmedian-modified wiener filter with segmentation method to improve resolution in \ncomputed tomographic images. J. Korean Phys. Soc. 84 (7), 573\u2013581.\nPinckaers, H., Bulten, W., Van der Laak, J., Litjens, G., 2021. Detection of prostate cancer \nin whole-slide images through end-to-end training with image-level labels. IEEE \nTrans. Med. Imaging 40 (7), 1817\u20131826.\nPirovano, A., Almeida, L.G., Ladjal, S., Bloch, I., Berlemont, S., 2021. Computer-aided \ndiagnosis tool for cervical cancer screening with weakly supervised localization and \ndetection of abnormalities using adaptable and explainable classifier. Med. Image \nAnal. 73, 102167.\nPlissiti, M.E., Dimitrakopoulos, P., Sfikas, G., Nikou, C., Krikoni, O., Charchanti, A., \n2018. Sipakmed: A new dataset for feature and image based classification of normal \nand pathological cervical cells in pap smear images. 2018 25th IEEE international \nconference on image processing (ICIP). IEEE, pp. 3144\u20133148.\nPrabhu, S., Prasad, K., Robels-Kelly, A., Lu, X., 2022. Ai-based carcinoma detection and \nclassification using histopathological images: A systematic review. Comput. Biol. \nMed. 142, 105209.\nQian, Y., Zhang, Z., Wang, B., 2021. Procdet: A new method for prostate cancer detection \nbased on mr images. IEEE Access 9, 143495\u2013143505.\nRagab, M.G., Abdulkadir, S.J., Muneer, A., Alqushaibi, A., Sumiea, E.H., Qureshi, R., Al- \nSelwi, S.M., Alhussian, H., 2024. A comprehensive systematic review of yolo for \nmedical object detection (2018 to 2023). IEEE Access 12, 57815\u201357836.\nRahman, T.Y., Mahanta, L.B., Das, A.K., Sarma, J.D., 2020. Histopathological imaging \ndatabase for oral cancer analysis. Data Brief. 29, 105114.\nRai, H.M., 2024. Cancer detection and segmentation using machine learning and deep \nlearning techniques: A review. Multimed. Tools Appl. 83 (9), 27001\u201327035.\nRajagopalan, K., Babu, S., 2020. The detection of lung cancer using massive artificial \nneural network based on soft tissue technique. BMC Med. Inform. Decis. Mak. 20, \n1\u201313.\nRen, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: Towards real-time object \ndetection with region proposal networks. Adv. Neural Inf. Process. Syst. 28.\nRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for \nbiomedical image segmentation. Medical image computing and computer-assisted \nintervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, \nOctober 5-9, 2015, proceedings, part III 18. Springer, pp. 234\u2013241.\nRotemberg, V., Kurtansky, N., Betz-Stablein, B., Caffery, L., Chousakos, E., Codella, N., \nCombalia, M., Dusza, S., Guitera, P., Gutman, D., et al., 2021. A patient-centric \ndataset of images and metadata for identifying melanomas using clinical context. Sci. \ndata 8 (1), 34.\nS\u00b4anchez-Cauce, R., P\u00b4erez-Mart\u00edn, J., Luque, M., 2021. Multi-input convolutional neural \nnetwork for breast cancer detection using thermal images and clinical data. Comput. \nMethods Prog. Biomed. 204, 106045.\nSaber, A., Sakr, M., Abo-Seida, O.M., Keshk, A., Chen, H., 2021. A novel deep-learning \nmodel for automatic detection and classification of breast cancer using the transfer- \nlearning technique. IEEE Access 9, 71194\u201371209.\nSadad, T., Hussain, A., Munir, A., Habib, M., Khan, S.A., Hussain, S., Yang, S., \nAlawairdhi, M., 2020. Identification of breast malignancy by marker-controlled \nwatershed transformation and hybrid feature set for healthcare. Appl. Sci. 10 (6), \n1900.\nSaha, A., Hosseinzadeh, M., Huisman, H., 2021. End-to-end prostate cancer detection in \nbpmri via 3d cnns: effects of attention mechanisms, clinical priori and decoupled \nfalse positive reduction. Med. Image Anal. 73, 102155.\nSahoo, P., Saha, S., Mondal, S., Seera, M., Sharma, S.K., Kumar, M., 2023. Enhancing \ncomputer-aided cervical cancer detection using a novel fuzzy rank-based fusion. \nIEEE Access 11, 145281\u2013145294.\nSakr, A.S., Soliman, N.F., Al-Gaashani, M.S., P\u0142awiak, P., Ateya, A.A., Hammad, M., \n2022. An efficient deep learning approach for colon cancer detection. Appl. Sci. 12 \n(17), 8450.\nM. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen.Mobilenetv2: Inverted \nresiduals and linear bottlenecks.In: Proceedings of the IEEE conference on computer \nvision and pattern recognition, 4510-4520, 2018.\nSchwartz, D., Sawyer, T.W., Thurston, N., Barton, J., Ditzler, G., 2022. Ovarian cancer \ndetection using optical coherence tomography and convolutional neural networks. \nNeural Comput. Appl. 34 (11), 8977\u20138987.\nSedghi, A., Mehrtash, A., Jamzad, A., Amalou, A., Wells, W.M., Kapur, T., Kwak, J.T., \nTurkbey, B., Choyke, P., Pinto, P., et al., 2020. Improving detection of prostate \ncancer foci via information fusion of mri and temporal enhanced ultrasound. Int. J. \nComput. Assist. Radiol. Surg. 15, 1215\u20131223.\nSengupta, A., Ye, Y., Wang, R., Liu, C., Roy, K., 2019. Going deeper in spiking neural \nnetworks: Vgg and residual architectures. Front. Neurosci. 13, 95.\nSetio, A.A.A., Traverso, A., De Bel, T., Berens, M.S.N., Van Den Bogaard, C., Cerello, P., \nChen, H., Dou, Q., Fantacci, M.E., Geurts, B., et al., 2017. Validation, comparison, \nand combination of algorithms for automatic detection of pulmonary nodules in \ncomputed tomography images: the luna16 challenge. Med. Image Anal. 42, 1\u201313.\nShah, D., Khan, M.A.U., Abrar, M., Amin, F., Alkhamees, B.F., AlSalman, H., 2024. \nEnhancing the quality and authenticity of synthetic mammogram images for \nimproved breast cancer detection. IEEE Access 12, 12189\u201312198.\nShamim, M.Z.M., Syed, S., Shiblee, M., Usman, M., Ali, S.J., Hussein, H.S., Farrag, M., \n2022. Automated detection of oral pre-cancerous tongue lesions using deep learning \nfor early diagnosis of oral cavity cancer. Comput. J. 65 (1), 91\u2013104.\nSharma, P., Nayak, D.R., Balabantaray, B.K., Tanveer, M., Nayak, R., 2023. A survey on \ncancer detection via convolutional neural networks: Current challenges and future \ndirections. Neural Netw.\nSharmin, S., Ahammad, T., Talukder, Md.A., Ghose, P., 2023. A hybrid dependable deep \nfeature extraction and ensemble-based machine learning approach for breast cancer \ndetection. IEEE Access 11, 87694\u201387708.\nShiraishi, J., Katsuragawa, S., Ikezoe, J., Matsumoto, T., Kobayashi, T., Komatsu, K., \nMatsui, M., Fujita, H., Kodera, Y., Doi, K., 2000. Development of a digital image \ndatabase for chest radiographs with and without a lung nodule: receiver operating \ncharacteristic analysis of radiologists\u2019 detection of pulmonary nodules. Am. J. \nRoentgenol. 174 (1), 71\u201374.\nSiegel, R.L., Giaquinto, A.N., Jemal, A., 2024. Cancer statistics, 2024. CA: a Cancer J. \nClin. 74 (1), 12\u201349.\nSilva, L.F., Saade, D.C.M., Sequeiros, G.O., Silva, A.C., Paiva, A.C., Bravo, R.S., Conci, A., \n2014. A N. Database Breast Res. Infrared Image J. Med. Imaging Health Inform. 4 \n(1), 92\u2013100.\nSkin cancer. American Cancer Society, \u3008https:\/\/www.cancer.org\/cancer\/types\/skin-canc \ner.html\u3009.2024.Accessed: 05-07-2024.\nL. Soler, A. Hostettler, V. Agnus, A. Charnoz, J.-B. Fasquel, J. Moreau, A.-B. Osswald, M. \nBouhadjar, and J. Marescaux.3d image reconstruction for comparison of algorithm \ndatabase.\u3008https:\/\/www.ircad.fr\/research\/data-sets\/liver-segmentation-3d-irca \ndb-01\u3009, 2010.\nSpanhol, F.A., Oliveira, L.S., Petitjean, C., Heutte, L., 2015. A dataset for breast cancer \nhistopathological image classification. Ieee Trans. Biomed. Eng. 63 (7), 1455\u20131462.\nSriwastawa, A., ArulJothi, J.A., 2024. Vision transformer and its variants for image \nclassification in digital breast cancer histopathology: A comparative study. \nMultimed. Tools Appl. 83 (13), 39731\u201339753.\nStabellini, N., Chandar, A.K., Chak, A., Barda, A.J., Dmukauskas, M., Waite, K., \nBarnholtz-Sloan, J.S., 2022. Sex differences in esophageal cancer overall and by \nhistological subtype. Sci. Rep. 12 (1), 5248.\nStrelcenia, E., Prakoonwit, S., 2023. Improving cancer detection classification \nperformance using gans in breast cancer data. IEEE Access 11, 71594\u201371615.\nSuckling J., Parker J., Dance D., Astley S., Hutt I., and Boggis C. Mammographic image \nanalysis society (mias) database v1.21. University of Cambridge Repository, 2015.\nSun, L., Ma, W., Ding, X., Huang, Y., Liang, D., Paisley, J., 2019a. A 3d spatially weighted \nnetwork for segmentation of brain tissue from mri. IEEE Trans. Med. Imaging 39 (4), \n898\u2013909.\nSun, L., Shao, W., Zhang, D., Liu, M., 2019b. Anatomical attention guided deep networks \nfor roi segmentation of brain mr images. IEEE Trans. Med. Imaging 39 (6), \n2000\u20132012.\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, \nand A. Rabinovich.Going deeper with convolutions.In: Proceedings of the IEEE \nconference on computer vision and pattern recognition, 1-9, 2015.\nC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.Rethinking the inception \narchitecture for computer vision.In: Proceedings of the IEEE conference on computer \nvision and pattern recognition, 2818-2826, 2016.\nTalukder, M.A., Islam, M.M., Uddin, M.A., Akhter, A., Hasan, K.F., Moni, M.A., 2022. \nMachine learning-based lung and colon cancer detection using deep feature \nextraction and ensemble learning. Expert Syst. Appl. 205, 117695.\nTeoh, J.R., Hasikin, K., Lai, K.W., Wu, X., Li, C., 2024. Enhancing early breast cancer \ndiagnosis through automated microcalcification detection using an optimized \nensemble deep learning framework. PeerJ Comput. Sci. 10, e2082.\nThanh, D.N.H., Prasath, V.B.S., Hieu, L.M., Hien, N.N., 2020. Melanoma skin cancer \ndetection method based on adaptive principal curvature, colour normalisation and \nfeature extraction with the abcd rule. J. Digit. Imaging 33 (3), 574\u2013585.\nTolkach, Y., Ovtcharov, V., Pryalukhin, A., Eich, M.-L., Gaisa, N.T., Braun, M., \nRadzhabov, A., Quaas, A., Hammerer, P., Dellmann, A., et al., 2023. An international \nmulti-institutional validation study of the algorithm for prostate cancer detection \nand gleason grading. NPJ Precis. Oncol. 7 (1), 77.\nToma, T.A., Biswas, S., Miah, M.S., Alibakhshikenari, M., Virdee, B.S., Fernando, S., \nRahman, M.H., Ali, S.M., Arpanaei, F., Hossain, M.A., et al., 2023. Breast cancer \ndetection based on simplified deep learning technique with histopathological image \nusing breakhis database. Radio Sci. 58 (11), 1\u201318.\nVan Ginneken, B., Armato III, S.G., de Hoop, B., van Amelsvoort-van de Vorst, S., \nDuindam, T., Niemeijer, M., Murphy, K., Schilham, A., Retico, A., Fantacci, M.E., \net al., 2010. Comparing and combining algorithms for computer-aided detection of \npulmonary nodules in computed tomography scans: the anode09 study. Med. Image \nAnal. 14 (6), 707\u2013722.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., \nPolosukhin, I., 2017. Attention is all you need. Adv. Neural Inf. Process. Syst. 30.\nWang, S., Hamian, M., 2021. Skin cancer detection based on extreme learning machine \nand a developed version of thermal exchange optimization. Comput. Intell. Neurosci. \n2021 (1), 9528664.\nWang, Y., Wang, N., Xu, M., Yu, J., Qin, C., Luo, X., Yang, X., Wang, T., Li, A., Ni, D., \n2020. Deeply-supervised networks with threshold loss for cancer detection in \nautomated breast ultrasound. IEEE Trans. Med. Imaging 39 (4), 866\u2013876.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n23 \n\nWang, J., Qiao, L., Zhou, S., Zhou, J., Wang, J., Li, J., Ying, S., Chang, C., Shi, J., 2024. \nWeakly supervised lesion detection and diagnosis for breast cancers with partially \nannotated ultrasound images, 1-1 IEEE Trans. Med. Imaging, 1-1. \nWei, L., Ding, K., Hu, H., 2020. Automatic skin cancer detection in dermoscopy images \nbased on ensemble lightweight deep learning network. IEEE Access 8, 99633\u201399647.\nWelikala, R.A., Remagnino, P., Lim, J.H., Chan, C.S., Rajendran, S., Kallarakkal, T.G., \nZain, R.B., Jayasinghe, R.D., Rimal, J., Kerr, A.R., et al., 2020. Automated detection \nand classification of oral lesions using deep learning for early detection of oral \ncancer. IEEE Access 8, 132677\u2013132693.\nWilson, P.F.R., Gilany, M., Jamzad, A., Fooladgar, F., Nhat To, M.N., Wodlinger, B., \nAbolmaesumi, P., Mousavi, P., 2023. Self-supervised learning with limited labeled \ndata for prostate cancer detection in high-frequency ultrasound. IEEE Trans. \nUltrason., Ferroelectr., Freq. Control 70 (9), 1073\u20131083.\nXue, Y., Li, N., Wei, X., Wan, R.A., Wang, C., 2020. Deep learning-based earlier detection \nof esophageal cancer using improved empirical wavelet transform from endoscopic \nimage. IEEE Access 8, 123765\u2013123772.\nYan, R., Ren, F., Wang, Z., Wang, L., Zhang, T., Liu, Y., Rao, X., Zheng, C., Zhang, F., \n2020. Breast cancer histopathological image classification using a hybrid deep \nneural network. Methods 173, 52\u201360.\nYan, Y.-J., Cheng, N.-L., Jan, C.-I., Tsai, M.-H., Chiou, J.-C., Ou-Yang, M., 2021. Band- \nselection of a portal led-induced autofluorescence multispectral imager to improve \noral cancer detection. Sensors 21 (9), 3219.\nYaqoob, A., Aziz, R.M., Verma, N.K., 2023a. Applications and techniques of machine \nlearning in cancer classification: A systematic review. Hum. -Centr Intell. Syst. 3 (4), \n588\u2013615.\nYaqoob, A., Aziz, R.M., Verma, N.K., Lalwani, P., Makrariya, A., Kumar, P., 2023b. \nA review on nature-inspired algorithms for cancer disease prediction and \nclassification. Mathematics 11 (5), 1081.\nS. Zagoruyko and N. Komodakis.Wide residual networks.arXiv preprint arXiv: \n1605.07146, 2016.\nZeng, R., Qu, B., Liu, W., Li, J., Li, H., Bing, P., Duan, S., Zhu, L., 2024. Fastleakyresnet- \ncir: A novel deep learning framework for breast cancer detection and classification. \nIEEE Access 12, 70825\u201370832.\nZheng, J., Lin, D., Gao, Z., Wang, S., He, M., Fan, J., 2020. Deep learning assisted \nefficient adaboost algorithm for breast cancer detection and early diagnosis. IEEE \nAccess 8, 96946\u201396954.\nZolfaghari, B., Mirsadeghi, L., Bibak, K., Kavousi, K., 2023. Cancer prognosis and \ndiagnosis methods based on ensemble learning. ACM Comput. Surv. 55 (12), 1\u201334.\nI. Ahmad and F. Alqurashi                                                                                                                                                                                                                   \nCritical Reviews in Oncology \/ Hematology 204 (2024) 104528 \n24 \n",
                  "abstract":"Cancer, characterized by the uncontrolled division of abnormal cells that harm body tissues, necessitates early detection for effective treatment. Medical imaging is crucial for identifying various cancers, yet its manual interpretation by radiologists is often subjective, labour-intensive, and time-consuming. Consequently, there is a critical need for an automated decision-making process to enhance cancer detection and diagnosis. Previously, a lot of work was done on surveys of different cancer detection methods, and most of them were focused on specific cancers and limited techniques. This study presents a comprehensive survey of cancer detection methods. It entails a review of 99 research articles collected from the Web of Science, IEEE, and Scopus databases, published between 2020 and 2024. The scope of the study encompasses 12 types of cancer, including breast, cervical, ovarian, prostate, esophageal, liver, pancreatic, colon, lung, oral, brain, and skin cancers. This study discusses different cancer detection techniques, including medical imaging data, image preprocessing, segmentation, feature extraction, deep learning and transfer learning methods, and evaluation metrics. Eventually, we summarised the datasets and techniques with research challenges and limitations. Finally, we provide future directions for enhancing cancer detection techniques.",
                  "cita":"Istiak Ahmad and Fahad Alqurashi (2024). Early cancer detection using deep learning and medical imaging: A survey.",
                  "palabra_clave":"survey",
                  "codigo":"Istiak_2024"
         }
]