% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
\titlerunning{Universidad Politecnica Salesiana}

%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%รยบ
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{hyperref}
\usepackage{float}
\usepackage{multirow}
\usepackage{cite}
\usepackage{breqn}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage{subfigure}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage{tabularx} % Importa el paquete
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb} 
\usepackage{array}
\usepackage{algpseudocode}
\usepackage{blindtext}
\usepackage{color}
\usepackage{algorithm}
\usepackage{epstopdf}
\usepackage{placeins}
\usepackage{wrapfig}
\usepackage{graphicx} % Necesario para \includegraphics
\usepackage{enumitem}

\restylefloat{algorithm}
\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}
\newcounter{mytempeqncnt}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
	
\begin{document}
%
\title{Survey of lda}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Remigio Hurtado\inst{1}}
%
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Universidad Polit\'ecnica Salesiana, Calle Vieja 12-30 y Elia Liut, Cuenca, Ecuador. \email{rhurtadoo@ups.edu.ec}\\
\url{ups.edu.ec}}
%
\maketitle    
%
\begin{abstract}
This article presents a comprehensive overview of the integration of machine learning (ML) and deep learning (DL) techniques in brain tumor histopathology, focusing on glioblastoma (GBM). Our methodology ensures a systematic analysis of scientific literature, beginning with meticulous data preparation using advanced natural language processing techniques. We employ Latent Dirichlet Allocation (LDA) to identify and categorize underlying topics, which leads to the development of a recommendation system that suggests relevant articles. By creating various knowledge representations, including textual summaries and visualizations, we synthesize the main findings into an accessible format, culminating in a user	extendash{}friendly report.

The systematic review of 54 studies reveals that while ML/DL methods are increasingly utilized for classifying and segmenting tumor types, the integration of biological data with histopathological findings remains limited. Many studies lack rigorous reporting standards, raising concerns about reproducibility. The review underscores the predominance of support vector machines and convolutional neural networks in GBM research but highlights a gap in studies that effectively combine histological and molecular data. This integration is essential for advancing understanding of GBM pathogenesis and improving patient outcomes. In conclusion, while ML/DL approaches show promise in this field, future research should prioritize methodological rigor and interdisciplinary collaboration to develop comprehensive models that enhance diagnostic accuracy and treatment strategies for GBM.
\end{abstract}

\begin{keywords}
machine learning, glioblastoma, deep learning, histopathology, cancer detection
\end{keywords}

\section{Introduction}
The application of machine learning (ML) and deep learning (DL) in various domains has revolutionized the way complex problems are approached, particularly in the fields of medicine and biomedical research. One area where these technologies have shown remarkable promise is in understanding and diagnosing brain tumors, specifically glioblastoma (GBM). GBM is notorious for its aggressive behavior and heterogeneity, making it a challenging target for traditional diagnostic methods. The integration of advanced computational techniques offers a new frontier for improving diagnostic accuracy and treatment strategies, underscoring the importance of exploring these methodologies in depth.

As the prevalence of brain tumors continues to rise, there is an urgent need for innovative solutions that enhance diagnostic capabilities. Traditional diagnostic approaches heavily rely on the expertise of pathologists and radiologists, which can be time	extendash{}consuming and prone to human error. This reality presents a compelling motivation for the adoption of automated systems that leverage ML and DL for more accurate and efficient tumor classification and segmentation. By using algorithms that can analyze vast amounts of data, researchers can uncover complex patterns and relationships that human observers might overlook, thus improving the overall understanding of GBM and its implications for patient outcomes.

Despite the promising advancements, the implementation of ML and DL in GBM research is still in its nascent stages, revealing a significant gap in the literature. A systematic review of studies indicates that while numerous applications of these techniques exist, many focus solely on the classification and segmentation of tumor types based on histopathological data. The majority of the research has not adequately integrated genomic and clinical data, which are crucial for enhancing the understanding of tumor behavior and patient outcomes. This lack of interdisciplinary approaches highlights the need for comprehensive methodologies that can bridge the gap between various data types, allowing for a holistic view of GBM pathogenesis.

Addressing these challenges is imperative, as the integration of multiple data sources can lead to more personalized and effective treatment plans for GBM patients. For example, studies have shown that the presence of specific molecular markers can significantly influence tumor classification and prognosis when combined with histopathological features. However, many current studies fall short of reporting their training and evaluation methodologies in detail, which raises concerns about the reproducibility and robustness of the developed models. Improving transparency in data reporting and methodology is essential for fostering trust in these models and ensuring that they can be effectively utilized in clinical settings.

To tackle the identified problems, recent investigations have proposed several methodologies that leverage ML and DL techniques. Support Vector Machines (SVM) and convolutional neural networks (CNN) have emerged as popular choices for classification and segmentation tasks, particularly in the context of histopathological images. These models have demonstrated the ability to extract meaningful features from complex data, leading to improved accuracy in predicting tumor grades and patient survival outcomes. Furthermore, hybrid approaches that combine traditional machine learning methods with deep learning architectures have shown promise in enhancing classification performance and robustness.

Our methodology for conducting a comprehensive survey of the literature surrounding ML and DL applications in GBM is designed to ensure a systematic analysis of scientific literature. It begins with the careful preparation of text data, utilizing advanced natural language processing (NLP) techniques to clean and refine the information. This initial step is crucial for ensuring high	extendash{}quality data that is ready for detailed analysis. Once the data has been prepared, we employ Latent Dirichlet Allocation (LDA) to identify and categorize the underlying topics within the text, allowing us to uncover hidden thematic structures. This process not only aids in the organization of the literature but also facilitates the development of a recommendation system that suggests the most relevant articles, enhancing the coherence of the survey.

Moreover, the creation of various knowledge representations is integral to synthesizing the main findings of our survey. This includes generating textual summaries and visualizing relationships between topics and documents. By employing the GPT API to generate explanatory text, we ensure that the findings are presented in an accessible format, ultimately leading to a comprehensive report that combines text and figures for a complete overview of the analysis. The final product reflects a robust and automated approach to creating survey articles, aiming to bridge the existing gaps in GBM research.

In conclusion, while the application of ML and DL techniques in GBM histopathology presents significant potential, much more work is needed to fully realize their capabilities. The integration of diverse data types and improved transparency in research methodologies are critical steps toward enhancing diagnostic and prognostic tools. By fostering interdisciplinary collaborations and standardizing practices, the field can advance toward more effective and personalized treatment strategies for GBM patients, ultimately improving survival rates and quality of life. As highlighted in our survey, the future of GBM research lies in the successful amalgamation of computational techniques with biological insights, paving the way for breakthroughs in understanding this complex disease.

The key contributions of our study can be summarized as follows: 

\begin{itemize}
    \item \textbf{Comprehensive Review of the State of the Art}: Detailed analysis and synthesis of the most relevant and recent contributions in the existing literature.
    \item \textbf{Structured Methodology Based on CRISP	extendash{}DM}: Application of a standard methodology to guide the data mining process, including phases from problem understanding to knowledge representation generation.
    \item \textbf{Advanced Machine Learning Models}: Use of Latent Dirichlet Allocation (LDA) to identify and categorize latent topics and development of a recommendation system based on the document	extendash{}topic matrix, in order to identify the most relevant themes and the most important associated documents.
    \item \textbf{Synthesis of Knowledge Generation}: Creation of textual summaries and visualization of relationships between topics and documents to enhance understanding.
    \item \textbf{Comprehensive and Understandable Reports}: Integration of representations in detailed reports that combine text and figures to provide a clear and complete overview of the analysis.
\end{itemize}

In the remainder of the document, the following is presented: The development methodology of this study, then, the evaluation in the field, the topics: Automated Tumor Classification, DCB UNet Segmentation, Deep Learning Glioblastoma,  subsequently, the trends in lda, related works and original contributions, and finally, the conclusions.
\section{Methodology}

Our methodology is designed to ensure a comprehensive and systematic analysis of scientific literature. The methodology used in this study builds on our previous work, including a methodology that leverages machine learning and natural language processing techniques \cite{Hurtado2023}, and a novel method for predicting the importance of scientific articles on topics of interest using natural language processing and recurrent neural networks \cite{Lopez2024}. The process is structured into three phases: data preparation, topic modeling, and the generation and integration of knowledge representations. Each phase is essential for transforming raw text data into meaningful insights, and the detailed parameters and algorithm are explained below. The table \ref{tabDescription} describes the parameters for understanding the overall process and algorithm. The high-level process is presented in Fig. \ref{fig:Methodology}, and the detailed algorithm is outlined in Table \ref{tab:Algorithm}.\\ 

In the data preparation phase, we focus on extracting and cleaning text from scientific documents using natural language processing (NLP) techniques. This phase involves several steps to ensure that the text data is ready for analysis. First, text is extracted from the documents, and non-alphabetic characters that do not add value to the analysis are removed. Next, the text is converted to lowercase, and stopwords (common words that do not contribute much meaning) are removed. We then apply lemmatization, which transforms words to their base form (e.g., "running" becomes "run"). Each document is tokenized (split into individual words or terms), and n-grams (combinations of words) are identified to find common terms. We generate a unified set of common terms, denoted as $TE$, which includes both the terms extracted from the documents and basic terms relevant to any field of study, such as [Fundamentals, Evaluation of Solutions, Trends]. Finally, each document is vectorized with respect to $TE$, resulting in the Document-Term Matrix (DTM).\\

The DTM is a crucial component for topic modeling. It is a matrix where the rows represent the documents in the corpus, and the columns represent the terms (words or n-grams) extracted from the corpus. Each cell in the matrix contains a value indicating the presence or frequency of a term in a document. This structured representation of the text data allows us to apply machine learning techniques to uncover hidden patterns.\\

In the topic modeling phase, we use Latent Dirichlet Allocation (LDA), a popular machine learning technique for identifying topics within a set of documents. By applying LDA to the DTM, we transform the matrix into a space of topics. Specifically, LDA provides us with two key matrices: the Topic-Term Matrix ($\beta$) and the Document-Topic Matrix ($\theta$). The Topic-Term Matrix ($\beta$) indicates the probability that a term is associated with a specific topic, while the Document-Topic Matrix ($\theta$) indicates the probability that a document belongs to a specific topic.

To enhance this approach, we integrate predefined topic representations and refine document-topic assignments. In addition to extracting topics purely from the document-term matrix (DTM), we incorporate a set of predefined topics, namely [Fundamentals, Evaluation of Solutions, Trends], which are represented using a predefined set of keywords generated by a language generation model. This ensures that the model captures both the inherent structure of the dataset and domain-relevant themes. The predefined topics are processed through a keyword extraction function, which selects the most relevant words associated with each topic based on the provided corpus.

To construct a more robust topic representation, we create predefined topic matrices that integrate predefined topic vectors with the most relevant keywords extracted for each topic. These matrices are then normalized and combined with the LDA-generated topic distribution, ensuring a refined alignment of document-topic assignments. By doing so, we mitigate the limitations of purely unsupervised topic modeling, which might generate topics that lack semantic clarity.

Finally, to improve interpretability, we assign meaningful names to the discovered topics by combining the highest-probability terms from the topic-term matrix ($\beta$). Using a language generation model, we generate concise and descriptive topic names, ensuring that each topic is easily understandable. This process results in a refined set of topics ($T$), the most relevant terms ($K$), and a topic-term graph ($G_{tk}$) that illustrates relationships between topics and key terms. This methodology enhances the effectiveness of topic modeling by integrating both machine learning and domain-specific knowledge, leading to a more structured and meaningful representation of the analyzed corpus.

The final phase involves the generation and integration of knowledge representations, which include summaries, keywords, and interactive visualizations. For each topic $t$ in $T$, we identify the $N$ most relevant documentsรขโฌโthose with the highest probabilities in $\theta$. This set, $D_t$, represents the documents most closely related to each topic. We then generate summaries of these documents, each with a maximum of $W_b$ words, using a language generation model. These summaries include references to the most relevant documents, which are added to the set $R_d$ if they are not already included. We also integrate all the summaries and generate $J$ suggested keywords using a language generation model. Additionally, we create interactive graphs from the $G_{tk}$ graph, showcasing nodes and relationships between the topics and terms, and highlighting significant connections.We also integrate all the summaries and generate $J$ suggested keywords using a language generation model. Additionally, the topic titles were improved based on the generated summaries using a language generation model, ensuring that the final topic names more accurately reflect the summarized content.
\\

This methodology provides a clear, structured approach to analyzing scientific literature, leveraging advanced NLP and machine learning techniques to generate useful and comprehensible knowledge representations. This is the methodology used in the development of this study, which presents the fundamentals, solution evaluation techniques, trends, and other topics of interest within this field of study. This structured approach ensures a thorough review and synthesis of the current state of knowledge, providing valuable insights and a solid foundation for future research.

\begin{table*}[!h]
	\caption{\centering Description of parameters of the proposed methodology}
	\resizebox{\textwidth}{!}{ % Ajusta el ancho de la tabla al texto
		\begin{tabular}{|l|l|}
			\hline
			\textbf{Parameter} & \textbf{Description}                                                            \\ \hline
			$T$                  & Set of topics to be generated with the LDA model. \\ \hline
			$\#T$                & Cardinality of $T$. That is, the number of topics (dimensions).                                    \\ \hline
			$K$                  & Set of common terms with the highest probability in topic $t$ used to label that topic. \\ \hline
			$\#K$ 				 & Cardinality of $K$.                                         \\ \hline
			$W_{t}$   			 & Maximum number of words for combining the common terms of topic $t$ into a new topic name.\\ \hline
			$N$                  & Number of the most relevant documents to generate a summary of topic $t$.\\ \hline
			$W_b$                & Maximum number of words to generate a summary of the list of $N$ most related documents to topic $t$.\\ \hline
			$J$                  & Number of suggested keywords to be generated as knowledge representation.\\ \hline
			$W_k$ 				 & Number of keywords selected by language generation model from the LDA-generated words to construct the predefined matrix. \\ \hline
		\end{tabular}
	}
	\label{tabDescription}
\end{table*}


\begin{figure*}[!h]
	\centering
	\includegraphics[width=1.0\textwidth]{Figures/method.png}
	\caption{Methodology for the generation of knowledge representations}
	\label{fig:Methodology}
\end{figure*}

%Actual
\begin{figure*}[!h]
	\centering
	\resizebox{\textwidth}{!}{ % Ajusta el ancho de la tabla a \textwidth
		\begin{tabular}{l}
			\hline
			\textbf{General Algorithm} \\
			\hline
			\textbf{Input:} Field of study, Scientific articles collected from virtual libraries, $\#T$, $\#K$, $W_{t}$, $N$, $W_b$, $J$, $W_k$\\
			\hline
			\textbf{Phase 1: Data Preparation with Natural Language Processing (NLP)} \\
			\quad a. Extract of text from documents. \\
			\quad b. Remove non-alphabetic characters that do not add value to the analysis. \\
			\quad c. Convert to lowercase and remove stopwords. \\
			\quad d. Apply lemmatization to transform words to their base form. \\
			\quad e. In each document, tokenize and identify n-grams to identify common terms (words or n-grams). \\
			\quad f. Unified generation of common terms of all documents. Where $TE$ is the unified set of common terms.\\
			\quad g. Add in $TE$ the basic terms for any field of study, such as: [Fundamentals, Evaluation of Solutions, Trends]. \\
			\quad h. Vectorize each document with respect to $TE$ and generate Document-Term Matrix (DTM).\\
			\quad \textbf{Output:} For each document [title, original text, common terms], and Document-Term Matrix (DTM)\\
			\textbf{Phase 2: Topic Modeling whit Machine Learning} \\
			\quad \textbf{Input:} Document-Term Matrix (DTM) \\
			\quad a. Apply Latent Dirichlet Allocation (LDA) to transform the DTM Matrix into a space of $\#T$ topics (dimensions).\\
			\quad b. Obtain the Topic-Term Matrix ($\beta$) that indicates the probability that a term is generated by a specific topic.\\ 
			\quad c. Obtain the Document-Topic Matrix ($\theta$) that indicates the probability that a document belongs to a specific topic.\\
			\quad d. Generate predefined topic representations using language generation model by sending the LDA-generated words and selecting $W_k$ keywords for each predefined topic.\\
			\quad e. Construct predefined matrices by combining predefined topic vectors with the $W_k$ selected keywords.\\
			\quad f. Normalize and combine the predefined matrices with the LDA-generated topic distribution to refine document-topic assignments, resulting in the new Matrix ($\beta$).\\
			\quad g. For each unknown $t$ topic in $\beta$, assign a name or label to the $t$ topic by combining the $\#K$ highest probability\\
			\quad \quad common terms in $\beta$ associated with that $t$ topic. This generates: \\
			\quad \quad The $T$ Set with the $\#T$ most relevant topics. \\
			\quad \quad The $K$ Set with the $\#K$ most relevant terms. \\
			\quad \quad The $G_{tk}$ Graph of the relationships between the most relevant topics ($T$) and the most relevant terms ($K$). \\
			\quad h. For each topic $t$ in $T$, modify topic $t$ by combining the common terms of $t$ into a new topic name with at most \\
			\quad \quad $W_{t}$ words using a language generation model.\\
			\quad \textbf{Output:} Relevant Topics ($T$), Topic-Term Matrix ($\beta$), Document-Topic Matrix ($\theta$) \\
			\textbf{Phase 3: Generation and Integration of Knowledge Representations} \\
			\quad \textbf{Input:} Relevant Topics $T$, Document-Topic Matrix ($\theta$) \\
			\quad \textbf{3.1: Knowledge representations through summaries and keywords}\\
			\quad \quad a. For each $t$ topic in $T$, obtain its $N$ most relevant documents, i.e., those with the highest probabilities in $\theta$. \\
			\quad \quad \quad Thus, $D_t$ represents the set of documents most related to each topic $t$.\\
			\quad \quad b. For each topic $t$ in $T$, and from $D_t$ generate a summary of the list of documents most related to that topic $t$ with \\
			\quad \quad \quad at most $W_b$ words using a language generation model. \\
			\quad \quad c. Incorporate into the summary the text citing references to the most relevant documents. Add these references to the \\ 
			\quad \quad \quad set $R_d$, which will contain all cited references, including new references if they have not been previously included.\\
			\quad \quad d. Integrate all the summaries and from them generate $J$ suggested keywords using a language generation model.\\
			\quad \quad e. Improve topic titles with a language generation model.\\
			\quad \textbf{3.2: Knowledge representations through knowledge visualizations with interactive graphs}\\
			\quad \quad a. From the $G_{tk}$ graph, generate an interactive graph with nodes and relationships between the topics $T$\\
			\quad \quad and the $K$ most relevant terms.\\
			\quad \quad b. Highlight the connections between the topics $T$ and the $K$ most relevant terms.\\
			\quad \textbf{3.3: Integration of Knowledge Representations}\\
			\hline
			\textbf{Output:} Knowledge Representations \\
			\hline
		\end{tabular}
	}
	\caption{\centering General algorithm of the methodology incorporating natural language processing, machine learning techniques and language generation models}
	\label{tab:Algorithm}
\end{figure*}

\FloatBarrier


\section{Fundamental of lda}
Machine learning (ML) and deep learning (DL) techniques have shown significant potential in advancing brain tumor histopathology research, particularly for glioblastoma (GBM), which is known for its complex pathogenesis and heterogeneity. Despite the promising applications of ML/DL in various medical fields, their implementation in GBM histopathological studies has been comparatively limited. A systematic review of existing literature reveals that a majority of studies have focused on the classification and segmentation of tumor types using ML/DL methods based on histopathological data, often correlating these findings with genomic and clinical data to enhance understanding of tumor behavior and patient outcomes. 

Recent investigations indicate a growing trend towards integrating bioinformatics with histopathological analysis, emphasizing the necessity of using a multi-omics approach to improve diagnostic accuracy and treatment strategies for GBM. For instance, several studies have illustrated that the presence of specific molecular markers can significantly influence tumor classification and prognosis when combined with histopathological data. However, many of these studies have not adequately reported their training and evaluation methodologies, raising concerns about reproducibility and the robustness of the models developed. 

The review highlights that the most commonly used ML/DL models in GBM-related research are support vector machines (SVM) and convolutional neural networks (CNN), specifically those based on architectures like ResNet and U-Net. These models have been employed to extract features from histopathological images and have shown promise in predicting tumor grades and patient survival outcomes. Nonetheless, there remains a significant gap in literature concerning the comprehensive integration of histopathological data with other omics data types. Only a small subset of studies has effectively utilized this integration to uncover deeper biological insights into GBM pathogenesis.

Moreover, the systematic review indicates that while there is a robust interest in the application of ML/DL techniques, the majority of studies tend to focus on enhancing model performance metrics without sufficiently addressing the underlying biological complexities of GBM. This limitation underscores the need for more interdisciplinary approaches that combine histopathological data with molecular and genomic insights. Such strategies could lead to the development of more effective and personalized treatment plans for GBM patients.

In conclusion, although ML/DL approaches are making strides in the field of GBM histopathological research, there is still much work to be done. Future studies should aim to standardize methodologies, improve data reporting practices, and focus on the integration of diverse data types to foster a more comprehensive understanding of GBM. This could ultimately lead to better diagnostic and prognostic tools that are accessible and applicable in clinical settings.
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Figures/lda_topic_graph.png}
\caption{Grafo de relaciones temรกticas generado mediante LDA, donde los nodos representan temas identificados y sus palabras clave asociadas. La palabra clave central conecta los temas, y las palabras compartidas aparecen enlazadas a mรบltiples etiquetas segรบn su relevancia en el modelo. Los pesos de los enlaces reflejan la importancia de cada tรฉrmino dentro de su tรณpico.}
\end{figure}
\section{Evaluation of lda}
The application of machine learning (ML) and deep learning (DL) techniques in the analysis of brain tumor histopathology, particularly glioblastoma (GBM), has gained significant traction recently, highlighting the potential for these technologies to enhance understanding and treatment of this complex disease. Despite the advancements in ML/DL methods, the integration of these approaches into GBM histopathological studies remains limited, suggesting a gap that could be addressed to improve diagnostic and prognostic outcomes. A systematic review of the literature reveals that while numerous studies have utilized ML/DL techniques for analyzing histopathological data, only a small subset has focused specifically on GBM, with many others exploring various brain tumors without establishing a clear correlation between biological and histopathological data.

The review analyzed 54 eligible studies, revealing that most research has concentrated on employing ML/DL for classification and segmentation tasks in brain tumor histopathology, with an emphasis on identifying tumor subtypes and predicting patient survival. Among the studies reviewed, 37 were specifically related to GBM, indicating a strong research interest in this area. However, the majority of these studies primarily relied on traditional histopathological analyses, often failing to integrate additional -omics data that could provide deeper insights into tumor biology and patient outcomes. For instance, while some studies have successfully correlated histopathological features with molecular markers, the overall trend shows a lack of comprehensive approaches that combine various data types to enhance the understanding of GBM's heterogeneity and pathogenesis \cite{Chun_2025}.

Furthermore, the review highlights a concerning trend regarding the reporting standards in ML/DL research within this field. Many studies did not adequately specify their training and evaluation methodologies, making it challenging to assess the reproducibility and robustness of the models utilized. This lack of transparency is particularly problematic given the critical implications of GBM diagnoses and the potential for ML/DL tools to significantly impact clinical decision-making. The review suggests that improved reporting standards, including clear definitions of data sources, model training processes, and evaluation metrics, would facilitate better comparisons and validation of ML/DL applications in brain tumor research \cite{Chun_2025}.

Moreover, the findings suggest a growing interest in integrating histopathological data with genomic and proteomic information to create more comprehensive predictive models. Studies that employed such integrative approaches demonstrated enhanced classification accuracy and provided valuable insights into tumor characteristics and behaviors. For example, some research has successfully utilized multiscale feature extraction and deep learning architectures to uncover complex relationships between histological features and molecular profiles, thus paving the way for personalized treatment strategies \cite{Chun_2025}.

In conclusion, while there is a burgeoning interest in the application of ML/DL methods in GBM histopathology, significant work remains to be done to fully realize the potential of these technologies. By fostering collaborations that emphasize the integration of diverse data types and improving the standardization of reporting in research, the field can move towards more robust and generalizable models that ultimately enhance patient care and outcomes. Future research should prioritize these integrative strategies to ensure that the insights gained are not only statistically significant but also clinically relevant, thereby contributing to the ongoing battle against GBM and improving survival rates for patients \cite{Chun_2025}.
\section{Automated tumor classification}
Automated brain tumor classification from MRI images has become increasingly crucial due to the high mortality rates associated with brain cancer. Traditional diagnostic methods rely heavily on the expertise of radiologists, which can be time-consuming and prone to errors, especially given the complex nature of brain tumors. Recent advancements in artificial intelligence (AI) and deep learning have presented promising solutions. This paper proposes an enhanced model using Residual Networks (ResNet) to classify various types of brain tumors, including Meningiomas, Gliomas, and Pituitary tumors, with a high degree of accuracy.

The proposed approach utilizes a benchmark dataset comprising 3064 MRI images of brain tumors. Extensive data augmentation techniques, such as horizontal and vertical flipping, rotation, shifting, zooming, ZCA whitening, shearing, and brightness manipulation, were employed to increase the dataset's diversity and improve classification accuracy. The model's architecture is designed to handle the challenges posed by the inherent complexities of MRI images, such as varied shapes and intensities of tumors. By leveraging the ResNet50 architecture, the model can manage deeper networks effectively, mitigating issues related to vanishing gradients and accuracy degradation commonly found in traditional neural networks \cite{Sarah_2020}.

The results of the proposed model are promising, achieving a classification accuracy of 99% for image-level evaluations and 97% for patient-level assessments. These results are superior to previous approaches that reported accuracies ranging from 84% to 93% \cite{Sarah_2020}. The use of ResNet not only facilitates high accuracy but also allows for better generalization across different tumor types due to its ability to learn rich feature representations from the data. Moreover, metrics such as precision, recall, and F1-score were calculated to provide a comprehensive evaluation of the model performance, addressing the limitations of relying solely on accuracy, especially in imbalanced datasets \cite{Sarah_2020}.

In comparison to earlier methodologies which required manual feature extraction and segmentation prior to classification, the deep learning approach streamlines the process, automatically learning features from raw MRI data. This not only enhances efficiency but also reduces the likelihood of manual errors that can occur during the preprocessing stages \cite{Sarah_2020}. The effectiveness of the proposed model highlights the potential of deep learning techniques in improving diagnostic accuracy and efficiency in medical imaging, particularly for challenging cases such as brain tumors.

Future work is suggested to expand on this research by exploring larger datasets and incorporating additional tumor types, which can further validate the robustness of the model. By optimizing the model for various imaging modalities, including X-rays and ultrasounds, the proposed approach could be adapted for broader applications within medical diagnostics. Overall, the integration of advanced AI methodologies in the classification of brain tumors from MRI images shows significant potential to enhance clinical decision-making processes and improve patient outcomes in the fight against cancer.
\section{Dcb unet segmentation}
Recent advancements in deep learning have significantly enhanced the automatic segmentation of brain tumors in magnetic resonance imaging (MRI). Effective segmentation plays a crucial role in the diagnosis and treatment of brain tumors, as it aids in identifying tumor boundaries and characteristics, which are essential for clinical decision-making. Traditional manual segmentation methods are not only time-consuming but also prone to human error, leading to an increased need for automated solutions. Deep learning techniques, particularly convolutional neural networks (CNNs), have emerged as a powerful alternative, offering improved accuracy and efficiency in this domain \cite{Akter_2024, Ullah_2023, Zhao_2024}.

The UNet architecture has been widely used for biomedical image segmentation, including brain tumor detection, due to its ability to capture spatial hierarchies through its encoder-decoder structure. However, conventional UNet models often struggle with feature extraction and contextual information fusion, which can negatively impact segmentation accuracy \cite{Ronneberger_2015, Lu_2022, Jena_2023}. To address these limitations, recent research has focused on integrating advanced techniques such as attention mechanisms and residual connections into the UNet framework. Such modifications have led to the development of hybrid architectures that enhance feature representation and improve segmentation performance.

One promising approach is the introduction of the Depthwise Convolution Bottleneck (DCB) within the UNet architecture. This method leverages depthwise convolutions to reduce the number of parameters while maintaining the model's ability to learn complex features. By incorporating depthwise convolutions into the skip connections, the DCB-UNet model effectively transfers high-level features from the encoder to the decoder, enhancing the overall segmentation quality \cite{Koh_2020, Daimary_2020, Havaei_2017}. The model's architecture allows for better preservation of spatial information, which is critical for accurately delineating tumor boundaries.

Experimental results demonstrate that the DCB-UNet model outperforms traditional UNet and other state-of-the-art segmentation methods. For instance, the model achieved an impressive accuracy of 99.82%, an Intersection over Union (IoU) score of 83.34%, and a Dice Similarity Coefficient (DSC) of 90.83% across various test datasets \cite{Hanine_2024, Sathish_2024, Shomirov_2022}. These metrics indicate not only the model's effectiveness in distinguishing tumor regions from healthy brain tissue but also its potential for clinical application in routine diagnostic processes.

Moreover, the integration of data augmentation strategies during training has further improved the model's robustness and generalizability. By employing techniques such as rotation, scaling, and shearing, the training dataset is effectively expanded, allowing the model to learn from a broader range of variations in tumor presentation \cite{Garcea_2023, Buda_2019, Prados_2017}. This approach mitigates the risk of overfitting and enhances the model's ability to perform well on unseen data.

As brain tumors exhibit significant variability in size, shape, and location, the DCB-UNet model's ability to maintain high accuracy across diverse cases is particularly noteworthy. The use of depthwise convolutions not only simplifies the network but also facilitates the extraction of intricate features that are often crucial for accurate segmentation. Consequently, the DCB-UNet model stands out as a robust solution for automated brain tumor segmentation, promising to advance clinical practice by providing timely and precise diagnostic information \cite{Kumar_2023, Joshi_2024, Xu_2024}.

In conclusion, the integration of depthwise convolution bottleneck blocks into the UNet architecture has yielded significant improvements in brain tumor segmentation. The DCB-UNet model demonstrates superior performance metrics, establishing it as a leading technique in the realm of medical image analysis. Future research should focus on further enhancing this model and evaluating its effectiveness across different imaging modalities and clinical scenarios to solidify its role in the diagnostic workflow \cite{Ding_2019, Karimzadeh_2021, Liu_2023}.
\section{Deep learning glioblastoma}
Deep learning (DL) and machine learning (ML) techniques have emerged as powerful tools in the realm of brain tumor histopathology, particularly in the study of glioblastoma (GBM). This systematic review analyzes the application of these advanced computational methods in understanding GBM pathology by integrating histological data with genomic and proteomic information. The increasing complexity of GBM's pathogenesis necessitates innovative approaches to enhance diagnostic and prognostic capabilities in clinical settings.

A comprehensive analysis of 54 relevant studies reveals a significant trend towards employing ML/DL methodologies to dissect the intricacies of GBM. Notably, the review indicates that while many studies focus on the classification of tumor grades and subtypes or the segmentation of tumor microenvironments, only a limited number successfully integrate biological data with histopathological findings. This integration is crucial for developing models that can predict patient outcomes based on histological features and underlying molecular characteristics \cite{20,34,41}.

The findings further highlight the predominance of certain ML/DL models in GBM research. Support Vector Machines (SVM) and k-nearest neighbors (kNN) are frequently utilized classifiers, while Convolutional Neural Networks (CNNs), particularly those based on ResNet and U-Net architectures, dominate the segmentation tasks. The review emphasizes that combining these models with ensemble techniques can yield improved classification performance and robustness in predictions \cite{33,41,42}.

Despite the advancements, several limitations persist in the current body of research. A notable concern is the lack of transparency in reporting data sources and the methodologies used for training and evaluating models. Many studies do not specify the origin of their datasets, leading to reproducibility issues and potential biases in model performance evaluations. Furthermore, a significant number of studies utilize single-source datasets, which may not adequately represent the heterogeneity observed in GBM patients across different populations \cite{32,34,36}.

The review also identifies a gap in studies that effectively utilize ML/DL techniques to uncover biological insights related to GBM pathology. While there is a growing interest in integrating H\&E-stained images with other -omics data, only a few studies have successfully contextualized these relationships to gain a deeper understanding of GBM. For instance, recent research demonstrates that combining histological features with genomic and transcriptomic data can lead to enhanced predictive capabilities for patient survival and treatment response, indicating that future investigations should prioritize this integrative approach \cite{30,34,48}.

In conclusion, the review underscores the potential of ML/DL techniques to advance our understanding of GBM histopathology, while also highlighting the need for improved reporting standards and data transparency in the field. By fostering collaborative efforts and utilizing diverse datasets, future research can lead to the development of comprehensive models that not only enhance diagnostic accuracy but also provide valuable insights into the complex biology of GBM.
\section{Trends of lda}
The advancement of deep learning and medical imaging has significantly impacted early cancer detection, enhancing diagnostic accuracy and efficiency. Cancer, marked by uncontrolled cell growth, poses serious health risks and presents various challenges in detection and treatment. Early detection is crucial, as it greatly improves the chances of successful treatment and survival rates. Medical imaging technologies, such as X-rays, CT scans, MRI, and ultrasound, are essential for identifying different types of cancer, but their manual interpretation can be subjective and time-consuming. Therefore, there is a pressing need for automated systems to assist healthcare professionals in diagnosing cancer more effectively \cite{Istiak_2024}.

This survey provides a comprehensive overview of cancer detection methods, examining 99 research articles published between 2020 and 2024. The scope of the study encompasses 12 cancer types, including breast, cervical, ovarian, prostate, esophageal, liver, pancreatic, colon, lung, oral, brain, and skin cancers. Various techniques are discussed, including medical imaging data acquisition, image preprocessing, segmentation, feature extraction, and deep learning methodologies. The integration of deep learning algorithms into the cancer detection process has shown promising results, as these algorithms can efficiently analyze large datasets and recognize complex patterns that human observers might overlook \cite{Istiak_2024}.

One of the primary challenges in cancer detection is the imbalance in datasets, where non-cancerous images often outnumber cancerous ones. This imbalance can lead to biased models that fail to adequately address minority classes, resulting in a higher false negative rate \cite{Istiak_2024}. Additionally, accurate labeling of medical images is critical, yet it can be hindered by inter-observer variability, necessitating expert radiologists for annotation. Furthermore, the limited availability of high-quality medical images due to privacy concerns and the high costs associated with obtaining and labeling data poses significant obstacles in the development of robust detection systems.

Deep learning has emerged as a powerful tool in medical imaging, with convolutional neural networks (CNNs) being the most commonly used architecture for cancer detection. CNNs excel at extracting meaningful features from images, enabling precise identification of tumors and other abnormalities. Techniques such as transfer learning, where models pre-trained on large datasets are fine-tuned on specific cancer datasets, have proven effective in improving performance despite limited available data \cite{Istiak_2024}. Hybrid models that integrate traditional machine learning approaches with deep learning architectures are also being explored to enhance interpretability and performance.

Segmentation is another critical aspect of cancer detection, as it involves accurately delineating tumors from healthy tissue. Advanced techniques such as U-Net and region-based CNNs have been employed to improve segmentation accuracy, allowing for better visualization of tumor boundaries and characteristics \cite{Istiak_2024}. Moreover, evaluation metrics such as accuracy, precision, recall, F1-score, and AUC-ROC are essential for assessing the performance of detection models, ensuring that they provide reliable diagnostic information.

The findings from this survey highlight the importance of addressing data quality and quantity issues, enhancing model generalization, and improving interpretability in cancer detection systems. Future research should focus on developing robust models that can handle diverse datasets, integrating multiple imaging modalities, and ensuring ethical considerations are met in the use of AI in healthcare. By leveraging the advancements in deep learning and medical imaging, the goal is to create accurate and reliable cancer detection systems that can ultimately lead to better patient outcomes \cite{Istiak_2024}.
\section{Related works and original contributions of the paper}
The article by Vong et al. (2025) provides a systematic review of the application of machine learning (ML) and deep learning (DL) techniques in brain tumor histopathology, with a specific focus on glioblastoma (GBM). It highlights the limited yet growing trend in utilizing these methodologies to explore the pathogenesis of GBM by correlating histopathological data with omics data. The review identifies common ML/DL models used in GBM research, such as SVM classifiers and ResNet-based CNNs, and emphasizes the need for more rigorous reporting standards regarding training and evaluation methodologies. The authors call for future efforts to enhance the integration of diverse data types to generate deeper biological insights into GBM, thus advancing the field and improving clinical outcomes \cite{Chun_2025}.

In the comprehensive survey presented by Ahmad and Alqurashi (2024), the authors analyze the evolving landscape of early cancer detection using deep learning and medical imaging. Covering a range of cancer types, the study synthesizes insights from 99 research articles published between 2020 and 2024, outlining various cancer detection methodologies, including medical imaging data acquisition, preprocessing, segmentation, and deep learning techniques. The work underscores the significance of automated decision-making processes to enhance cancer diagnosis, address dataset imbalances, and overcome challenges related to manual image interpretation. The authors stress the potential of advanced deep learning algorithms, such as CNNs and hybrid models, to improve diagnostic accuracy and efficiency in cancer detection, while proposing future directions for research aimed at developing robust models for diverse patient populations \cite{Istiak_2024}.

The synthesis of findings from these articles provides valuable insights into the current landscape of ML/DL applications in medical imaging and histopathology. Both reviews highlight the transformative potential of these technologies in enhancing diagnostic accuracy and patient outcomes in brain tumors and other cancers. They emphasize the pressing need for improved methodological transparency, integration of diverse data types, and novel approaches in model development to tackle the inherent complexities associated with cancer pathogenesis. This collaborative framework can pave the way for future research that aims to standardize practices and enhance the robustness of predictive models, ultimately contributing to more personalized and effective treatment strategies across oncology.

\begin{itemize}[label=\textbullet]
    \item User a machine learning-based methodology to select the most relevant papers, standing out the latest and most cited papers in the area.
    \item Provides a comprehensive survey on the integration of machine learning techniques in glioblastoma histopathology, highlighting gaps in data reporting and the need for interdisciplinary approaches.
    \item Fundamentals of Brain Cancer Deep Learning highlights the potential of machine learning in glioblastoma histopathology, emphasizing the need for integrating molecular data to improve diagnostics and treatment outcomes.
    \item Evaluation of Brain Tumor Histopathology underscores the limited integration of machine learning techniques in glioblastoma studies, stressing the importance of robust methodologies and transparency for reproducible results.
    \item Automated Tumor Classification emphasizes the effectiveness of deep learning models like ResNet in classifying various brain tumors from MRI images, significantly enhancing diagnostic accuracy and efficiency.
    \item DCB UNet Segmentation presents advances in automated brain tumor segmentation using depthwise convolution, achieving high accuracy and improving clinical decision-making through enhanced delineation of tumor boundaries.
    \item Deep Learning Glioblastoma explores the integration of histological and molecular data in glioblastoma research, revealing gaps in current studies and the need for interdisciplinary approaches to enhance understanding.
    \item Trends in Cancer Detection highlights the impact of deep learning on improving accuracy in cancer diagnostics, addressing challenges in data imbalance and emphasizing the importance of robust model development.
\end{itemize}
\section{Conclusions}
The synthesis of findings from recent literature underscores the transformative potential of machine learning (ML) and deep learning (DL) techniques in enhancing the understanding and treatment of brain tumors, particularly glioblastoma (GBM). The comprehensive surveys highlight significant gaps in the integration of histopathological data with molecular and genomic insights, emphasizing the necessity for more interdisciplinary approaches to enrich diagnostic and prognostic capabilities. Automated tumor classification methods, particularly those utilizing advanced architectures like ResNet, have demonstrated marked improvements in diagnostic accuracy and efficiency. Furthermore, innovative segmentation techniques, such as the depthwise convolution bottleneck in UNet, reveal promising advancements in delineating tumor boundaries, which are crucial for clinical decision	extendash{}making. Despite these advancements, the reviews call attention to the need for improved transparency in data reporting and methodology standardization, ensuring reproducibility and robustness in research outcomes. As the field evolves, future work must prioritize the integration of diverse data types and the development of scalable models to address the complexities of cancer pathogenesis. By fostering collaboration across disciplines, the ultimate goal remains to enhance patient care and treatment strategies, paving the way for more personalized approaches in oncology.
\begin{thebibliography}{00}
    \bibitem{Chun_2025} Chun Kiet Vong and Alan Wang and Mike Dragunow and Thomas I-H. Park and Vickie Shim (2025). Brain tumour histopathology through the lens of deep learning: A systematic review.
\bibitem{Sarah_2020} Sarah Ali {Abdelaziz Ismael} and Ammar Mohammed and Hesham Hefny (2020). An enhanced deep learning approach for brain cancer MRI images classification using residual networks.
\bibitem{Amran_2024} Amran Hossain and Rafiqul Islam and Mohammad Tariqul Islam and Phumin Kirawanich and Mohamed S. Soliman (2024). FT-FEDTL: A fine-tuned feature-extracted deep transfer learning model for multi-class microwave-based brain tumor classification.
\bibitem{Istiak_2024} Istiak Ahmad and Fahad Alqurashi (2024). Early cancer detection using deep learning and medical imaging: A survey.
\bibitem{b1}
    Hurtado, Remigio; Picรยณn, Cristian; Muรยฑoz, Arantxa; Hurtado, Juan.
    "Survey of Intent-Based Networks and a Methodology Based on Machine Learning and Natural Language Processing."
    In Proceedings of Eighth International Congress on Information and Communication Technology.
    Springer Nature Singapore, Singapore, 2024.
    \bibitem{b2}
    Park, Keunheung; Kim, Jinmi; Lee, Jiwoong. 
    ``Visual Field Prediction using Recurrent Neural Network,'' 
    \emph{Scientific Reports}, 
    vol. 9, no. 1, p. 8385, 
    2019, 
    https://doi.org/10.1038/s41598-019-44852-6.
    
    \bibitem{b3}
    Xu, M.; Du, J.; Guan, Z.; Xue, Z.; Kou, F.; Shi, L.; Xu, X.; Li, A. 
    ``A Multi-RNN Research Topic Prediction Model Based on Spatial Attention and Semantic Consistency-Based Scientific Influence Modeling,'' 
    \emph{Comput Intell Neurosci}, 
    vol. 2021, 
    2021, 
    p. 1766743, 
    doi: 10.1155/2021/1766743.
    
    \bibitem{b4}
    Kreutz, Christin; Schenkel, Ralf.
    ``Scientific Paper Recommendation Systems: a Literature Review of recent Publications,''
    2022/01/03.
	\bibitem{Hurtado2023} Hurtado, R., et al. "Survey of Intent-Based Networks and a Methodology Based on Machine Learning and Natural Language Processing." International Congress on Information and Communication Technology. Singapore: Springer Nature Singapore, 2023.
    \bibitem{Lopez2024} Lopez, A., Dutan, D., Hurtado, R. "A New Method for Predicting the Importance of Scientific Articles on Topics of Interest Using Natural Language Processing and Recurrent Neural Networks." In: Yang, X.S., Sherratt, S., Dey, N., Joshi, A. (eds) Proceedings of Ninth International Congress on Information and Communication Technology. ICICT 2024 2024. Lecture Notes in Networks and Systems, vol 1013. Springer, Singapore. https://doi.org/10.1007/978-981-97-3559-4\_50.
\end{thebibliography}
\end{document}