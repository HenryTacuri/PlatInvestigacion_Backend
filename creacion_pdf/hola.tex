% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
\titlerunning{Universidad Politecnica Salesiana}

%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%Âº
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

%
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{float}
\usepackage{multirow}
\usepackage{cite}
\usepackage{breqn}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage{subfigure}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage{tabularx} % Importa el paquete
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb} 
\usepackage{array}
\usepackage{algpseudocode}
\usepackage{blindtext}
\usepackage{color}
\usepackage{algorithm}
\usepackage{epstopdf}
\restylefloat{algorithm}
\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}
\newcounter{mytempeqncnt}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
	
\begin{document}
%
\title{Survey of hola}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Remigio Hurtado\inst{1}}
%
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University Politecnica Salesiana \email{Rhurtadoo@ups.edu.ec}\\
\url{ups.edu.ec}}
%
\maketitle    
%
\begin{abstract}
This paper presents a comprehensive methodology for the systematic analysis of scientific literature, employing advanced natural language processing techniques for meticulous text data preparation. Generative applications are reshaping various sectors, particularly in computer science, where personalization has become paramount. Major companies like Spotify and Netflix increasingly depend on recommender systems, indicating a growing expectation for the integration of generative learning techniques to enhance these systems. We propose a novel method for generating synthetic datasets tailored for evaluating recommendation performance under simulated conditions, addressing challenges such as increased dataset sizes and user counts. Our approach incorporates the Wasserstein concept into generative adversarial networks for recommender systems, enhancing synthetic dataset quality. Results show that our method effectively mitigates mode collapse, increases dataset size, and improves rating distributions while allowing for flexibility in user and item selection. In addition, we introduce a unique GAN model that utilizes dense, short, continuous embeddings, facilitating rapid and accurate learning. A clustering process converts dense samples into the necessary discrete formats, ensuring alignment with source dataset characteristics. Our findings indicate improved dataset quality and distribution compared to original datasets. The availability of the synthetic datasets and source code promotes access to high	extendash{}quality data for further experimentation, addressing the critical need for synthetic datasets in machine learning research. Our study significantly contributes to enhancing data quality assessment in recommender systems, fostering innovation and improving model performance across various applications.
\end{abstract}

\begin{keywords}
methodology, generative, recommender, datasets, quality
\end{keywords}

\section{Introduction}
\section*{Introduction}

In recent years, generative applications have significantly transformed various domains, including art, computer vision, speech processing, and natural language processing. Within computer science, personalization has emerged as a critical area of focus, particularly as leading companies such as Spotify, Netflix, TripAdvisor, Amazon, and Google increasingly leverage recommender systems to enhance user experiences. This trend raises the expectation that generative learning techniques will be progressively integrated into these systems, paving the way for more sophisticated and tailored recommendations.

Despite the advancements in recommender systems, a pressing issue persists: the need for high	extendash{}quality, diverse, and representative datasets for training and testing these models. The assessment of data quality is paramount, especially in machine learning research, where datasets must support training, validation, and testing tasks effectively. Existing datasets often fall short in terms of volume and variability, leading to suboptimal model performance. To address this, our research proposes a novel approach for generating synthetic datasets that mimic the complexities of real	extendash{}world applications, thereby enhancing the performance and accuracy of recommender systems.

Our methodology is meticulously designed to ensure a comprehensive and systematic analysis of scientific literature. It initiates with the careful preparation of text data, applying advanced natural language processing (NLP) techniques to extract salient insights. The introduction of synthetic datasets is crucial, as they can augment existing datasets, providing researchers with the flexibility to manipulate parameters such as user counts, item counts, and sample sizes, which is not feasible with conventional methods.

The proposed method utilizes Generative Adversarial Networks (GANs) to create collaborative filtering datasets in a parameterized manner. This unique approach allows for the specification of desired characteristics in the generated data, such as the number of users and items, while ensuring rapid and accurate learning through dense, short, and continuous embedding representations. By integrating advanced deep learning techniques, our architecture not only enhances the quality of synthetic datasets but also ensures alignment with the distributions of source datasets.

In this study, we present several key contributions:

	extendash{} **Exhaustive Review of the State of the Art**: A detailed analysis and synthesis of the most relevant and recent contributions in the existing literature.
	extendash{} **Novel GAN	extendash{}based Methodology**: Introduction of a parameterized GAN approach for generating high	extendash{}quality synthetic datasets tailored for recommender systems.
	extendash{} **Evaluation of Dataset Quality**: Comprehensive assessment of the generated datasets against established benchmarks, demonstrating improvements in distribution and quality.

Furthermore, the findings indicate that our approach effectively reduces mode collapse and enhances the size of synthetic datasets, thereby allowing researchers to conduct extensive testing under varying simulated conditions. To ensure reproducibility and foster further research, we have made the accompanying Python and Keras code available in open repositories, promoting accessibility and validation of our findings.

In conclusion, the integration of generative learning into recommender systems holds significant promise for enhancing personalization across various applications. Our study not only addresses the critical need for synthetic datasets in machine learning research but also lays a robust foundation for future advancements in this rapidly evolving field.

\begin{thebibliography}{}
\bibitem{ref1} 
\bibitem{ref2} 
\bibitem{ref3} 
\end{thebibliography}

\newcommand{\listRefsPaper}{ref1, ref2, ref3}
 The structure of this document is: the body of the document, then the conclusion, and finally the bibliography.
\section{Methodology}

Our methodology is designed to ensure a comprehensive and systematic analysis of scientific literature. The methodology used in this study builds on our previous work, including a methodology that leverages machine learning and natural language processing techniques \cite{Hurtado2023}, and a novel method for predicting the importance of scientific articles on topics of interest using natural language processing and recurrent neural networks \cite{Lopez2024}. The process is structured into three phases: data preparation, topic modeling, and the generation and integration of knowledge representations. Each phase is essential for transforming raw text data into meaningful insights, and the detailed parameters and algorithm are explained below. The table \ref{tabDescription} describes the parameters for understanding the overall process and algorithm. The high-level process is presented in Fig. \ref{fig:Methodology}, and the detailed algorithm is outlined in Table \ref{tab:Algorithm}.\\ 

In the data preparation phase, we focus on extracting and cleaning text from scientific documents using natural language processing (NLP) techniques. This phase involves several steps to ensure that the text data is ready for analysis. First, text is extracted from the documents, and non-alphabetic characters that do not add value to the analysis are removed. Next, the text is converted to lowercase, and stopwords (common words that do not contribute much meaning) are removed. We then apply lemmatization, which transforms words to their base form (e.g., "running" becomes "run"). Each document is tokenized (split into individual words or terms), and n-grams (combinations of words) are identified to find common terms. We generate a unified set of common terms, denoted as $TE$, which includes both the terms extracted from the documents and basic terms relevant to any field of study, such as [Fundamentals, Evaluation of Solutions, Trends]. Finally, each document is vectorized with respect to $TE$, resulting in the Document-Term Matrix (DTM).\\

The DTM is a crucial component for topic modeling. It is a matrix where the rows represent the documents in the corpus, and the columns represent the terms (words or n-grams) extracted from the corpus. Each cell in the matrix contains a value indicating the presence or frequency of a term in a document. This structured representation of the text data allows us to apply machine learning techniques to uncover hidden patterns.\\

In the topic modeling phase, we use Latent Dirichlet Allocation (LDA), a popular machine learning technique for identifying topics within a set of documents. By applying LDA to the DTM, we transform the matrix into a space of topics. Specifically, LDA provides us with two key matrices: the Topic-Term Matrix ($\beta$) and the Document-Topic Matrix ($\theta$). The Topic-Term Matrix ($\beta$) indicates the probability that a term is associated with a specific topic, while the Document-Topic Matrix ($\theta$) indicates the probability that a document belongs to a specific topic. For each topic in $\beta$, we assign a name by combining the most probable terms associated with that topic. This process results in the set $T$, which contains the most relevant topics, and the set $K$, which contains the most relevant terms. Additionally, we generate a graph $G_{tk}$ to represent the relationships between topics and terms. Using a language generation model, we refine the names of the topics to ensure they are concise and meaningful, with a maximum of $W_{t}$ words.\\

The final phase involves the generation and integration of knowledge representations, which include summaries, keywords, and interactive visualizations. For each topic $t$ in $T$, we identify the $N$ most relevant documentsâ€”those with the highest probabilities in $\theta$. This set, $D_t$, represents the documents most closely related to each topic. We then generate summaries of these documents, each with a maximum of $W_b$ words, using a language generation model. These summaries include references to the most relevant documents, which are added to the set $R_d$ if they are not already included. We also integrate all the summaries and generate $J$ suggested keywords using a language generation model. Additionally, we create interactive graphs from the $G_{tk}$ graph, showcasing nodes and relationships between the topics and terms, and highlighting significant connections.\\

This methodology provides a clear, structured approach to analyzing scientific literature, leveraging advanced NLP and machine learning techniques to generate useful and comprehensible knowledge representations. This is the methodology used in the development of this study, which presents the fundamentals, solution evaluation techniques, trends, and other topics of interest within this field of study. This structured approach ensures a thorough review and synthesis of the current state of knowledge, providing valuable insights and a solid foundation for future research.

\begin{table*}[!h]
	\caption{\centering Description of parameters of the proposed methodology}
	\begin{center}
		\begin{tabular}{|l|l|}
			\hline
			\textbf{Parameter} & \textbf{Description}                                                            \\ \hline
			$T$                  & Set of topics to be generated with the LDA model. \\ \hline
			$\#T$                & Cardinality of $T$. That is, the number of topics (dimensions).                                    \\ \hline
			$K$                  & Set of common terms with the highest probability in topic $t$ used to label that topic. \\ \hline
			$\#K$ 				 & Cardinality of $K$.                                         \\ \hline
			$W_{t}$   			 & Maximum number of words for combining the common terms of topic $t$ into a new topic name.\\ \hline
			$N$                  & Number of the most relevant documents to generate a summary of topic $t$.\\ \hline
			$W_b$                & Maximum number of words to generate a summary of the list of $N$ most related documents to topic $t$.\\ \hline
			$J$                  & Number of suggested keywords to be generated as knowledge representation.\\ \hline
		\end{tabular}
		\label{tabDescription}
	\end{center}
\end{table*}

\begin{figure*}[!h]
	\centering
	\includegraphics[width=1.0\textwidth]{metodo.png}
	\caption{Methodology for the generation of knowledge representations}
	\label{fig:Methodology}
\end{figure*}

%Actual
\begin{figure*}[!h]
	\centering
	\begin{tabular}{l}
		\hline
		\textbf{General Algorithm} \\
		\hline
		\textbf{Input:} Field of study, Scientific articles collected from virtual libraries, $\#T$, $\#K$, $W_{t}$, $N$, $W_b$, $J$\\
		\hline
		\textbf{Phase 1: Data Preparation with Natural Language Processing (NLP)} \\
		\quad a. Extract of text from documents. \\
		\quad b. Remove non-alphabetic characters that do not add value to the analysis. \\
		\quad c. Convert to lowercase and remove stopwords. \\
		\quad d. Apply lemmatization to transform words to their base form. \\
		\quad e. In each document, tokenize and identify n-grams to identify common terms (words or n-grams). \\
		\quad f. Unified generation of common terms of all documents. Where $TE$ is the unified set of common terms.\\
		\quad g. Add in $TE$ the basic terms for any field of study, such as: [Fundamentals, Evaluation of Solutions, Trends]. \\
		\quad h. Vectorize each document with respect to $TE$ and generate Document-Term Matrix (DTM).\\
		\quad \textbf{Output:} For each document [title, original text, common terms], and Document-Term Matrix (DTM)\\
		\textbf{Phase 2: Topic Modeling whit Machine Learning} \\
		\quad \textbf{Input:} Document-Term Matrix (DTM) \\
		\quad a. Apply Latent Dirichlet Allocation (LDA) to transform the DTM Matrix into a space of $\#T$ topics (dimensions).\\
		\quad b. Obtain the Topic-Term Matrix ($\beta$) that indicates the probability that a term is generated by a specific topic.\\ 
		\quad c. Obtain the Document-Topic Matrix ($\theta$) that indicates the probability that a document belongs to a specific topic.\\
		\quad d. For each unknown $t$ topic in $\beta$, assign a name or label to the $t$ topic by combining the $\#K$ highest probability\\
		\quad \quad common terms in $\beta$ associated with that $t$ topic. This generates: \\
		\quad \quad The $T$ Set with the $\#T$ most relevant topics. \\
		\quad \quad The $K$ Set with the $\#K$ most relevant terms. \\
		\quad \quad The $G_{tk}$ Graph of the relationships between the most relevant topics ($T$) and the most relevant terms ($K$). \\
		\quad e. For each topic $t$ in $T$, modify topic $t$ by combining the common terms of $t$ into a new topic name with at most \\
		\quad \quad $W_{t}$ words using a language generation model.\\
		\quad \textbf{Output:} Relevant Topics ($T$), Topic-Term Matrix ($\beta$), Document-Topic Matrix ($\theta$) \\
		\textbf{Phase 3: Generation and Integration of Knowledge Representations} \\
		\quad \textbf{Input:} Relevant Topics $T$, Document-Topic Matrix ($\theta$) \\
		\quad \textbf{3.1: Knowledge representations through summaries and keywords}\\
		\quad \quad a. For each $t$ topic in $T$, obtain its $N$ most relevant documents, i.e., those with the highest probabilities in $\theta$. \\
		\quad \quad \quad Thus, $D_t$ represents the set of documents most related to each topic $t$.\\
		\quad \quad b. For each topic $t$ in $T$, and from $D_t$ generate a summary of the list of documents most related to that topic $t$ with \\
		\quad \quad \quad at most $W_b$ words using a language generation model. \\
		\quad \quad c. Incorporate into the summary the text citing references to the most relevant documents. Add these references to the \\ 
		\quad \quad \quad set $R_d$, which will contain all cited references, including new references if they have not been previously included.\\
		\quad \quad d. Integrate all the summaries and from them generate $J$ suggested keywords using a language generation model.\\
		\quad \textbf{3.2: Knowledge representations through knowledge visualizations with interactive graphs}\\
		\quad \quad a. From the $G_{tk}$ graph, generate an interactive graph with nodes and relationships between the topics $T$\\
		\quad \quad and the $K$ most relevant terms.\\
		\quad \quad b. Highlight the connections between the topics $T$ and the $K$ most relevant terms.\\
		\quad \textbf{3.3: Integration of Knowledge Representations}\\
		\hline
		\textbf{Output:} Knowledge Representations \\
		\hline
	\end{tabular}
	\caption{\centering General algorithm of the methodology incorporating natural language processing, machine learning techniques and language generation models}
	\label{tab:Algorithm}
\end{figure*}


\section{Suggest offer advise}
Generative applications are currently transforming various fields, including art, computer vision, speech processing, and natural language processing. Within the realm of computer science, the area of personalization has gained significant importance, especially as major companies like Spotify, Netflix, TripAdvisor, Amazon, and Google increasingly rely on recommender systems. This trend suggests a rational expectation that generative learning techniques will be progressively utilized to enhance existing recommender systems.

This paper proposes a novel method for generating synthetic datasets specifically designed for testing the recommendation performance and accuracy of companies under different simulated conditions. Such conditions may include significant increases in dataset sizes, user numbers, or item counts. The proposed method improves upon state-of-the-art techniques by incorporating the Wasserstein concept into the seminal generative adversarial network for recommender systems (GANRS). This enhancement aims to create synthetic datasets that better reflect the complexities of real-world applications.

The findings demonstrate that the proposed method effectively reduces mode collapse, increases the size of synthetic datasets, and enhances the distribution of ratings. Additionally, it allows for flexibility in selecting the desired number of users, items, and the initial size of the dataset. Both the baseline GANRS and the new Wasserstein-based architecture, referred to as WGANRS, utilize advanced deep learning techniques to generate artificial profiles. Notably, they leverage dense, short, and continuous embeddings in the latent space, which contrasts with the sparse, large, and discrete samples used in prior models.

To facilitate reproducibility and further research, the authors have made available the accompanying Python and Keras code in open repositories. This resource includes the synthetic datasets generated for testing the effectiveness of the proposed architecture. Such transparency is crucial for the research community, enabling others to validate and build upon these findings.

In conclusion, the integration of generative learning into recommender systems holds significant promise for enhancing personalization across various applications. The methodologies developed in this study provide a robust framework for generating synthetic datasets that can be utilized in extensive testing scenarios. As the field continues to evolve, the implications for both academic research and practical applications in industry are profound. The continuous improvement of recommender systems through advanced generative techniques is a vital step toward optimizing user experiences and providing tailored content in a data-driven world.
\section{Data quality assessment}
The assessment of data quality is crucial in machine learning, particularly for research and educational purposes where diverse, representative, and open datasets are required. Such datasets must contain ample samples to effectively manage training, validation, and testing tasks. In the field of Recommender Systems, there are numerous subfields focused on continuously enhancing accuracy and other quality measures. To support this variety in research, it is essential to augment existing datasets with synthetic ones.

This paper introduces a novel method based on Generative Adversarial Networks (GANs) to generate collaborative filtering datasets in a parameterized manner. This allows researchers to select their desired number of users, items, samples, and levels of stochastic variability. Such parameterization is not feasible with conventional GANs, which typically operate on fixed input sizes.

The GAN model proposed in this research is unique as it utilizes dense, short, and continuous embedding representations of items and users, rather than the more common sparse, large, and discrete vectors. This approach ensures both rapid and accurate learning, which is a significant improvement over traditional methods that rely on large and sparse input vectors. By integrating a DeepMF model, the architecture extracts dense embeddings for users and items, facilitating the generation of high-quality synthetic datasets.

To convert the dense samples produced by the GAN into the discrete and sparse formats required for creating each synthetic dataset, a clustering process is implemented. This two-step approach not only enhances the quality of the generated datasets but also ensures that they align closely with the distributions and characteristics of the source datasets. The results obtained from three different source datasets indicate that the generated datasets exhibit adequate distributions and expected quality values, showcasing an evolution in quality compared to the original datasets.

The availability of synthetic datasets and source code for researchers further promotes the accessibility of high-quality data, encouraging more extensive experimentation and validation in the field. By providing these resources, the study contributes significantly to the ongoing efforts to improve data quality assessment in machine learning applications, particularly for recommender systems.

In conclusion, the proposed GAN-based method addresses the critical need for synthetic datasets in machine learning research. It not only facilitates the augmentation of existing datasets but also enhances the overall quality of data used in training and testing recommender systems. Such advancements are essential for driving innovation and improving the performance of machine learning models in various applications.
\section{Trend analysis essentials}
Matrix factorization models are essential in the realm of commercial collaborative filtering recommender systems. The research presented tested six prominent matrix factorization models across four collaborative filtering datasets. The intent was to gauge not only accuracy but also various quality measures that extend beyond basic predictions. These measures included the ability to recommend items in both ordered and unordered lists, as well as evaluating the novelty and diversity of the recommendations.

The results indicated that each matrix factorization model has its unique advantages based on specific criteria such as simplicity, the requisite level of prediction quality, and the necessary recommendation quality. Additionally, the study emphasized the importance of novelty and diversity in recommendations, which are critical for enhancing user experience and engagement. The ability to explain the recommendations made by the models also emerged as a crucial factor, as users often seek to understand the rationale behind suggested items.

Another significant aspect highlighted in the findings is the capability of assigning semantic interpretations to hidden factors within the models. This feature can enhance the relevance of recommendations, as it allows for a more intuitive understanding of user preferences and item characteristics. Furthermore, the research underscored the advisability of recommending to groups of users, which can be particularly beneficial in scenarios where collaborative consumption is prevalent.

To ensure the reliability of the models and the reproducibility of the experiments, an open framework was employed. This approach not only fosters transparency in the research process but also facilitates further exploration by other researchers in the field. The implementation code used in the experiments has been made available, allowing for independent validation and comparison of results.

Overall, the comprehensive evaluation of these matrix factorization models sheds light on their effectiveness in collaborative filtering recommender systems. By addressing the various dimensions of recommendation quality, including accuracy, novelty, diversity, and interpretability, the study provides valuable insights that can guide future developments in this area. The findings are pivotal for both academic research and practical applications, as they highlight the multifaceted nature of recommender systems and the continuous need for improvement in model performance.
\section{Conclusion}
The proposed research significantly advances the field of recommender systems through the introduction of innovative methodologies aimed at enhancing data quality, personalization, and model performance. By leveraging generative adversarial networks to create synthetic datasets, the study addresses the critical need for diverse and representative data in machine learning. This approach not only augments existing datasets but also ensures improved distribution and quality of the generated data, facilitating rigorous testing and validation of recommendation algorithms. The thorough evaluation of various matrix factorization models further underscores the importance of multiple quality measures, including novelty and diversity, in user engagement. The findings advocate for a deeper understanding of user preferences through semantic interpretations of hidden factors, thereby enhancing recommendation relevance. Additionally, the provision of open	extendash{}source code and synthetic datasets promotes transparency and reproducibility, encouraging further research and innovation in the domain. Ultimately, these contributions are pivotal for both academic inquiry and practical implementations, paving the way for more effective and user	extendash{}centered recommender systems in an increasingly data	extendash{}driven landscape.
\begin{thebibliography}{00}
    \bibitem{Jesus_2023} Jesus Bobadilla and Abraham Gutierrez and Raciel Yera and Luis Martinez (2023). Creating synthetic datasets for collaborative filtering recommender systems using generative adversarial networks.
\bibitem{b1}
    Hurtado, Remigio; PicÃ³n, Cristian; MuÃ±oz, Arantxa; Hurtado, Juan.
    "Survey of Intent-Based Networks and a Methodology Based on Machine Learning and Natural Language Processing."
    In Proceedings of Eighth International Congress on Information and Communication Technology.
    Springer Nature Singapore, Singapore, 2024.
    \bibitem{b2}
    Park, Keunheung; Kim, Jinmi; Lee, Jiwoong. 
    ``Visual Field Prediction using Recurrent Neural Network,'' 
    \emph{Scientific Reports}, 
    vol. 9, no. 1, p. 8385, 
    2019, 
    https://doi.org/10.1038/s41598-019-44852-6.
    
    \bibitem{b3}
    Xu, M.; Du, J.; Guan, Z.; Xue, Z.; Kou, F.; Shi, L.; Xu, X.; Li, A. 
    ``A Multi-RNN Research Topic Prediction Model Based on Spatial Attention and Semantic Consistency-Based Scientific Influence Modeling,'' 
    \emph{Comput Intell Neurosci}, 
    vol. 2021, 
    2021, 
    p. 1766743, 
    doi: 10.1155/2021/1766743.
    
    \bibitem{b4}
    Kreutz, Christin; Schenkel, Ralf.
    ``Scientific Paper Recommendation Systems: a Literature Review of recent Publications,''
    2022/01/03.
	\bibitem{Hurtado2023} Hurtado, R., et al. "Survey of Intent-Based Networks and a Methodology Based on Machine Learning and Natural Language Processing." International Congress on Information and Communication Technology. Singapore: Springer Nature Singapore, 2023.
    \bibitem{Lopez2024} Lopez, A., Dutan, D., Hurtado, R. "A New Method for Predicting the Importance of Scientific Articles on Topics of Interest Using Natural Language Processing and Recurrent Neural Networks." In: Yang, X.S., Sherratt, S., Dey, N., Joshi, A. (eds) Proceedings of Ninth International Congress on Information and Communication Technology. ICICT 2024 2024. Lecture Notes in Networks and Systems, vol 1013. Springer, Singapore. https://doi.org/10.1007/978-981-97-3559-4\_50.
\end{thebibliography}
\end{document}